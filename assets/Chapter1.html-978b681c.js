import{_ as t}from"./plugin-vue_export-helper-c27b6911.js";import{r as o,o as i,c as l,b as n,e,f as a,h as r}from"./app-7864f323.js";const c="/images/huggingface/section1/transformers.svg",p={},d=r('<h2 id="_1-nlp-介绍" tabindex="-1"><a class="header-anchor" href="#_1-nlp-介绍" aria-hidden="true">#</a> 1. NLP 介绍</h2><p>NLP 的任务不仅仅是理解单个字词的含义，而是要理解上下文的含义。</p><p>NLP 任务有很多，比如：</p><ul><li><strong>对整个句子进行分类</strong>：获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关</li><li><strong>对句子中的每个词语进行分类</strong>：识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）</li><li><strong>生成上下文</strong>：用自动生成的文本完成提示，用屏蔽词填充文本中的空白</li><li><strong>从文本中提取答案</strong>：给定问题和上下文，根据上下文中提供的信息提取问题的答案</li><li><strong>根据输入文本生成新的句子</strong>：将文本翻译成另一种语言，总结文本</li></ul><h3 id="_1-1-术语-architectures-vs-checkpoints" tabindex="-1"><a class="header-anchor" href="#_1-1-术语-architectures-vs-checkpoints" aria-hidden="true">#</a> 1.1 术语：Architectures vs. checkpoints</h3><p>在接下来的学习中，你将会看到 architectures、checkpoints，还有 models 这些术语。</p><ul><li>Architecture: 模型框架。每一层的定义、模型中发生的每个操作的定义。</li><li>Checkpoints: 对于一个给定 architecture 的权重。</li><li>Model: 范语，可能是指 architecture，也可能是指 checkpoints。</li></ul><p>如：BERT 是一个 architecture。bert-base-cased 是由 Google 团队为 BERT 训练的初始权重，它是 checkpoints。我们可以说 BERT model，也可以说 bert-base-cased model.</p><h2 id="_2-transformers-能做什么" tabindex="-1"><a class="header-anchor" href="#_2-transformers-能做什么" aria-hidden="true">#</a> 2. Transformers 能做什么</h2>',9),u={href:"https://github.com/huggingface/transformers",target:"_blank",rel:"noopener noreferrer"},h={href:"https://huggingface.co/models",target:"_blank",rel:"noopener noreferrer"},m=r(`<h3 id="_2-1-快速体验-🤗-transformers-库" tabindex="-1"><a class="header-anchor" href="#_2-1-快速体验-🤗-transformers-库" aria-hidden="true">#</a> 2.1 快速体验 🤗 Transformers 库</h3><p>🤗 Transformers 库提供了 <code>pipeline()</code> 函数，它聚合了预训练模型和对应的文本预处理。使用该函数可以直接根据输入返回目标输出。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

<span class="token comment"># 选择任务 sentiment-analysis，创建分类器对象</span>
<span class="token comment"># 没有指定 model，则会使用默认 model</span>
classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">&quot;sentiment-analysis&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 1 传入一个句子</span>
classifier<span class="token punctuation">(</span><span class="token string">&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class="token punctuation">)</span>
<span class="token comment"># 结果：[{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9598048329353333}]</span>

<span class="token comment"># 2 传入多个句子</span>
classifier<span class="token punctuation">(</span>
    <span class="token punctuation">[</span><span class="token string">&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;I hate this so much!&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
<span class="token comment"># 结果</span>
<span class="token comment"># [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9598048329353333},</span>
<span class="token comment">#  {&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9994558691978455}]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,3),g={href:"https://huggingface.co/models",target:"_blank",rel:"noopener noreferrer"},f=n("code",null,"model",-1),_=r(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

generator <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">&quot;text-generation&quot;</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">&quot;distilgpt2&quot;</span><span class="token punctuation">)</span>
generator<span class="token punctuation">(</span>
    <span class="token string">&quot;In this course, we will teach you how to&quot;</span><span class="token punctuation">,</span>
    max_length<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span>
    num_return_sequences<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-2-局限性-偏见" tabindex="-1"><a class="header-anchor" href="#_2-2-局限性-偏见" aria-hidden="true">#</a> 2.2 局限性 &amp; 偏见</h3><p>为了在大规模数据上进行预训练，研究员们会收集尽可能多的数据，这其中可能会夹杂一些意识形态或者价值观的刻板印象。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

unmasker <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">&quot;fill-mask&quot;</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">&quot;bert-base-uncased&quot;</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">&quot;This man works as a [MASK].&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">&quot;token_str&quot;</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span>

result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">&quot;This woman works as a [MASK].&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">&quot;token_str&quot;</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-text" data-ext="text"><pre class="language-text"><code>[&#39;lawyer&#39;, &#39;carpenter&#39;, &#39;doctor&#39;, &#39;waiter&#39;, &#39;mechanic&#39;]
[&#39;nurse&#39;, &#39;waitress&#39;, &#39;teacher&#39;, &#39;maid&#39;, &#39;prostitute&#39;]
</code></pre></div><p>观察结果，有明显的性别相关性，妓女成为了“女性工作”相关的前五名答案之一。</p><div class="hint-container warning"><p class="hint-container-title">注意</p><p>原始模型中很容易掺杂性别歧视、种族歧视等问题，在模型上进一步微调并不会消除这种偏差。</p></div><h2 id="_3-transformer-背景知识" tabindex="-1"><a class="header-anchor" href="#_3-transformer-背景知识" aria-hidden="true">#</a> 3. Transformer 背景知识</h2><p>Transformer 架构于 2017 年 6 月推出。大体上可以将 Transformer 模型分为三类：</p><ul><li>GPT-like (自回归（auto-regressive）Transformer 模型)</li><li>BERT-like (自编码（auto-encoding）Transformer 模型)</li><li>BART/T5-like (序列到序列（sequence-to-sequence）Transformer 模型)</li></ul><h3 id="_3-1-transformer-是语言模型-language-model" tabindex="-1"><a class="header-anchor" href="#_3-1-transformer-是语言模型-language-model" aria-hidden="true">#</a> 3.1 Transformer 是语言模型（language model）</h3><p>包括 GPT、BERT、BART、T5 等 Transformer 模型都是语言模型，即他们已经以自监督学习（self-supervised）的方式在大量文本上进行了训练。</p><p>这类模型在其进行训练的语料上进行了理解，但是对于具体问题，它就没那么有针对性了，于是我们需要进行迁移学习（transfer learning）。在迁移学习时，对于具体问题，我们使用人工标注的数据以有监督的方式进行精调（fine-tune）。</p><h3 id="_3-2-transformer-是大模型" tabindex="-1"><a class="header-anchor" href="#_3-2-transformer-是大模型" aria-hidden="true">#</a> 3.2 Transformer 是大模型</h3><p>实现更好性能的一般策略是增加模型的大小以及预训练的数据量。</p><img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" width="60%"><h3 id="_3-3-迁移学习-transfer-learning" tabindex="-1"><a class="header-anchor" href="#_3-3-迁移学习-transfer-learning" aria-hidden="true">#</a> 3.3 迁移学习（Transfer Learning）</h3><p>预训练（Pretraining）指从头开始训练模型。这往往需要使用大规模语料，花费长达数周的时间。</p><p>微调（Fine-tuning）是在预训练好的模型上进行进一步的训练。要进行微调，你需要使用预训练模型以及针对特定任务的数据集再次进行训练。进行微调可以有效降低时间、设备成本，使用更小的数据集完成。</p><h2 id="_4-transformer-结构" tabindex="-1"><a class="header-anchor" href="#_4-transformer-结构" aria-hidden="true">#</a> 4. Transformer 结构</h2>`,20),k={class:"hint-container info"},b=n("p",{class:"hint-container-title"},"扩展阅读",-1),v={href:"http://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noopener noreferrer"},T=n("p",null,"Transformer 主要由两部分组成：",-1),q=n("ul",null,[n("li",null,"Encoders (编码器): 编码器接收输入并构建其表示（即特征）。这意味着对模型进行了优化，以从输入中获得理解。"),n("li",null,"Decoders (解码器): 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。")],-1),y=n("p",null,"这两部分可以单独使用，这取决于你要做什么任务：",-1),x=n("p",null,[n("strong",null,"Encoder-only 模型（auto-encoding models）"),e("：适用于需要理解输入的任务，如句子分类和命名实体识别。")],-1),w={href:"https://huggingface.co/docs/transformers/model_doc/albert",target:"_blank",rel:"noopener noreferrer"},E={href:"https://huggingface.co/docs/transformers/model_doc/bert",target:"_blank",rel:"noopener noreferrer"},I={href:"https://huggingface.co/docs/transformers/model_doc/distilbert",target:"_blank",rel:"noopener noreferrer"},B={href:"https://huggingface.co/docs/transformers/model_doc/electra",target:"_blank",rel:"noopener noreferrer"},R={href:"https://huggingface.co/docs/transformers/model_doc/roberta",target:"_blank",rel:"noopener noreferrer"},A=n("p",null,[n("strong",null,"Decoder-only 模型（auto-regressive models）"),e("：适用于生成任务，如文本生成。")],-1),L={href:"https://huggingface.co/docs/transformers/model_doc/ctrl",target:"_blank",rel:"noopener noreferrer"},P={href:"https://huggingface.co/docs/transformers/model_doc/openai-gpt",target:"_blank",rel:"noopener noreferrer"},N={href:"https://huggingface.co/docs/transformers/model_doc/gpt2",target:"_blank",rel:"noopener noreferrer"},C={href:"https://huggingface.co/docs/transformers/model_doc/transfo-xl",target:"_blank",rel:"noopener noreferrer"},V=n("p",null,[n("strong",null,"Encoder-decoder 模型（sequence-to-sequence models）"),e("：适用于需要根据输入进行生成的任务，如翻译或摘要。预训练这类模型可以使用 encode 或 decoder 的目标。")],-1),G={href:"https://huggingface.co/docs/transformers/model_doc/bart",target:"_blank",rel:"noopener noreferrer"},M={href:"https://huggingface.co/docs/transformers/model_doc/mbart",target:"_blank",rel:"noopener noreferrer"},S={href:"https://huggingface.co/docs/transformers/model_doc/marian",target:"_blank",rel:"noopener noreferrer"},D={href:"https://huggingface.co/docs/transformers/model_doc/t5",target:"_blank",rel:"noopener noreferrer"},F=r('<h3 id="_4-1-注意力层-attention-layers" tabindex="-1"><a class="header-anchor" href="#_4-1-注意力层-attention-layers" aria-hidden="true">#</a> 4.1 注意力层（Attention Layers）</h3><p>注意力层使得模型对不同位置的字词有着不同的关注程度。</p><p>比如，在做文本翻译任务时，将 &quot;I like eating apples&quot; 翻译成中文，在翻译 like 时，模型需要关注 I 和 eating 来获得正确的翻译，而对 apples 的关注度可能小一些；翻译 &quot;It feels like a soft blanket&quot; 时，关注 feels 会帮助模型获得正确的翻译。</p><h3 id="_4-2-原始模型" tabindex="-1"><a class="header-anchor" href="#_4-2-原始模型" aria-hidden="true">#</a> 4.2 原始模型</h3><p>Transformer 最开始是为了翻译任务而设计的。</p><p>在训练过程中，encoder 和 decoder 分别接收两种语言的同一个句子。encoder 使用注意力层，可以“看到”该句子中的全部字词。而 decoder 只能看到已经翻译好的字词（即在正在被翻译的字词之前已经生成的部分）。 比如 decoder 已经生成了3个单词，在生成第4个单词时，我们会把前三个单词也作为输入，连同 encoder 输出的部分一起作为 decoder 的输入来生成第4个单词。</p><p>为了加快训练，我们会喂给 decoder 完整的目标，但是不允许它使用没有预测的词汇。例如，我们正在预测第4个单词，但是模型看到了目标中的第4个单词，显然这样的模型在实际中不会获得好的效果。</p><p>最初的 Transformer 结构如下：</p><p><img src="'+c+'" alt=""></p><p>注意，在 decoder 中，第一个注意力层关注所有 decoder 的过去的输入，第二个注意力层，使用了来自 encoder 的输出。因此它能够获得完整的输入句子来对当前词语进行最佳预测。</p><p>我们还可以使用注意力遮罩层（attention mask）以使得模型关注某些表示。比如，在批处理句子时，会使用填充的方式使句子长度保持一致，填充的内容无意义，我们不希望模型关注它。</p><h2 id="_5-小结" tabindex="-1"><a class="header-anchor" href="#_5-小结" aria-hidden="true">#</a> 5. 小结</h2>',12),H=n("code",null,"pipeline()",-1),j={href:"https://huggingface.co/models",target:"_blank",rel:"noopener noreferrer"},K={href:"http://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noopener noreferrer"};function O(X,z){const s=o("ExternalLinkIcon");return i(),l("div",null,[d,n("p",null,[e("你可以使用"),n("a",u,[e("🤗 Transformers 库"),a(s)]),e("来创建并使用公开的模型。你可以在"),n("a",h,[e("模型中心"),a(s)]),e("中查找预训练模型。你也可以在 Hub 中上传你自己的模型。")]),m,n("p",null,[e("目前支持的 pipeline 见 "),n("a",g,[e("Model 中心"),a(s)]),e("。 如果不想使用默认模型，可通过 "),f,e(" 参数传递对应的模型名称。")]),_,n("div",k,[b,n("p",null,[e("推荐 "),n("a",v,[e("The Illustrated Transformer"),a(s)]),e(" 这篇文章。在该文章中，作者使用动图清晰地描述了 Transformer 的结构和原理。")])]),T,q,y,n("ul",null,[n("li",null,[x,n("p",null,[e("这类模型有 "),n("a",w,[e("ALBERT"),a(s)]),e(", "),n("a",E,[e("BERT"),a(s)]),e(", "),n("a",I,[e("DistillBERT"),a(s)]),e(", "),n("a",B,[e("ELECTRA"),a(s)]),e(", "),n("a",R,[e("RoBERTa"),a(s)])])]),n("li",null,[A,n("p",null,[e("这类模型有 "),n("a",L,[e("CTRL"),a(s)]),e(", "),n("a",P,[e("GPT"),a(s)]),e(", "),n("a",N,[e("GPT-2"),a(s)]),e(", "),n("a",C,[e("Transformer XL"),a(s)])])]),n("li",null,[V,n("p",null,[e("这类模型有 "),n("a",G,[e("BART"),a(s)]),e(", "),n("a",M,[e("mBART"),a(s)]),e(", "),n("a",S,[e("Marian"),a(s)]),e(", "),n("a",D,[e("T5"),a(s)])])])]),F,n("p",null,[e("本节内容介绍了 NLP 任务以及如何使用 🤗 Transformers 中的 "),H,e(" 函数来执行不同的 NLP 任务。你可以在"),n("a",j,[e("模型中心"),a(s)]),e("中查找模型，按照 Model Card 中的说明或者使用页面上的 inference API 进行使用。")]),n("p",null,[e("我们简单介绍了 Transformer 的结构，如果你想做进一步了解，推荐阅读 "),n("a",K,[e("The Illustrated Transformer"),a(s)]),e("。")])])}const U=t(p,[["render",O],["__file","Chapter1.html.vue"]]);export{U as default};
