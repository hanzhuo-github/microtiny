import{_ as l}from"./plugin-vue_export-helper-c27b6911.js";import{o as n,c as i,a as m,b as s,d as a,f as t}from"./app-1e7cdb66.js";const p="/images/ml/drl/deep-q-learning.jpeg",r="/images/ml/drl/deep-q-network.jpg",e="/images/ml/drl/Q-target.jpg",c="/images/ml/drl/sampling-training.jpg",o="/images/ml/drl/experience-replay.jpg",g="/images/ml/drl/fixed-q-target-pseudocode.jpg",h={},u=s("p",null,[a("Q-Learning 是 "),s("em",null,"tabular method"),a("，It is not scalable. 当 state space 比较小时，我们可以使用之前的 Q-learning，训练 Q Table 来实现目标。当 state space 很大时（玩 Atari 游戏时，可能达到 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"1"),s("msup",null,[s("mn",null,"0"),s("mn",null,"9")])]),s("annotation",{encoding:"application/x-tex"},"10^9")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mord"},[s("span",{class:"mord"},"0"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"9")])])])])])])])])])]),a(" 到 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"1"),s("msup",null,[s("mn",null,"0"),s("mn",null,"11")])]),s("annotation",{encoding:"application/x-tex"},"10^{11}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mord"},[s("span",{class:"mord"},"0"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"11")])])])])])])])])])])]),a(" 种状态之多），Q Table 就会很低效。在这种情况下，我们不再使用 Q 表，而是使用 Neural Network 来获取状态、根据该状态为每一个 action 估计 Q-value，这种方式就是 Deep Q-Learning。")],-1),d=s("figure",null,[s("img",{src:p,alt:"",width:"600",tabindex:"0"}),s("figcaption")],-1),v=t('<h2 id="_1-the-deep-q-network-dqn" tabindex="-1"><a class="header-anchor" href="#_1-the-deep-q-network-dqn" aria-hidden="true">#</a> 1. The Deep Q-Network (DQN)</h2><p>Deep Q-Learning Network 的架构如下：</p><figure><img src="'+r+'" alt="" width="600" tabindex="0"><figcaption></figcaption></figure><p>我们输入 state（a stack of 4 frames），输出在该状态下每个可能的 action 的 Q-value 向量。这样，我们就可以像 Q-Learning 一样，使用 epsilon-greedy policy 来决定采取哪种行动。</p><h3 id="dqn-的过程" tabindex="-1"><a class="header-anchor" href="#dqn-的过程" aria-hidden="true">#</a> DQN 的过程</h3><p>为了降低 state 的复杂性，我们对输入进行降维（84 * 84）并灰度化（grayscale），将 3 通道 GRB（<code>160*210*3</code>）降低到 1 维（<code>84*84*1</code>）。</p><blockquote><p>能这么做是因为 Atari environments 中的颜色并没有很重要的信息。同样，我们也可以截掉屏幕的一部分，如果它没有什么重要信息。</p></blockquote><p>我们将 4 帧信息堆叠在一起（这样可以捕捉到 temporal information，解决 temporal limitation 的问题）作为输入。</p><p>然后经过 3 层 convolutional layers。卷积层可以捕捉并利用图像的空间的关系。由于我们使用了 4 个帧，它还能利用这些帧之间的一些 temporal properties。</p><p>最后，经过几个 fully connected layers，为该状态下每个可能的 action 输出对应的 Q-value。</p><h2 id="_2-the-deep-q-learning-algorithm" tabindex="-1"><a class="header-anchor" href="#_2-the-deep-q-learning-algorithm" aria-hidden="true">#</a> 2. The Deep Q-Learning Algorithm</h2><p>Deep Q-Learning 和 Q-Learning 的主要不同之处在于：Q-Learning 直接对 Q-value 进行更新</p>',12),y=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"Q"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"S"),s("mi",null,"t")]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"A"),s("mi",null,"t")]),s("mo",{stretchy:"false"},")"),s("mo",null,"←"),s("mi",null,"Q"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"S"),s("mi",null,"t")]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"A"),s("mi",null,"t")]),s("mo",{stretchy:"false"},")"),s("mo",null,"+"),s("mi",null,"α"),s("mo",{stretchy:"false"},"["),s("msub",null,[s("mi",null,"R"),s("mrow",null,[s("mi",null,"t"),s("mo",null,"+"),s("mn",null,"1")])]),s("mo",null,"+"),s("mi",null,"γ"),s("munder",null,[s("mo",null,[s("mi",null,"m"),s("mi",null,"a"),s("mi",null,"x")]),s("mi",null,"a")]),s("mi",null,"Q"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"S"),s("mrow",null,[s("mi",null,"t"),s("mo",null,"+"),s("mn",null,"1"),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"A"),s("mi",null,"t")])])]),s("mo",{stretchy:"false"},")"),s("mo",null,"−"),s("mi",null,"Q"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"S"),s("mi",null,"t")]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"A"),s("mi",null,"t")]),s("mo",{stretchy:"false"},")"),s("mo",{stretchy:"false"},"]")]),s("annotation",{encoding:"application/x-tex"}," Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma \\mathop{max}\\limits_{a} Q(S_{t+1, A_t}) - Q(S_t, A_t)] ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"A"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"←"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"A"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"α"),s("span",{class:"mopen"},"["),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0077em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"t"),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mtight"},"1")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2083em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.45em","vertical-align":"-0.7em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.05556em"}},"γ"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.4306em"}},[s("span",{style:{top:"-2.4em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"a")])])]),s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",null,[s("span",{class:"mop"},[s("span",{class:"mord mathnormal"},"ma"),s("span",{class:"mord mathnormal"},"x")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3283em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"t"),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mtight"},"1"),s("span",{class:"mpunct mtight"},","),s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"A"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2963em"}},[s("span",{style:{top:"-2.357em","margin-left":"0em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.143em"}},[s("span")])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"A"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")]")])])])])],-1),f=t('<p>而 Deep Q-Learning 创建 loss function（比较 Q-value prediction 和 Q-target），利用 gradient descent 来更新 Deep Q-Network 的权重来估计 Q-value。</p><figure><img src="'+e+'" alt="" width="600" tabindex="0"><figcaption></figcaption></figure><p>Deep Q-Learning 算法有两个阶段：</p><ul><li>Sampling：执行动作，将观察到的 experience tuples 存到 replay memory 中。</li><li>Training：随机选择一小批 tuples，利用 gradient descent 从这批 tuples 中学习</li></ul><figure><img src="'+c+'" alt="" width="700" tabindex="0"><figcaption></figcaption></figure><p>由于使用了非线性 Q-value function（Neural Network）和 bootstrapping（利用现有估计更新目标，而不是实际的完整的回报），Deep Q-Learning 的训练可能不稳定。</p><p>为了使得训练更稳定，我们采取了下面的措施：</p><ol><li><em>Experience Replay</em> to make more <strong>efficient use of experiences</strong>.</li><li><em>Fixed Q-Target</em> <strong>to stabilize the training</strong>.</li><li><em>Double Deep Q-Learning</em>, to <strong>handle the problem of the overestimation of Q-values</strong>.</li></ol><h3 id="_2-1-experience-replay" tabindex="-1"><a class="header-anchor" href="#_2-1-experience-replay" aria-hidden="true">#</a> 2.1 Experience Replay</h3><p>创建 replay memory 有两个作用：</p><ol><li><p>在训练过程中高效利用 experiences。</p><p>一般来说，在 online reinforcement learning 中，agent 和环境进行交互，获得 experiences（state, action, reward, next state），从中学习（updates the neural network），然后就 discard 了这些 experiences。</p><p>使用 replay buffer 存储 Experience samples，我们可以在训练过程中重复使用，这样更加高效。</p><p>它也能够让 agent 多次学习相同的 experiences。</p></li><li><p>避免忘记以前的 experiences，减少了 experiences 之间的 correlation。</p><p>如果我们给神经网络提供 sequential samples of experiences，当它获得新 experiences 时，往往会忘记以前的 experiences。比如，agent 玩了第一关，然后玩了第二关，它可能会忘记第一关怎么玩了。</p><p>解决的办法就是，当 agent 与环境进行交互时，将 experience tuples 存到 replay buffer 中，之后取样一批 tuples。这样就避免了网络只学它刚刚做的事情了。</p><p>通过随机抽样，消除了 observation sequences 中的相关性，避免 action values 振荡以及灾难性的偏离。</p></li></ol><p>我们初始化了 replay buffer D，其 capacity N 是一个超参。在训练过程中，存储 experiences，并取样一个批次喂给 Deep Q-Network。</p><figure><img src="'+o+'" alt="" width="600" tabindex="0"><figcaption></figcaption></figure><h3 id="_2-2-fixed-q-target" tabindex="-1"><a class="header-anchor" href="#_2-2-fixed-q-target" aria-hidden="true">#</a> 2.2 Fixed Q-Target</h3><p>TD error（或称 loss）：TD target（Q-Target）和当前 Q-value（estimation of Q）的差。</p><p>但是我们不知道实际的 TD target 是多少。我们需要对其进行估计。使用 Bellman equation，TD target 就是在该状态下采取该行动获得的 reward 加上下一状态的最高 Q value 的折扣值。</p><figure><img src="'+e+'" alt="" width="600" tabindex="0"><figcaption></figcaption></figure><p>问题在于，我们用于估计 TD Target 和 Q-value 的参数（神经网络权重）是一样的。于是，TD 和 Q-value 之间存在显著的相关性。在训练的每一步，权重的更新既会使 Q-value 更接近 TD Target，但也会导致目标值本身发生变化。这就像是在追逐一个不断移动的目标。这个问题会导致在训练过程中出现显著的振荡。因为 Q-value 和 TD Target 在不断相互影响，它们可能在每次训练迭代中都在变化，这会导致训练的不稳定性，使得 agent 难以稳定地学习。</p><p>在伪代码中可以看到：</p><ul><li>使用的单独的参数固定的网络来估计 TD Target</li><li>每 C 个 step 从 Deep Q-Network 中复制参数来更新 target network</li></ul><figure><img src="'+g+'" alt="" width="600" tabindex="0"><figcaption></figcaption></figure><h3 id="_2-3-double-deep-q-learning" tabindex="-1"><a class="header-anchor" href="#_2-3-double-deep-q-learning" aria-hidden="true">#</a> 2.3 Double Deep Q-Learning</h3><p>Double DQNs (或 Double Deep Q-Learning neural networks) 是为了解决 overestimation of Q-values。</p><p>计算 TD 目标时，我们如何确定执行哪个行动会得到最高的 Q-value。尤其是训练开始的时候，没有足够多的信息，以最大的 Q-value 来确定最优 action 可能会导致 false positive。如果非最优的 actions 规律性地高于最优 actions 的Q值，学习将变得复杂。</p><p>解决的方案是，使用两个网络来计算 Q target：</p><ul><li>使用 DQN network 选择最佳 action (有最高 Q-value 的 action)</li><li>使用 Target network 计算执行该行动后的目标 Q-value</li></ul>',26);function b(Q,x){return n(),i("div",null,[u,d,m(" 我们将使用 [RL-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) 来训练 agent 玩 Space Invaders 和其他 Atari 游戏。 "),v,y,f])}const z=l(h,[["render",b],["__file","deep-q-learning.html.vue"]]);export{z as default};
