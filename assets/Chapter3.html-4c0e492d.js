import{_ as c}from"./plugin-vue_export-helper-c27b6911.js";import{r as p,o as i,c as l,b as n,d as s,e as a,w as u,f as e}from"./app-1e7cdb66.js";const r={},d=e('<p>上一篇文章中介绍了如何使用 tokenizer 和预训练模型来进行推理。接下来我们将介绍如何在自己的数据集上进行微调（Fine-tuning）。在本篇文章中，你将了解到：</p><ul><li>如何从 Hub 中准备大型数据集</li><li>如何使用 high-level API 微调模型</li><li>如何使用自定义训练过程</li><li>如何利用 🤗 Accelerate 库在任何分布式设备上轻松运行自定义训练过程</li></ul><h2 id="_1-处理数据" tabindex="-1"><a class="header-anchor" href="#_1-处理数据" aria-hidden="true">#</a> 1. 处理数据</h2><div class="hint-container note"><p class="hint-container-title">注</p><p>如果你不想了解这些细节，或者想先运行数据处理的整体代码，请直接从 <a href="#_2-%E4%BD%BF%E7%94%A8-trainer-api-%E6%88%96-keras-%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83">2</a> 开始阅读</p></div>',4),k={href:"https://huggingface.co/datasets",target:"_blank",rel:"noopener noreferrer"},m={href:"https://aclanthology.org/I05-5002.pdf",target:"_blank",rel:"noopener noreferrer"},v={href:"https://gluebenchmark.com/",target:"_blank",rel:"noopener noreferrer"},b=e(`<h3 id="_1-1-从-hub-中加载数据集" tabindex="-1"><a class="header-anchor" href="#_1-1-从-hub-中加载数据集" aria-hidden="true">#</a> 1.1 从 Hub 中加载数据集</h3><p>🤗 Datasets 库提供了简单易用的命令来下载并缓存 Hub 中的数据集</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset

raw_datasets <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
raw_datasets
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code>DatasetDict<span class="token punctuation">(</span><span class="token punctuation">{</span>
    train<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">3668</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    validation<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">408</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    test<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">1725</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre></div><p>我们得到了一个 <code>DatasetDict</code> 对象，它有 training set, validation set, 和 test set。每一个集合中包含这样几列：sentence1、sentence2、label、idx，以及行数（即数据数量）。</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>缓存路径为 <code>~/.cache/huggingface/datasets</code> 你可以通过设置 <code>HF_HOME</code> 环境变量来自定义缓存路径。</p></div><p>你可以先看看数据：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>raw_train_dataset <span class="token operator">=</span> raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span>
raw_train_dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">:</span> <span class="token string">&#39;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#39;</span><span class="token punctuation">,</span>
 <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">:</span> <span class="token string">&#39;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#39;</span><span class="token punctuation">,</span>
 <span class="token string">&#39;label&#39;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
 <span class="token string">&#39;idx&#39;</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span>
</code></pre></div><p>可以通过查看 raw_train_dataset 的 <code>features</code> 来查看 label 的含义。0 是 not_equivalent，1 是 equivalent。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>raw_train_dataset<span class="token punctuation">.</span>features
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>{&#39;sentence1&#39;: Value(dtype=&#39;string&#39;, id=None),
 &#39;sentence2&#39;: Value(dtype=&#39;string&#39;, id=None),
 &#39;label&#39;: ClassLabel(names=[&#39;not_equivalent&#39;, &#39;equivalent&#39;], id=None),
 &#39;idx&#39;: Value(dtype=&#39;int32&#39;, id=None)}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_1-2-数据集预处理" tabindex="-1"><a class="header-anchor" href="#_1-2-数据集预处理" aria-hidden="true">#</a> 1.2 数据集预处理</h3><p>我们需要将文本转化成数字表示，这样模型才能进行处理。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
tokenized_sentences_1 <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tokenized_sentences_2 <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>上面的代码确实将文本转化成了数字表示，但是我们需要传入句子对</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">&quot;This is the first sentence.&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;This is the second one.&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>convert_ids_to_tokens<span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">&quot;input_ids&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2003</span><span class="token punctuation">,</span> <span class="token number">1996</span><span class="token punctuation">,</span> <span class="token number">2034</span><span class="token punctuation">,</span> <span class="token number">6251</span><span class="token punctuation">,</span> <span class="token number">1012</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2003</span><span class="token punctuation">,</span> <span class="token number">1996</span><span class="token punctuation">,</span> <span class="token number">2117</span><span class="token punctuation">,</span> <span class="token number">2028</span><span class="token punctuation">,</span> <span class="token number">1012</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
  <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
  <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
<span class="token punctuation">[</span><span class="token string">&#39;[CLS]&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;this&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;is&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;the&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;first&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;.&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;[SEP]&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;this&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;is&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;the&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;second&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;one&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;.&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;[SEP]&#39;</span><span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我们在上一篇文章中介绍了 input_ids 和 attention_mask，没有介绍 token_type_ids。在这个例子中，token_type_ids 表示输入的哪部分是第一个句子，哪一个是第二个句子。</p><p>我们可以看到模型需要的输入形式是 [CLS] sentence1 [SEP] sentence2 [SEP]（使用不同的 checkpoints 时该结构会不一样），所以 token_type_ids（使用其他的 checkpoints 时，可能不会有 token_type_ids） 的值是</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token punctuation">[</span><span class="token string">&#39;[CLS]&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;this&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;is&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;the&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;first&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;.&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;[SEP]&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;this&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;is&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;the&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;second&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;one&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;.&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;[SEP]&#39;</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span>      <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">0</span><span class="token punctuation">,</span>    <span class="token number">0</span><span class="token punctuation">,</span>     <span class="token number">0</span><span class="token punctuation">,</span>       <span class="token number">0</span><span class="token punctuation">,</span>          <span class="token number">0</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">,</span>       <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">1</span><span class="token punctuation">,</span>    <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>        <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>   <span class="token number">1</span><span class="token punctuation">,</span>       <span class="token number">1</span><span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>我们可以为 tokenizer 提供句子对列表</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>tokenized_dataset <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>
    raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这是有用的，但是也有一些不足。tokenization 过程中需要在 RAM 中保存整个数据集，如果你的 RAM 空间不足将会有问题。</p><p>我们使用 <code>Dataset.map()</code> 方法来构建数据集，它不会将整个 dataset 都加载到内存中，且结果会被缓存，下次执行时不需要重复计算。首先创建函数对输入进行 tokenization：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">tokenized_function</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>我们将 padding 参数去掉了，因为将所有的数据 padding 到最大长度效率不高，更好的做法是当我们构建一个 batch 时 pad 该 batch 中的数据，这样我们只需要将长度填充为该 batch 中的最大长度。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 设置 batched 为 True，使得同时对数据集中的多个元素同时做处理，加速了预处理</span>
tokenized_datasets <span class="token operator">=</span> raw_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenized_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
tokenized_datasets
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code>DatasetDict<span class="token punctuation">(</span><span class="token punctuation">{</span>
    train<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">3668</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    validation<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">408</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    test<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">1725</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre></div><p>🤗 Datasets 库用 map() 函数的处理方式是想数据集中添加新的字段，新的字段即预处理函数返回的字典中的每个键。</p><p>可以通过传递 <code>num_proc</code> 参数给 map() 以启动多进程。🤗 Tokenizers 库已经使用了多线程，于是这里我们没有启用多进程。</p><p>最后一项任务就是在每个 batch 进行 padding，即 dynamic padding.</p><h3 id="_1-3-动态填充-dynamic-padding" tabindex="-1"><a class="header-anchor" href="#_1-3-动态填充-dynamic-padding" aria-hidden="true">#</a> 1.3 动态填充（Dynamic Padding）</h3><p>在批处理中这将数据整理到一个 batch 的函数称为 collate function. 它是构建 DataLoader 时的一个参数，默认是一个函数，它把你的数据集转化为 Pytorch tensors，并将它们拼接起来。</p><p>🤗 Transformers 库通过 <code>DataCollatorWithPadding</code> 提供了 collate function。它接收一个 tokenizer (以获取 padding token、确定是在输入的左侧还是右侧进行 padding)。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> DataCollatorWithPadding

data_collator <span class="token operator">=</span> DataCollatorWithPadding<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><details class="hint-container details"><summary>我们可以验证一下 data_collator 是否能在 batch 上进行正确的 padding</summary><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>samples <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span>
samples <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> samples<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> k <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">&quot;idx&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> samples<span class="token punctuation">[</span><span class="token string">&quot;input_ids&quot;</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token comment"># [50, 59, 47, 67, 59, 50, 62, 32]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我们取了 train set 中前 8 个作为一个 batch，去掉了 idx、sentence1、sentence2 字段。</p><p>input_ids 的最大长度为 67，则这个 batch 经过 padding 之后将会被填充到 67</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>batch <span class="token operator">=</span> data_collator<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>
<span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>shape <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">67</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">67</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">67</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;labels&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre></div></details><p>现在，我们已经将原数数据转化成模型可处理的 batches，下面我们要进行微调了。</p><h2 id="_2-使用-trainer-api-进行微调" tabindex="-1"><a class="header-anchor" href="#_2-使用-trainer-api-进行微调" aria-hidden="true">#</a> 2. 使用 Trainer API 进行微调</h2><p>🤗 Transformers 提供了 <code>Trainer</code> 类来微调各种预训练模型。最难的步骤大概是为 <code>Trainer.train()</code> 配置运行环境。</p><p>我们快速回顾一下上一部分的预处理：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> DataCollatorWithPadding

raw_dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

tokenized_datasets <span class="token operator">=</span> raw_dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
data_collator <span class="token operator">=</span> DataCollatorWithPadding<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-1-训练-training" tabindex="-1"><a class="header-anchor" href="#_2-1-训练-training" aria-hidden="true">#</a> 2.1 训练（Training）</h3><p>第一步，在我们定义 <code>Trainer</code> 之前我们要先定义 <code>TrainingArguments</code> 类，它包含 <code>Trainer</code> 训练和评估时所用的全部超参。必须提供的唯一参数是训练模型的存储路径，也是 checkpoints 的路径。其余的参数都可以设置为默认值，对于基础的微调来说表现得也很不错。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments

training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span><span class="token string">&quot;test-trainer&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,45),g={class:"hint-container tip"},h=n("p",{class:"hint-container-title"},"提示",-1),_=e('<details class="hint-container details"><summary>🤗 官方示例 accelerate 版本错误解决方案</summary><p>在 CoLab 上运行 🤗 官方示例时，如果遇到下面的错误，</p><div class="language-text" data-ext="text"><pre class="language-text"><code>ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n</code></pre></div><p>可以尝试下面方法，首先更新 accelerate 和 transformers</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>!pip install -U accelerate\n!pip install -U transformers\n</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>然后 Restart runtime</p></details>',1),y=e(`<p>第二步，定义模型。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在实例化 model 时你会看到 warning，这是因为 BERT 没有对句子对进行过预训练，于是预训练模型的 head 被替换成了做 sequence classification 的 head。</p><p>现在我们可以定义 <code>Trainer</code> 了，将我们之前构造的对象（model, training_args, training &amp; validation datasets, data_collator 以及 tokenizer）作为参数传递。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> Trainer

trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token punctuation">,</span>
    training_args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    eval_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;validation&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    data_collator<span class="token operator">=</span>data_collator<span class="token punctuation">,</span>
    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">注意</p><p>当在 Trainer 中传递 tokenizer 时，Trainer 使用的默认 data_collator 和我们之前使用 DataCollatorWithPadding 定义的是一样的。所以我们可以不传递 data_collator。</p></div><p>调用 Trainer 的 <code>train()</code> 方法，我们就可以在自己的数据集上微调模型了。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>运行上面代码后，我们将开始微调，每 500 steps 会输出一次 training loss。但是它不会告诉你这个模型表现得怎么样，因为：</p><ul><li>我们没有配置 Trainer 让它在训练时进行评估。想要进行评估可以设置 <code>evaluation_strategy</code> 为 “steps”（每eval_steps 进行评估） 或 “epoch”（在每个 epoch 之后进行评估）。</li><li>我们没有为 Trainer 提供评估的方法。我们可以传递通过 <code>compute_metrics()</code> 函数提供计算模型性能的方法。没有提供该方法的话，评估时会直接输出 loss，并不直观。</li></ul><h3 id="_2-2-评估-evaluation" tabindex="-1"><a class="header-anchor" href="#_2-2-评估-evaluation" aria-hidden="true">#</a> 2.2 评估（Evaluation）</h3><p>我们来看一下如何构建 <code>compute_metrics()</code> 函数并在训练时使用它。</p><p>可以使用 <code>Trainer.predict()</code> 方法进行预测。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>predictions <span class="token operator">=</span> trainer<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;validation&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>predictions<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># (408, 2) (408,)</span>

<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment"># predictions.predictions 的输出是 logits，为了获得预测结果，可以将 logits 的最大值的取出</span>
preds <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>predictions<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>Trainer.predict()</code> 的输出是一个命名元祖，有三个字段：predictions, label_ids, 和 metrics。metrics 字段包含 loss、时间 metrics（预测用了多长时间，总计时长、平均时长）。如果我们自定义了 compute_metrics() 函数并传递给了 Trainer，那么该字段还会包括 compute_metrics() 函数返回的 metrics。</p>`,15),f={href:"https://github.com/huggingface/evaluate/",target:"_blank",rel:"noopener noreferrer"},q=n("code",null,"evaluate.load()",-1),w=n("code",null,"compute()",-1),x=e(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> evaluate

metric <span class="token operator">=</span> evaluate<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>predictions<span class="token operator">=</span>preds<span class="token punctuation">,</span> references<span class="token operator">=</span>predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;accuracy&#39;</span><span class="token punctuation">:</span> <span class="token number">0.8578431372549019</span><span class="token punctuation">,</span> <span class="token string">&#39;f1&#39;</span><span class="token punctuation">:</span> <span class="token number">0.8996539792387542</span><span class="token punctuation">}</span>
</code></pre></div><p>我们最终得到了 accuracy 和 f1。这是用来衡量 MRPC 的 metrics。</p><p>现在我们可以定义 <code>compute_metrics()</code> 函数了：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">compute_metrics</span><span class="token punctuation">(</span>eval_preds<span class="token punctuation">)</span><span class="token punctuation">:</span>
    metric <span class="token operator">=</span> evaluate<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
    logits<span class="token punctuation">,</span> labels <span class="token operator">=</span> eval_preds
    predictions <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>predictions<span class="token operator">=</span>predictions<span class="token punctuation">,</span> references<span class="token operator">=</span>labels<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果想要在每个 epoch 之后输出这些 metrics，我们可以在 Trainer 中传递 <code>compute_metrics()</code> 函数</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span><span class="token string">&quot;test-trainer&quot;</span><span class="token punctuation">,</span> evaluation_strategy<span class="token operator">=</span><span class="token string">&quot;epoch&quot;</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token punctuation">,</span>
    training_args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    eval_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;validation&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    data_collator<span class="token operator">=</span>data_collator<span class="token punctuation">,</span>
    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
    compute_metrics<span class="token operator">=</span>compute_metrics<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这次我们再执行 <code>trainer.train()</code> 时就会在每个 epoch 结束时输出 validation loss 和 metrics。</p><p><code>Trainer</code> 在多 GPU 和多 TPU 上开箱即用，且提供了很多配置项，比如通过配置 <code>fp16=True</code> 来启动 mixed-precision 训练。我们会在第 10 章介绍这些配置项。</p><h2 id="_3-使用-pytorch-训练" tabindex="-1"><a class="header-anchor" href="#_3-使用-pytorch-训练" aria-hidden="true">#</a> 3. 使用 Pytorch 训练</h2><p>在 2 中我们介绍了如何使用 <code>Trainer</code> 类进行微调。现在我们不使用 <code>Trainer</code> 来达到同样的目的。</p><p>数据预处理的方式和之前介绍的一样，我们假定你已经完成了这步。</p><details class="hint-container details"><summary>数据预处理</summary><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> DataCollatorWithPadding

raw_datasets <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>


tokenized_datasets <span class="token operator">=</span> raw_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
data_collator <span class="token operator">=</span> DataCollatorWithPadding<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></details><h3 id="_3-1-准备" tabindex="-1"><a class="header-anchor" href="#_3-1-准备" aria-hidden="true">#</a> 3.1 准备</h3><p>之前我们直接将 tokenized_datasets 传给 <code>Trainer</code> 让它自己处理，现在我们需要手动处理：</p><ul><li>tokenized_datasets 中的 sentence1, sentence2, idx 不是 model 需要的输入，需要删掉</li><li>将列 label 改为 labels</li><li>将 dataset 的格式设为 Pytorch tensor</li></ul><p>对应的代码：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>tokenized_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span>remove_columns<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;idx&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tokenized_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span>rename_column<span class="token punctuation">(</span><span class="token string">&quot;label&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;labels&quot;</span><span class="token punctuation">)</span>
tokenized_datasets<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">&quot;torch&quot;</span><span class="token punctuation">)</span>
tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>column_names
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">[</span><span class="token string">&#39;labels&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">]</span>
</code></pre></div><p>接下来在定义 training loop 之前，还要先定义几个对象：</p><h4 id="_3-1-1-数据加载器-dataloader-用于迭代批次" tabindex="-1"><a class="header-anchor" href="#_3-1-1-数据加载器-dataloader-用于迭代批次" aria-hidden="true">#</a> 3.1.1 数据加载器（dataloader）：用于迭代批次</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader

train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>
    tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>data_collator
<span class="token punctuation">)</span>
eval_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>
    tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;validation&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>data_collator
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><details class="hint-container details"><summary>快速检验下是否有错</summary><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
    <span class="token keyword">break</span>
<span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>shape <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;labels&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">76</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">76</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">76</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre></div></details><p>至此，数据预处理完成了。</p><h4 id="_3-1-2-model" tabindex="-1"><a class="header-anchor" href="#_3-1-2-model" aria-hidden="true">#</a> 3.1.2 model</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><details class="hint-container details"><summary>快速检验下是否有错</summary><p>我们将上面检验 dataloader 是否出错时使用的 batch 传递给 model</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>loss<span class="token punctuation">,</span> outputs<span class="token punctuation">.</span>logits<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code>tensor<span class="token punctuation">(</span><span class="token number">0.6617</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div></details><h4 id="_3-1-3-优化器-optimizer" tabindex="-1"><a class="header-anchor" href="#_3-1-3-优化器-optimizer" aria-hidden="true">#</a> 3.1.3 优化器（optimizer）</h4>`,28),z=n("code",null,"Trainer",-1),T={href:"https://arxiv.org/abs/1711.05101",target:"_blank",rel:"noopener noreferrer"},A=n("code",null,"AdamW",-1),P=n("code",null,"Adam",-1),C=e(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW

optimizer <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">5e-5</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_3-1-4-学习率调度器-learning-rate-scheduler" tabindex="-1"><a class="header-anchor" href="#_3-1-4-学习率调度器-learning-rate-scheduler" aria-hidden="true">#</a> 3.1.4 学习率调度器（learning rate scheduler）</h4><p>默认的 learning rate scheduler 实现的是简单的从 5e-5 到 0 的线性衰减。为了定义学习率调度器，我们需要知道要进行多少 training steps，即 epoch 乘 training batches（training dataloader 的长度）。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> get_scheduler

<span class="token comment"># Trainer 默认训练 3 轮</span>
num_epochs <span class="token operator">=</span> <span class="token number">3</span>
num_training_steps <span class="token operator">=</span> num_epochs <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>
lr_scheduler <span class="token operator">=</span> get_scheduler<span class="token punctuation">(</span>
    <span class="token string">&quot;linear&quot;</span><span class="token punctuation">,</span>
    optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
    num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    num_training_steps<span class="token operator">=</span>num_training_steps<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span>    <span class="token comment"># 1377</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-2-training-loop" tabindex="-1"><a class="header-anchor" href="#_3-2-training-loop" aria-hidden="true">#</a> 3.2 Training Loop</h3><p>我们可以设置 device 为 gpu 以让 model在 GPU 上运行：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>现在可以开始训练啦！为了让我们知道训练的进度，可以使用进度条（<code>tqdm</code> 库）。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> tqdm<span class="token punctuation">.</span>auto <span class="token keyword">import</span> tqdm

progress_bar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>number_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
        batch <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        progress_bar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>下面我们添加一些输出，以在训练过程中查看训练效果</p><h3 id="_3-3-evaluation-loop" tabindex="-1"><a class="header-anchor" href="#_3-3-evaluation-loop" aria-hidden="true">#</a> 3.3 Evaluation Loop</h3><p>我们仍然使用 🤗 Evaluate 库提供的 metric。之前我们用过 metric.compute() 方法了。在 prediction loop 中使用 add_batch() ，metrics 会跟着 batches 累积，当我们将全部 batch 的结果累积后就可以使用 metric.compute() 得到最后的结果。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> evaluate

metric <span class="token operator">=</span> evaluate<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> batch <span class="token keyword">in</span> eval_dataloader<span class="token punctuation">:</span>
    batch <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
    
    logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits
    predictions <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    metric<span class="token punctuation">.</span>add_batch<span class="token punctuation">(</span>predictions<span class="token operator">=</span>predictions<span class="token punctuation">,</span> references<span class="token operator">=</span>batch<span class="token punctuation">[</span><span class="token string">&quot;labels&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;accuracy&#39;</span><span class="token punctuation">:</span> <span class="token number">0.8431372549019608</span><span class="token punctuation">,</span> <span class="token string">&#39;f1&#39;</span><span class="token punctuation">:</span> <span class="token number">0.8907849829351535</span><span class="token punctuation">}</span>
</code></pre></div><h3 id="_3-4-使用-🤗-accelerate-进行加速" tabindex="-1"><a class="header-anchor" href="#_3-4-使用-🤗-accelerate-进行加速" aria-hidden="true">#</a> 3.4 使用 🤗 Accelerate 进行加速</h3><p>使用 🤗 Accelerate 我们可以在多个 GPU 或 TPU 上进行分布式训练。</p><p>我们在之前的代码上进行简单修改即可完成：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token operator">+</span> <span class="token keyword">from</span> accelerate <span class="token keyword">import</span> Accelerator
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> AutoModelForSequenceClassification<span class="token punctuation">,</span> get_scheduler

<span class="token operator">+</span> accelerator <span class="token operator">=</span> Accelerator<span class="token punctuation">(</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e-5</span><span class="token punctuation">)</span>

<span class="token operator">-</span> device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span>
<span class="token operator">-</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token operator">+</span> train_dataloader<span class="token punctuation">,</span> eval_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer <span class="token operator">=</span> accelerator<span class="token punctuation">.</span>prepare<span class="token punctuation">(</span>
<span class="token operator">+</span>     train_dataloader<span class="token punctuation">,</span> eval_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer
<span class="token operator">+</span> <span class="token punctuation">)</span>

num_epochs <span class="token operator">=</span> <span class="token number">3</span>
num_training_steps <span class="token operator">=</span> num_epochs <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>
lr_scheduler <span class="token operator">=</span> get_scheduler<span class="token punctuation">(</span>
    <span class="token string">&quot;linear&quot;</span><span class="token punctuation">,</span>
    optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
    num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    num_training_steps<span class="token operator">=</span>num_training_steps<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

progress_bar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
<span class="token operator">-</span>       batch <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
<span class="token operator">-</span>       loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">+</span>       accelerator<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
        

        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        progress_bar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><div class="highlight-lines"><div class="highlight-line"> </div><br><br><div class="highlight-line"> </div><br><br><br><br><div class="highlight-line"> </div><div class="highlight-line"> </div><br><div class="highlight-line"> </div><div class="highlight-line"> </div><div class="highlight-line"> </div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><div class="highlight-line"> </div><br><br><div class="highlight-line"> </div><div class="highlight-line"> </div><br><br><br><br><br><br></div><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>🤗 Accelerate 会帮你处理设备的问题，所以你可以删除 device 那段代码（你也可以使用 <code>accelerator.device</code> 来代替 <code>device</code>）。</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>为了充分利用集群 TPU 的加速，建议把所有的数据填充到固定的长度（配置 tokenizer 的 <code>padding=&quot;max_length&quot;</code>）。</p></div><details class="hint-container details"><summary>如果你要复制粘贴分布式训练的代码，请看这里</summary><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> accelerate <span class="token keyword">import</span> Accelerator
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> AutoModelForSequenceClassification<span class="token punctuation">,</span> get_scheduler

accelerator <span class="token operator">=</span> Accelerator<span class="token punctuation">(</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e-5</span><span class="token punctuation">)</span>

train_dl<span class="token punctuation">,</span> eval_dl<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer <span class="token operator">=</span> accelerator<span class="token punctuation">.</span>prepare<span class="token punctuation">(</span>
    train_dataloader<span class="token punctuation">,</span> eval_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer
<span class="token punctuation">)</span>

num_epochs <span class="token operator">=</span> <span class="token number">3</span>
num_training_steps <span class="token operator">=</span> num_epochs <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dl<span class="token punctuation">)</span>
lr_scheduler <span class="token operator">=</span> get_scheduler<span class="token punctuation">(</span>
    <span class="token string">&quot;linear&quot;</span><span class="token punctuation">,</span>
    optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
    num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    num_training_steps<span class="token operator">=</span>num_training_steps<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

progress_bar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
        accelerator<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        progress_bar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></details><p>将代码存到 train.py 中，该脚本可以在任何分布式设备上运行。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>accelerate config
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>回答弹出的问题，然后它会将你的答案写入配置文件中。然后你可以使用下面的命令使用该配置文件启动分布式训练。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>accelerate launch train.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>如果你想在 Notebook 中尝试，你把代码贴到函数下面（比如 <code>training_function()</code> ），然后在 cell 中执行：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> accelerate <span class="token keyword">import</span> notebook_launcher

notebook_launcher<span class="token punctuation">(</span>training_function<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,27),D={class:"hint-container info"},S=n("p",{class:"hint-container-title"},"更多示例",-1),E={href:"https://github.com/huggingface/accelerate/tree/main/examples",target:"_blank",rel:"noopener noreferrer"},L=n("h2",{id:"总结",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#总结","aria-hidden":"true"},"#"),s(" 总结")],-1),M=n("p",null,"在前两章中你了解了 model 和 tokenizer，现在你学会了如何微调。回顾本章：",-1),W=n("ul",null,[n("li",null,"在 Hub 中查看并下载 datasets"),n("li",null,"学会了如何加载、预处理数据集，包括动态填充和 collator"),n("li",null,"实现微调以及评估"),n("li",null,"较底层实现 training loop"),n("li",null,"使用 🤗 Accelerate 以在 GPU 集群或 TPU 集群上进行训练")],-1);function F(U,R){const t=p("ExternalLinkIcon"),o=p("RouterLink");return i(),l("div",null,[d,n("p",null,[s("Hub 中不仅有 models，还有很多 "),n("a",k,[s("datasets"),a(t)]),s(".")]),n("p",null,[s("我们将使用 "),n("a",m,[s("MRPC（Microsoft Research Paraphrase Corpus）数据集"),a(t)]),s("，它是 "),n("a",v,[s("GLUE benchmark"),a(t)]),s(" 的十个数据集之一，该 benchmark 用来衡量 ML 模型在 10 个不同文本分类任务中的性能。MRPC 数据集有 5801 个句子对，每个句子对有一个标签来指明两个句子是否同义。")]),b,n("div",g,[h,n("p",null,[s("如果你想在训练过程中自动上传你的模型到 Hub 上，可以在 TrainingArguments 中传递 push_to_hub=True。我们将在 "),a(o,{to:"/ai/huggingface-nlp/section1/Chapter4.html"},{default:u(()=>[s("Chapter 4")]),_:1}),s(" 中详细介绍。")]),_]),y,n("p",null,[s("构建 compute_metrics() 需要用到 "),n("a",f,[s("🤗 Evaluate 库"),a(t)]),s("。我们可以使用 "),q,s(" 函数加载与 MRPC 数据集有关的 metrics，它返回的对象有 "),w,s(" 方法，可以用来进行 metric calculation。")]),x,n("p",null,[s("我们使用 "),z,s(" 的默认 optimizer："),n("a",T,[A,a(t)]),s("，它和 "),P,s(" 类似，主要差异在于他们的权重衰减正则化（weight decay regularization）不同。")]),C,n("div",D,[S,n("p",null,[s("你可以在 "),n("a",E,[s("🤗 Accelerate repo"),a(t)]),s(" 中查看更多示例。")])]),L,M,W])}const H=c(r,[["render",F],["__file","Chapter3.html.vue"]]);export{H as default};
