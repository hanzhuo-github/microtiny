<!DOCTYPE html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.63" />
    <meta name="theme" content="VuePress Theme Hope" />
    <title>3. 微调预训练模型 | MicroTiny</title><meta name="description" content="记录日常工作学习生活的笔记">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-4dd79c28.css" as="style"><link rel="stylesheet" href="/assets/style-4dd79c28.css">
    <link rel="modulepreload" href="/assets/app-7864f323.js"><link rel="modulepreload" href="/assets/Chapter3.html-67fc6b65.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/assets/Chapter3.html-c9912e41.js"><link rel="prefetch" href="/assets/blog.html-ff026009.js" as="script"><link rel="prefetch" href="/assets/index.html-ad61850a.js" as="script"><link rel="prefetch" href="/assets/index.html-500a956e.js" as="script"><link rel="prefetch" href="/assets/index.html-b24c4b60.js" as="script"><link rel="prefetch" href="/assets/index.html-b3222f6c.js" as="script"><link rel="prefetch" href="/assets/introduction.html-6d427fbc.js" as="script"><link rel="prefetch" href="/assets/index.html-76cc9f73.js" as="script"><link rel="prefetch" href="/assets/metrics.html-8e11f4c5.js" as="script"><link rel="prefetch" href="/assets/Chapter1.html-b0ed1514.js" as="script"><link rel="prefetch" href="/assets/Chapter2.html-88f0b6a4.js" as="script"><link rel="prefetch" href="/assets/Chapter4.html-c19b065b.js" as="script"><link rel="prefetch" href="/assets/Chapter5.html-1bb72c8d.js" as="script"><link rel="prefetch" href="/assets/Chapter6.html-8f66fce2.js" as="script"><link rel="prefetch" href="/assets/404.html-f04ced3e.js" as="script"><link rel="prefetch" href="/assets/index.html-263523c9.js" as="script"><link rel="prefetch" href="/assets/index.html-7ffcc525.js" as="script"><link rel="prefetch" href="/assets/index.html-f6d32f3d.js" as="script"><link rel="prefetch" href="/assets/index.html-88930a72.js" as="script"><link rel="prefetch" href="/assets/index.html-19de02d9.js" as="script"><link rel="prefetch" href="/assets/index.html-2e547c56.js" as="script"><link rel="prefetch" href="/assets/index.html-71fc6e23.js" as="script"><link rel="prefetch" href="/assets/index.html-b8bdf50b.js" as="script"><link rel="prefetch" href="/assets/index.html-4ae32afb.js" as="script"><link rel="prefetch" href="/assets/index.html-45fd0fc4.js" as="script"><link rel="prefetch" href="/assets/index.html-115afd2d.js" as="script"><link rel="prefetch" href="/assets/index.html-82f875a5.js" as="script"><link rel="prefetch" href="/assets/index.html-b195f8b7.js" as="script"><link rel="prefetch" href="/assets/blog.html-4849eb2f.js" as="script"><link rel="prefetch" href="/assets/index.html-156d99af.js" as="script"><link rel="prefetch" href="/assets/index.html-da3b4957.js" as="script"><link rel="prefetch" href="/assets/index.html-cfe41128.js" as="script"><link rel="prefetch" href="/assets/index.html-facb6605.js" as="script"><link rel="prefetch" href="/assets/introduction.html-d78fb171.js" as="script"><link rel="prefetch" href="/assets/index.html-2feeb25b.js" as="script"><link rel="prefetch" href="/assets/metrics.html-7eb6e585.js" as="script"><link rel="prefetch" href="/assets/Chapter1.html-978b681c.js" as="script"><link rel="prefetch" href="/assets/Chapter2.html-87effc5f.js" as="script"><link rel="prefetch" href="/assets/Chapter4.html-0889106c.js" as="script"><link rel="prefetch" href="/assets/Chapter5.html-41c3e730.js" as="script"><link rel="prefetch" href="/assets/Chapter6.html-074d4cc2.js" as="script"><link rel="prefetch" href="/assets/404.html-5830ba13.js" as="script"><link rel="prefetch" href="/assets/index.html-6c83d58e.js" as="script"><link rel="prefetch" href="/assets/index.html-d36c4c60.js" as="script"><link rel="prefetch" href="/assets/index.html-8f11d9e7.js" as="script"><link rel="prefetch" href="/assets/index.html-f6298afb.js" as="script"><link rel="prefetch" href="/assets/index.html-349d556b.js" as="script"><link rel="prefetch" href="/assets/index.html-71d9e6f1.js" as="script"><link rel="prefetch" href="/assets/index.html-02bc51f2.js" as="script"><link rel="prefetch" href="/assets/index.html-7300494a.js" as="script"><link rel="prefetch" href="/assets/index.html-4934d068.js" as="script"><link rel="prefetch" href="/assets/index.html-3d74759e.js" as="script"><link rel="prefetch" href="/assets/index.html-bae533fd.js" as="script"><link rel="prefetch" href="/assets/index.html-8e155c00.js" as="script"><link rel="prefetch" href="/assets/index.html-27965cfe.js" as="script"><link rel="prefetch" href="/assets/browser-21db0a97.js" as="script"><link rel="prefetch" href="/assets/waline-meta-56fbc549.js" as="script"><link rel="prefetch" href="/assets/component-d894833a.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-5794cde2.js" as="script"><link rel="prefetch" href="/assets/pageview-1b945067.js" as="script"><link rel="prefetch" href="/assets/SearchResult-75655a2b.js" as="script"><link rel="prefetch" href="/assets/Card-49209644.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand" href="/"><!----><!----><span class="vp-site-name">MicroTiny</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a class="vp-link nav-link" href="/"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>首页<!----></a></div><div class="nav-item hide-in-mobile"><a class="vp-link nav-link" href="/blog.html"><span class="font-icon icon fa-fw fa-sm fas fa-blog" style=""></span>博客<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><div class="nav-item"><div class="dropdown-wrapper i18n-dropdown"><button type="button" class="dropdown-title" aria-label="选择语言"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a class="vp-link nav-link active" href="/ai/huggingface/section1/Chapter3.html"><!---->简体中文<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/en/"><!---->English<!----></a></li></ul></button></div></div><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/hanzhuo-github/hanzhuo-github.github.io" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" class="outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="search-pro-button" role="search" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><!----><a class="vp-link nav-link active vp-sidebar-title" href="/ai/huggingface/"><!---->Hugging Face<!----></a><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><!----><span class="vp-sidebar-title">👋	Hugging Face 初步</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><!--[--><a class="vp-link nav-link vp-sidebar-link vp-sidebar-page" href="/ai/huggingface/section1/Chapter1.html"><!---->1. Transformer Models<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a class="vp-link nav-link vp-sidebar-link vp-sidebar-page" href="/ai/huggingface/section1/Chapter2.html"><!---->2. 使用 🤗 Transformers<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a class="vp-link nav-link active vp-sidebar-link vp-sidebar-page active" href="/ai/huggingface/section1/Chapter3.html"><!---->3. 微调预训练模型<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_1-处理数据"><!---->1. 处理数据<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_1-1-从-hub-中加载数据集"><!---->1.1 从 Hub 中加载数据集<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_1-2-数据集预处理"><!---->1.2 数据集预处理<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_1-3-动态填充-dynamic-padding"><!---->1.3 动态填充（Dynamic Padding）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_2-使用-trainer-api-进行微调"><!---->2. 使用 Trainer API 进行微调<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_2-1-训练-training"><!---->2.1 训练（Training）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_2-2-评估-evaluation"><!---->2.2 评估（Evaluation）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_3-使用-pytorch-训练"><!---->3. 使用 Pytorch 训练<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_3-1-准备"><!---->3.1 准备<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_3-1-1-数据加载器-dataloader-用于迭代批次"><!---->3.1.1 数据加载器（dataloader）：用于迭代批次<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_3-1-2-model"><!---->3.1.2 model<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_3-1-3-优化器-optimizer"><!---->3.1.3 优化器（optimizer）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_3-1-4-学习率调度器-learning-rate-scheduler"><!---->3.1.4 学习率调度器（learning rate scheduler）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_3-2-training-loop"><!---->3.2 Training Loop<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_3-3-evaluation-loop"><!---->3.3 Evaluation Loop<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#_3-4-使用-🤗-accelerate-进行加速"><!---->3.4 使用 🤗 Accelerate 进行加速<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/ai/huggingface/section1/Chapter3.html#总结"><!---->总结<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul><!--]--></li><li><!--[--><a class="vp-link nav-link vp-sidebar-link vp-sidebar-page" href="/ai/huggingface/section1/Chapter4.html"><!---->4. 共享 Models 和 Tokenizers<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">👌 Hugging Face 深入</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">🤟 Hugging Face 高级</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><a class="vp-link nav-link vp-sidebar-title" href="/ai/theory/"><!---->理论<!----></a><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><a class="vp-link nav-link vp-sidebar-title" href="/ai/statisticalLearning/"><!---->ISL<!----></a><span class="vp-arrow end"></span></button><!----></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->3. 微调预训练模型</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Soul</span></span><span property="author" content="Soul"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2023-08-04T03:49:23.000Z"></span><span class="page-pageview-info" aria-label="访问量🔢" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon eye-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="eye icon"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 00-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z"></path></svg><span id="ArtalkPV" class="waline-pageview-count" data-path="/ai/huggingface/section1/Chapter3.html">...</span></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 13 分钟</span><meta property="timeRequired" content="PT13M"></span><!----><!----></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<!----></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/#_1-处理数据">1. 处理数据</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_1-1-从-hub-中加载数据集">1.1 从 Hub 中加载数据集</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_1-2-数据集预处理">1.2 数据集预处理</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_1-3-动态填充-dynamic-padding">1.3 动态填充（Dynamic Padding）</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/#_2-使用-trainer-api-进行微调">2. 使用 Trainer API 进行微调</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_2-1-训练-training">2.1 训练（Training）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_2-2-评估-evaluation">2.2 评估（Evaluation）</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/#_3-使用-pytorch-训练">3. 使用 Pytorch 训练</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_3-1-准备">3.1 准备</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/#_3-1-1-数据加载器-dataloader-用于迭代批次">3.1.1 数据加载器（dataloader）：用于迭代批次</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/#_3-1-2-model">3.1.2 model</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/#_3-1-3-优化器-optimizer">3.1.3 优化器（optimizer）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/#_3-1-4-学习率调度器-learning-rate-scheduler">3.1.4 学习率调度器（learning rate scheduler）</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_3-2-training-loop">3.2 Training Loop</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_3-3-evaluation-loop">3.3 Evaluation Loop</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_3-4-使用-🤗-accelerate-进行加速">3.4 使用 🤗 Accelerate 进行加速</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/#总结">总结</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><p>上一篇文章中介绍了如何使用 tokenizer 和预训练模型来进行推理。接下来我们将介绍如何在自己的数据集上进行微调（Fine-tuning）。在本篇文章中，你将了解到：</p><ul><li>如何从 Hub 中准备大型数据集</li><li>如何使用 high-level API 微调模型</li><li>如何使用自定义训练过程</li><li>如何利用 🤗 Accelerate 库在任何分布式设备上轻松运行自定义训练过程</li></ul><h2 id="_1-处理数据" tabindex="-1"><a class="header-anchor" href="#_1-处理数据" aria-hidden="true">#</a> 1. 处理数据</h2><div class="hint-container note"><p class="hint-container-title">注</p><p>如果你不想了解这些细节，或者想先运行数据处理的整体代码，请直接从 <a href="#_2-%E4%BD%BF%E7%94%A8-trainer-api-%E6%88%96-keras-%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83">2</a> 开始阅读</p></div><p>Hub 中不仅有 models，还有很多 <a href="https://huggingface.co/datasets" target="_blank" rel="noopener noreferrer">datasets<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p><p>我们将使用 <a href="https://aclanthology.org/I05-5002.pdf" target="_blank" rel="noopener noreferrer">MRPC（Microsoft Research Paraphrase Corpus）数据集<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，它是 <a href="https://gluebenchmark.com/" target="_blank" rel="noopener noreferrer">GLUE benchmark<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 的十个数据集之一，该 benchmark 用来衡量 ML 模型在 10 个不同文本分类任务中的性能。MRPC 数据集有 5801 个句子对，每个句子对有一个标签来指明两个句子是否同义。</p><h3 id="_1-1-从-hub-中加载数据集" tabindex="-1"><a class="header-anchor" href="#_1-1-从-hub-中加载数据集" aria-hidden="true">#</a> 1.1 从 Hub 中加载数据集</h3><p>🤗 Datasets 库提供了简单易用的命令来下载并缓存 Hub 中的数据集</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset

raw_datasets <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
raw_datasets
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code>DatasetDict<span class="token punctuation">(</span><span class="token punctuation">{</span>
    train<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">3668</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    validation<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">408</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    test<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">1725</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre></div><p>我们得到了一个 <code>DatasetDict</code> 对象，它有 training set, validation set, 和 test set。每一个集合中包含这样几列：sentence1、sentence2、label、idx，以及行数（即数据数量）。</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>缓存路径为 ~/.cache/huggingface/datasets 你可以通过设置 <code>HF_HOME</code> 环境变量来自定义缓存路径。</p></div><p>你可以先看看数据：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>raw_train_dataset <span class="token operator">=</span> raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span>
raw_train_dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">:</span> <span class="token string">&#39;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#39;</span><span class="token punctuation">,</span>
 <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">:</span> <span class="token string">&#39;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#39;</span><span class="token punctuation">,</span>
 <span class="token string">&#39;label&#39;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
 <span class="token string">&#39;idx&#39;</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span>
</code></pre></div><p>可以通过查看 raw_train_dataset 的 <code>features</code> 来查看 label 的含义。0 是 not_equivalent，1 是 equivalent。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>raw_train_dataset<span class="token punctuation">.</span>features
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>{&#39;sentence1&#39;: Value(dtype=&#39;string&#39;, id=None),
 &#39;sentence2&#39;: Value(dtype=&#39;string&#39;, id=None),
 &#39;label&#39;: ClassLabel(names=[&#39;not_equivalent&#39;, &#39;equivalent&#39;], id=None),
 &#39;idx&#39;: Value(dtype=&#39;int32&#39;, id=None)}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_1-2-数据集预处理" tabindex="-1"><a class="header-anchor" href="#_1-2-数据集预处理" aria-hidden="true">#</a> 1.2 数据集预处理</h3><p>我们需要将文本转化成数字表示，这样模型才能进行处理。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
tokenized_sentences_1 <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tokenized_sentences_2 <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>上面的代码确实将文本转化成了数字表示，但是我们需要传入句子对</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">&quot;This is the first sentence.&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;This is the second one.&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>convert_ids_to_tokens<span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">&quot;input_ids&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2003</span><span class="token punctuation">,</span> <span class="token number">1996</span><span class="token punctuation">,</span> <span class="token number">2034</span><span class="token punctuation">,</span> <span class="token number">6251</span><span class="token punctuation">,</span> <span class="token number">1012</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2003</span><span class="token punctuation">,</span> <span class="token number">1996</span><span class="token punctuation">,</span> <span class="token number">2117</span><span class="token punctuation">,</span> <span class="token number">2028</span><span class="token punctuation">,</span> <span class="token number">1012</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
  <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
  <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
<span class="token punctuation">[</span><span class="token string">&#39;[CLS]&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;this&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;is&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;the&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;first&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;.&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;[SEP]&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;this&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;is&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;the&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;second&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;one&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;.&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;[SEP]&#39;</span><span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我们在上一篇文章中介绍了 input_ids 和 attention_mask，没有介绍 token_type_ids。在这个例子中，token_type_ids 表示输入的哪部分是第一个句子，哪一个是第二个句子。</p><p>我们可以看到模型需要的输入形式是 [CLS] sentence1 [SEP] sentence2 [SEP]（使用不同的 checkpoints 时该结构会不一样），所以 token_type_ids（使用其他的 checkpoints 时，可能不会有 token_type_ids） 的值是</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token punctuation">[</span><span class="token string">&#39;[CLS]&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;this&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;is&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;the&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;first&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;.&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;[SEP]&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;this&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;is&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;the&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;second&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;one&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;.&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;[SEP]&#39;</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span>      <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">0</span><span class="token punctuation">,</span>    <span class="token number">0</span><span class="token punctuation">,</span>     <span class="token number">0</span><span class="token punctuation">,</span>       <span class="token number">0</span><span class="token punctuation">,</span>          <span class="token number">0</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">,</span>       <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">1</span><span class="token punctuation">,</span>    <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>        <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>   <span class="token number">1</span><span class="token punctuation">,</span>       <span class="token number">1</span><span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>我们可以为 tokenizer 提供句子对列表</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>tokenized_dataset <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>
    raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    raw_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这是有用的，但是也有一些不足。tokenization 过程中需要在 RAM 中保存整个数据集，如果你的 RAM 空间不足将会有问题。</p><p>我们使用 <code>Dataset.map()</code> 方法来构建数据集，它不会将整个 dataset 都加载到内存中，且结果会被缓存，下次执行时不需要重复计算。首先创建函数对输入进行 tokenization：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">tokenized_function</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>我们将 padding 参数去掉了，因为将所有的数据 padding 到最大长度效率不高，更好的做法是当我们构建一个 batch 时 pad 该 batch 中的数据，这样我们只需要将长度填充为该 batch 中的最大长度。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 设置 batched 为 True，使得同时对数据集中的多个元素同时做处理，加速了预处理</span>
tokenized_datasets <span class="token operator">=</span> raw_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenized_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
tokenized_datasets
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code>DatasetDict<span class="token punctuation">(</span><span class="token punctuation">{</span>
    train<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">3668</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    validation<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">408</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    test<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">{</span>
        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sentence1&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;sentence2&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;label&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;idx&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        num_rows<span class="token punctuation">:</span> <span class="token number">1725</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre></div><p>🤗 Datasets 库用 map() 函数的处理方式是想数据集中添加新的字段，新的字段即预处理函数返回的字典中的每个键。</p><p>可以通过传递 <code>num_proc</code> 参数给 map() 以启动多进程。🤗 Tokenizers 库已经使用了多线程，于是这里我们没有启用多进程。</p><p>最后一项任务就是在每个 batch 进行 padding，即 dynamic padding.</p><h3 id="_1-3-动态填充-dynamic-padding" tabindex="-1"><a class="header-anchor" href="#_1-3-动态填充-dynamic-padding" aria-hidden="true">#</a> 1.3 动态填充（Dynamic Padding）</h3><p>在批处理中这将数据整理到一个 batch 的函数称为 collate function. 它是构建 DataLoader 时的一个参数，默认是一个函数，它把你的数据集转化为 Pytorch tensors，并将它们拼接起来。</p><p>🤗 Transformers 库通过 <code>DataCollatorWithPadding</code> 提供了 collate function。它接收一个 tokenizer (以获取 padding token、确定是在输入的左侧还是右侧进行 padding)。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> DataCollatorWithPadding

data_collator <span class="token operator">=</span> DataCollatorWithPadding<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><details class="hint-container details"><summary>我们可以验证一下 data_collator 是否能在 batch 上进行正确的 padding</summary><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>samples <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span>
samples <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> samples<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> k <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">&quot;idx&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> samples<span class="token punctuation">[</span><span class="token string">&quot;input_ids&quot;</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token comment"># [50, 59, 47, 67, 59, 50, 62, 32]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我们取了 train set 中前 8 个作为一个 batch，去掉了 idx、sentence1、sentence2 字段。</p><p>input_ids 的最大长度为 67，则这个 batch 经过 padding 之后将会被填充到 67</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>batch <span class="token operator">=</span> data_collator<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>
<span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>shape <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">67</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">67</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">67</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;labels&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre></div></details><p>现在，我们已经将原数数据转化成模型可处理的 batches，下面我们要进行微调了。</p><h2 id="_2-使用-trainer-api-进行微调" tabindex="-1"><a class="header-anchor" href="#_2-使用-trainer-api-进行微调" aria-hidden="true">#</a> 2. 使用 Trainer API 进行微调</h2><p>🤗 Transformers 提供了 <code>Trainer</code> 类来微调各种预训练模型。最难的步骤大概是为 <code>Trainer.train()</code> 配置运行环境。</p><p>我们快速回顾一下上一部分的预处理：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> DataCollatorWithPadding

raw_dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

tokenized_datasets <span class="token operator">=</span> raw_dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
data_collator <span class="token operator">=</span> DataCollatorWithPadding<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-1-训练-training" tabindex="-1"><a class="header-anchor" href="#_2-1-训练-training" aria-hidden="true">#</a> 2.1 训练（Training）</h3><p>第一步，在我们定义 <code>Trainer</code> 之前我们要先定义 <code>TrainingArguments</code> 类，它包含 <code>Trainer</code> 训练和评估时所用的全部超参。必须提供的唯一参数是训练模型的存储路径，也是 checkpoints 的路径。其余的参数都可以设置为默认值，对于基础的微调来说表现得也很不错。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments

training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span><span class="token string">&quot;test-trainer&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">提示</p><p>如果你想在训练过程中自动上传你的模型到 Hub 上，可以在 TrainingArguments 中传递 push_to_hub=True。我们将在 <a href="/ai/huggingface/section1/Chapter.html" class="">Chapter 4</a> 中详细介绍。</p><details class="hint-container details"><summary>🤗 官方示例 accelerate 版本错误解决方案</summary><p>在 CoLab 上运行 🤗 官方示例时，如果遇到下面的错误，</p><div class="language-text" data-ext="text"><pre class="language-text"><code>ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`
</code></pre></div><p>可以尝试下面方法，首先更新 accelerate 和 transformers</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>!pip install -U accelerate
!pip install -U transformers
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>然后 Restart runtime</p></details></div><p>第二步，定义模型。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在实例化 model 时你会看到 warning，这是因为 BERT 没有对句子对进行过预训练，于是预训练模型的 head 被替换成了做 sequence classification 的 head。</p><p>现在我们可以定义 <code>Trainer</code> 了，将我们之前构造的对象（model, training_args, training &amp; validation datasets, data_collator 以及 tokenizer）作为参数传递。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> Trainer

trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token punctuation">,</span>
    training_args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    eval_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;validation&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    data_collator<span class="token operator">=</span>data_collator<span class="token punctuation">,</span>
    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">注意</p><p>当在 Trainer 中传递 tokenizer 时，Trainer 使用的默认 data_collator 和我们之前使用 DataCollatorWithPadding 定义的是一样的。所以我们可以不传递 data_collator。</p></div><p>调用 Trainer 的 <code>train()</code> 方法，我们就可以在自己的数据集上微调模型了。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>运行上面代码后，我们将开始微调，每 500 steps 会输出一次 training loss。但是它不会告诉你这个模型表现得怎么样，因为：</p><ul><li>我们没有配置 Trainer 让它在训练时进行评估。想要进行评估可以设置 <code>evaluation_strategy</code> 为 “steps”（每eval_steps 进行评估） 或 “epoch”（在每个 epoch 之后进行评估）。</li><li>我们没有为 Trainer 提供评估的方法。我们可以传递通过 <code>compute_metrics()</code> 函数提供计算模型性能的方法。没有提供该方法的话，评估时会直接输出 loss，并不直观。</li></ul><h3 id="_2-2-评估-evaluation" tabindex="-1"><a class="header-anchor" href="#_2-2-评估-evaluation" aria-hidden="true">#</a> 2.2 评估（Evaluation）</h3><p>我们来看一下如何构建 <code>compute_metrics()</code> 函数并在训练时使用它。</p><p>可以使用 <code>Trainer.predict()</code> 方法进行预测。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>predictions <span class="token operator">=</span> trainer<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;validation&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>predictions<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># (408, 2) (408,)</span>

<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment"># predictions.predictions 的输出是 logits，为了获得预测结果，可以将 logits 的最大值的取出</span>
preds <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>predictions<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>Trainer.predict()</code> 的输出是一个命名元祖，有三个字段：predictions, label_ids, 和 metrics。metrics 字段包含 loss、时间 metrics（预测用了多长时间，总计时长、平均时长）。如果我们自定义了 compute_metrics() 函数并传递给了 Trainer，那么该字段还会包括 compute_metrics() 函数返回的 metrics。</p><p>构建 compute_metrics() 需要用到 <a href="https://github.com/huggingface/evaluate/" target="_blank" rel="noopener noreferrer">🤗 Evaluate 库<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。我们可以使用 <code>evaluate.load()</code> 函数加载与 MRPC 数据集有关的 metrics，它返回的对象有 <code>compute()</code> 方法，可以用来进行 metric calculation。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> evaluate

metric <span class="token operator">=</span> evaluate<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>predictions<span class="token operator">=</span>preds<span class="token punctuation">,</span> references<span class="token operator">=</span>predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;accuracy&#39;</span><span class="token punctuation">:</span> <span class="token number">0.8578431372549019</span><span class="token punctuation">,</span> <span class="token string">&#39;f1&#39;</span><span class="token punctuation">:</span> <span class="token number">0.8996539792387542</span><span class="token punctuation">}</span>
</code></pre></div><p>我们最终得到了 accuracy 和 f1。这是用来衡量 MRPC 的 metrics。</p><p>现在我们可以定义 <code>compute_metrics()</code> 函数了：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">compute_metrics</span><span class="token punctuation">(</span>eval_preds<span class="token punctuation">)</span><span class="token punctuation">:</span>
    metric <span class="token operator">=</span> evaluate<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
    logits<span class="token punctuation">,</span> labels <span class="token operator">=</span> eval_preds
    predictions <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>predictions<span class="token operator">=</span>predictions<span class="token punctuation">,</span> references<span class="token operator">=</span>labels<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果想要在每个 epoch 之后输出这些 metrics，我们可以在 Trainer 中传递 <code>compute_metrics()</code> 函数</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span><span class="token string">&quot;test-trainer&quot;</span><span class="token punctuation">,</span> evaluation_strategy<span class="token operator">=</span><span class="token string">&quot;epoch&quot;</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token punctuation">,</span>
    training_args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    eval_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;validation&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    data_collator<span class="token operator">=</span>data_collator<span class="token punctuation">,</span>
    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
    compute_metrics<span class="token operator">=</span>compute_metrics<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这次我们再执行 <code>trainer.train()</code> 时就会在每个 epoch 结束时输出 validation loss 和 metrics。</p><p><code>Trainer</code> 在多 GPU 和多 TPU 上开箱即用，且提供了很多配置项，比如通过配置 <code>fp16=True</code> 来启动 mixed-precision 训练。我们会在第 10 章介绍这些配置项。</p><h2 id="_3-使用-pytorch-训练" tabindex="-1"><a class="header-anchor" href="#_3-使用-pytorch-训练" aria-hidden="true">#</a> 3. 使用 Pytorch 训练</h2><p>在 2 中我们介绍了如何使用 <code>Trainer</code> 类进行微调。现在我们不使用 <code>Trainer</code> 来达到同样的目的。</p><p>数据预处理的方式和之前介绍的一样，我们假定你已经完成了这步。</p><details class="hint-container details"><summary>数据预处理</summary><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> DataCollatorWithPadding

raw_datasets <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
checkpoint <span class="token operator">=</span> <span class="token string">&quot;bert-base-uncased&quot;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>


tokenized_datasets <span class="token operator">=</span> raw_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
data_collator <span class="token operator">=</span> DataCollatorWithPadding<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></details><h3 id="_3-1-准备" tabindex="-1"><a class="header-anchor" href="#_3-1-准备" aria-hidden="true">#</a> 3.1 准备</h3><p>之前我们直接将 tokenized_datasets 传给 <code>Trainer</code> 让它自己处理，现在我们需要手动处理：</p><ul><li>tokenized_datasets 中的 sentence1, sentence2, idx 不是 model 需要的输入，需要删掉</li><li>将列 label 改为 labels</li><li>将 dataset 的格式设为 Pytorch tensor</li></ul><p>对应的代码：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>tokenized_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span>remove_columns<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;sentence1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;sentence2&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;idx&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tokenized_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span>rename_column<span class="token punctuation">(</span><span class="token string">&quot;label&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;labels&quot;</span><span class="token punctuation">)</span>
tokenized_datasets<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">&quot;torch&quot;</span><span class="token punctuation">)</span>
tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>column_names
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">[</span><span class="token string">&#39;labels&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">]</span>
</code></pre></div><p>接下来在定义 training loop 之前，还要先定义几个对象：</p><h4 id="_3-1-1-数据加载器-dataloader-用于迭代批次" tabindex="-1"><a class="header-anchor" href="#_3-1-1-数据加载器-dataloader-用于迭代批次" aria-hidden="true">#</a> 3.1.1 数据加载器（dataloader）：用于迭代批次</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader

train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>
    tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>data_collator
<span class="token punctuation">)</span>
eval_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>
    tokenized_datasets<span class="token punctuation">[</span><span class="token string">&quot;validation&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>data_collator
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><details class="hint-container details"><summary>快速检验下是否有错</summary><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
    <span class="token keyword">break</span>
<span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>shape <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;labels&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">76</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;token_type_ids&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">76</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">76</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre></div></details><p>至此，数据预处理完成了。</p><h4 id="_3-1-2-model" tabindex="-1"><a class="header-anchor" href="#_3-1-2-model" aria-hidden="true">#</a> 3.1.2 model</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><details class="hint-container details"><summary>快速检验下是否有错</summary><p>我们将上面检验 dataloader 是否出错时使用的 batch 传递给 model</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>loss<span class="token punctuation">,</span> outputs<span class="token punctuation">.</span>logits<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code>tensor<span class="token punctuation">(</span><span class="token number">0.6617</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div></details><h4 id="_3-1-3-优化器-optimizer" tabindex="-1"><a class="header-anchor" href="#_3-1-3-优化器-optimizer" aria-hidden="true">#</a> 3.1.3 优化器（optimizer）</h4><p>我们使用 <code>Trainer</code> 的默认 optimizer：<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer"><code>AdamW</code><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，它和 <code>Adam</code> 类似，主要差异在于他们的权重衰减正则化（weight decay regularization）不同。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW

optimizer <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">5e-5</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_3-1-4-学习率调度器-learning-rate-scheduler" tabindex="-1"><a class="header-anchor" href="#_3-1-4-学习率调度器-learning-rate-scheduler" aria-hidden="true">#</a> 3.1.4 学习率调度器（learning rate scheduler）</h4><p>默认的 learning rate scheduler 实现的是简单的从 5e-5 到 0 的线性衰减。为了定义学习率调度器，我们需要知道要进行多少 training steps，即 epoch 乘 training batches（training dataloader 的长度）。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> get_scheduler

<span class="token comment"># Trainer 默认训练 3 轮</span>
num_epochs <span class="token operator">=</span> <span class="token number">3</span>
num_training_steps <span class="token operator">=</span> num_epochs <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>
lr_scheduler <span class="token operator">=</span> get_scheduler<span class="token punctuation">(</span>
    <span class="token string">&quot;linear&quot;</span><span class="token punctuation">,</span>
    optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
    num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    num_training_steps<span class="token operator">=</span>num_training_steps<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span>    <span class="token comment"># 1377</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-2-training-loop" tabindex="-1"><a class="header-anchor" href="#_3-2-training-loop" aria-hidden="true">#</a> 3.2 Training Loop</h3><p>我们可以设置 device 为 gpu 以让 model在 GPU 上运行：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>现在可以开始训练啦！为了让我们知道训练的进度，可以使用进度条（<code>tqdm</code> 库）。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> tqdm<span class="token punctuation">.</span>auto <span class="token keyword">import</span> tqdm

progress_bar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>number_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
        batch <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        progress_bar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>下面我们添加一些输出，以在训练过程中查看训练效果</p><h3 id="_3-3-evaluation-loop" tabindex="-1"><a class="header-anchor" href="#_3-3-evaluation-loop" aria-hidden="true">#</a> 3.3 Evaluation Loop</h3><p>我们仍然使用 🤗 Evaluate 库提供的 metric。之前我们用过 metric.compute() 方法了。在 prediction loop 中使用 add_batch() ，metrics 会跟着 batches 累积，当我们将全部 batch 的结果累积后就可以使用 metric.compute() 得到最后的结果。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> evaluate

metric <span class="token operator">=</span> evaluate<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&quot;glue&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;mrpc&quot;</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> batch <span class="token keyword">in</span> eval_dataloader<span class="token punctuation">:</span>
    batch <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
    
    logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits
    predictions <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    metric<span class="token punctuation">.</span>add_batch<span class="token punctuation">(</span>predictions<span class="token operator">=</span>predictions<span class="token punctuation">,</span> references<span class="token operator">=</span>batch<span class="token punctuation">[</span><span class="token string">&quot;labels&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python" data-ext="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;accuracy&#39;</span><span class="token punctuation">:</span> <span class="token number">0.8431372549019608</span><span class="token punctuation">,</span> <span class="token string">&#39;f1&#39;</span><span class="token punctuation">:</span> <span class="token number">0.8907849829351535</span><span class="token punctuation">}</span>
</code></pre></div><h3 id="_3-4-使用-🤗-accelerate-进行加速" tabindex="-1"><a class="header-anchor" href="#_3-4-使用-🤗-accelerate-进行加速" aria-hidden="true">#</a> 3.4 使用 🤗 Accelerate 进行加速</h3><p>使用 🤗 Accelerate 我们可以在多个 GPU 或 TPU 上进行分布式训练。</p><p>我们在之前的代码上进行简单修改即可完成：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token operator">+</span> <span class="token keyword">from</span> accelerate <span class="token keyword">import</span> Accelerator
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> AutoModelForSequenceClassification<span class="token punctuation">,</span> get_scheduler

<span class="token operator">+</span> accelerator <span class="token operator">=</span> Accelerator<span class="token punctuation">(</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e-5</span><span class="token punctuation">)</span>

<span class="token operator">-</span> device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span>
<span class="token operator">-</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token operator">+</span> train_dataloader<span class="token punctuation">,</span> eval_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer <span class="token operator">=</span> accelerator<span class="token punctuation">.</span>prepare<span class="token punctuation">(</span>
<span class="token operator">+</span>     train_dataloader<span class="token punctuation">,</span> eval_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer
<span class="token operator">+</span> <span class="token punctuation">)</span>

num_epochs <span class="token operator">=</span> <span class="token number">3</span>
num_training_steps <span class="token operator">=</span> num_epochs <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>
lr_scheduler <span class="token operator">=</span> get_scheduler<span class="token punctuation">(</span>
    <span class="token string">&quot;linear&quot;</span><span class="token punctuation">,</span>
    optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
    num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    num_training_steps<span class="token operator">=</span>num_training_steps<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

progress_bar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
<span class="token operator">-</span>       batch <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
<span class="token operator">-</span>       loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">+</span>       accelerator<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
        

        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        progress_bar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><div class="highlight-lines"><div class="highlight-line"> </div><br><br><div class="highlight-line"> </div><br><br><br><br><div class="highlight-line"> </div><div class="highlight-line"> </div><br><div class="highlight-line"> </div><div class="highlight-line"> </div><div class="highlight-line"> </div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><div class="highlight-line"> </div><br><br><div class="highlight-line"> </div><div class="highlight-line"> </div><br><br><br><br><br><br></div><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>🤗 Accelerate 会帮你处理设备的问题，所以你可以删除 device 那段代码（你也可以使用 <code>accelerator.device</code> 来代替 <code>device</code>）。</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>为了充分利用集群 TPU 的加速，建议把所有的数据填充到固定的长度（配置 tokenizer 的 <code>padding=&quot;max_length&quot;</code>）。</p></div><details class="hint-container details"><summary>如果你要复制粘贴分布式训练的代码，请看这里</summary><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> accelerate <span class="token keyword">import</span> Accelerator
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> AutoModelForSequenceClassification<span class="token punctuation">,</span> get_scheduler

accelerator <span class="token operator">=</span> Accelerator<span class="token punctuation">(</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e-5</span><span class="token punctuation">)</span>

train_dl<span class="token punctuation">,</span> eval_dl<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer <span class="token operator">=</span> accelerator<span class="token punctuation">.</span>prepare<span class="token punctuation">(</span>
    train_dataloader<span class="token punctuation">,</span> eval_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer
<span class="token punctuation">)</span>

num_epochs <span class="token operator">=</span> <span class="token number">3</span>
num_training_steps <span class="token operator">=</span> num_epochs <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dl<span class="token punctuation">)</span>
lr_scheduler <span class="token operator">=</span> get_scheduler<span class="token punctuation">(</span>
    <span class="token string">&quot;linear&quot;</span><span class="token punctuation">,</span>
    optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
    num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    num_training_steps<span class="token operator">=</span>num_training_steps<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

progress_bar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
        accelerator<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        progress_bar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></details><p>将代码存到 train.py 中，该脚本可以在任何分布式设备上运行。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>accelerate config
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>回答弹出的问题，然后它会将你的答案写入配置文件中。然后你可以使用下面的命令使用该配置文件启动分布式训练。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>accelerate launch train.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>如果你想在 Notebook 中尝试，你把代码贴到函数下面（比如 <code>training_function()</code> ），然后在 cell 中执行：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> accelerate <span class="token keyword">import</span> notebook_launcher

notebook_launcher<span class="token punctuation">(</span>training_function<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container info"><p class="hint-container-title">更多示例</p><p>你可以在 <a href="https://github.com/huggingface/accelerate/tree/main/examples" target="_blank" rel="noopener noreferrer">🤗 Accelerate repo<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 中查看更多示例。</p></div><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h2><p>在前两章中你了解了 model 和 tokenizer，现在你学会了如何微调。回顾本章：</p><ul><li>在 Hub 中查看并下载 datasets</li><li>学会了如何加载、预处理数据集，包括动态填充和 collator</li><li>实现微调以及评估</li><li>较底层实现 training loop</li><li>使用 🤗 Accelerate 以在 GPU 集群或 TPU 集群上进行训练</li></ul></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/hanzhuo-github/hanzhuo-github.github.io/edit/main/ai/huggingface/section1/Chapter3.md" rel="noopener noreferrer" target="_blank" aria-label="编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: hanzhuosoul@gmail.com">Sunshine</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="vp-link nav-link prev" href="/ai/huggingface/section1/Chapter2.html"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->2. 使用 🤗 Transformers</div></a><a class="vp-link nav-link next" href="/ai/huggingface/section1/Chapter4.html"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">4. 共享 Models 和 Tokenizers<!----></div></a></nav><div id="comment" class="waline-wrapper" darkmode="false" style="display:block;"><div data-waline provider="Waline"><!--v-if--><div class="wl-comment"><!--v-if--><div class="wl-panel"><div class="wl-header item3"><!--[--><div class="wl-header-item"><label for="wl-nick">昵称</label><input id="wl-nick" class="wl-input wl-nick" name="nick" type="text" value></div><div class="wl-header-item"><label for="wl-mail">邮箱</label><input id="wl-mail" class="wl-input wl-mail" name="mail" type="email" value></div><div class="wl-header-item"><label for="wl-link">网址</label><input id="wl-link" class="wl-input wl-link" name="link" type="text" value></div><!--]--></div><textarea id="wl-edit" class="wl-editor" placeholder="请留言。(填写邮箱可在被回复时收到邮件提醒)"></textarea><div class="wl-preview" style="display:none;"><hr><h4>预览:</h4><div class="wl-content"></div></div><div class="wl-footer"><div class="wl-actions"><a href="https://guides.github.com/features/mastering-markdown/" title="Markdown Guide" aria-label="Markdown is supported" class="wl-action" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" ariaHidden="true"><path d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z" fill="currentColor"></path></svg></a><button type="button" class="wl-action" title="表情" style="display:none;"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M563.2 463.3 677 540c1.7 1.2 3.7 1.8 5.8 1.8.7 0 1.4-.1 2-.2 2.7-.5 5.1-2.1 6.6-4.4l25.3-37.8c1.5-2.3 2.1-5.1 1.6-7.8s-2.1-5.1-4.4-6.6l-73.6-49.1 73.6-49.1c2.3-1.5 3.9-3.9 4.4-6.6.5-2.7 0-5.5-1.6-7.8l-25.3-37.8a10.1 10.1 0 0 0-6.6-4.4c-.7-.1-1.3-.2-2-.2-2.1 0-4.1.6-5.8 1.8l-113.8 76.6c-9.2 6.2-14.7 16.4-14.7 27.5.1 11 5.5 21.3 14.7 27.4zM387 348.8h-45.5c-5.7 0-10.4 4.7-10.4 10.4v153.3c0 5.7 4.7 10.4 10.4 10.4H387c5.7 0 10.4-4.7 10.4-10.4V359.2c0-5.7-4.7-10.4-10.4-10.4zm333.8 241.3-41-20a10.3 10.3 0 0 0-8.1-.5c-2.6.9-4.8 2.9-5.9 5.4-30.1 64.9-93.1 109.1-164.4 115.2-5.7.5-9.9 5.5-9.5 11.2l3.9 45.5c.5 5.3 5 9.5 10.3 9.5h.9c94.8-8 178.5-66.5 218.6-152.7 2.4-5 .3-11.2-4.8-13.6zm186-186.1c-11.9-42-30.5-81.4-55.2-117.1-24.1-34.9-53.5-65.6-87.5-91.2-33.9-25.6-71.5-45.5-111.6-59.2-41.2-14-84.1-21.1-127.8-21.1h-1.2c-75.4 0-148.8 21.4-212.5 61.7-63.7 40.3-114.3 97.6-146.5 165.8-32.2 68.1-44.3 143.6-35.1 218.4 9.3 74.8 39.4 145 87.3 203.3.1.2.3.3.4.5l36.2 38.4c1.1 1.2 2.5 2.1 3.9 2.6 73.3 66.7 168.2 103.5 267.5 103.5 73.3 0 145.2-20.3 207.7-58.7 37.3-22.9 70.3-51.5 98.1-85 27.1-32.7 48.7-69.5 64.2-109.1 15.5-39.7 24.4-81.3 26.6-123.8 2.4-43.6-2.5-87-14.5-129zm-60.5 181.1c-8.3 37-22.8 72-43 104-19.7 31.1-44.3 58.6-73.1 81.7-28.8 23.1-61 41-95.7 53.4-35.6 12.7-72.9 19.1-110.9 19.1-82.6 0-161.7-30.6-222.8-86.2l-34.1-35.8c-23.9-29.3-42.4-62.2-55.1-97.7-12.4-34.7-18.8-71-19.2-107.9-.4-36.9 5.4-73.3 17.1-108.2 12-35.8 30-69.2 53.4-99.1 31.7-40.4 71.1-72 117.2-94.1 44.5-21.3 94-32.6 143.4-32.6 49.3 0 97 10.8 141.8 32 34.3 16.3 65.3 38.1 92 64.8 26.1 26 47.5 56 63.6 89.2 16.2 33.2 26.6 68.5 31 105.1 4.6 37.5 2.7 75.3-5.6 112.3z" fill="currentColor"></path></svg></button><button type="button" class="wl-action" title="表情包"><svg width="24" height="24" fill="currentcolor" viewBox="0 0 24 24"><path style="transform: translateY(0.5px)" d="M18.968 10.5H15.968V11.484H17.984V12.984H15.968V15H14.468V9H18.968V10.5V10.5ZM8.984 9C9.26533 9 9.49967 9.09367 9.687 9.281C9.87433 9.46833 9.968 9.70267 9.968 9.984V10.5H6.499V13.5H8.468V12H9.968V14.016C9.968 14.2973 9.87433 14.5317 9.687 14.719C9.49967 14.9063 9.26533 15 8.984 15H5.984C5.70267 15 5.46833 14.9063 5.281 14.719C5.09367 14.5317 5 14.2973 5 14.016V9.985C5 9.70367 5.09367 9.46933 5.281 9.282C5.46833 9.09467 5.70267 9.001 5.984 9.001H8.984V9ZM11.468 9H12.968V15H11.468V9V9Z"></path><path d="M18.5 3H5.75C3.6875 3 2 4.6875 2 6.75V18C2 20.0625 3.6875 21.75 5.75 21.75H18.5C20.5625 21.75 22.25 20.0625 22.25 18V6.75C22.25 4.6875 20.5625 3 18.5 3ZM20.75 18C20.75 19.2375 19.7375 20.25 18.5 20.25H5.75C4.5125 20.25 3.5 19.2375 3.5 18V6.75C3.5 5.5125 4.5125 4.5 5.75 4.5H18.5C19.7375 4.5 20.75 5.5125 20.75 6.75V18Z"></path></svg></button><input id="wl-image-upload" class="upload" type="file" accept=".png,.jpg,.jpeg,.webp,.bmp,.gif"><label for="wl-image-upload" class="wl-action" title="上传图片"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M784 112H240c-88 0-160 72-160 160v480c0 88 72 160 160 160h544c88 0 160-72 160-160V272c0-88-72-160-160-160zm96 640c0 52.8-43.2 96-96 96H240c-52.8 0-96-43.2-96-96V272c0-52.8 43.2-96 96-96h544c52.8 0 96 43.2 96 96v480z" fill="currentColor"></path><path d="M352 480c52.8 0 96-43.2 96-96s-43.2-96-96-96-96 43.2-96 96 43.2 96 96 96zm0-128c17.6 0 32 14.4 32 32s-14.4 32-32 32-32-14.4-32-32 14.4-32 32-32zm462.4 379.2-3.2-3.2-177.6-177.6c-25.6-25.6-65.6-25.6-91.2 0l-80 80-36.8-36.8c-25.6-25.6-65.6-25.6-91.2 0L200 728c-4.8 6.4-8 14.4-8 24 0 17.6 14.4 32 32 32 9.6 0 16-3.2 22.4-9.6L380.8 640l134.4 134.4c6.4 6.4 14.4 9.6 24 9.6 17.6 0 32-14.4 32-32 0-9.6-4.8-17.6-9.6-24l-52.8-52.8 80-80L769.6 776c6.4 4.8 12.8 8 20.8 8 17.6 0 32-14.4 32-32 0-8-3.2-16-8-20.8z" fill="currentColor"></path></svg></label><button type="button" class="wl-action" title="预览"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M710.816 654.301c70.323-96.639 61.084-230.578-23.705-314.843-46.098-46.098-107.183-71.109-172.28-71.109-65.008 0-126.092 25.444-172.28 71.109-45.227 46.098-70.756 107.183-70.756 172.106 0 64.923 25.444 126.007 71.194 172.106 46.099 46.098 107.184 71.109 172.28 71.109 51.414 0 100.648-16.212 142.824-47.404l126.53 126.006c7.058 7.06 16.297 10.979 26.406 10.979 10.105 0 19.343-3.919 26.402-10.979 14.467-14.467 14.467-38.172 0-52.723L710.816 654.301zm-315.107-23.265c-65.88-65.88-65.88-172.54 0-238.42 32.069-32.07 74.245-49.149 119.471-49.149 45.227 0 87.407 17.603 119.472 49.149 65.88 65.879 65.88 172.539 0 238.42-63.612 63.178-175.242 63.178-238.943 0zm0 0" fill="currentColor"></path><path d="M703.319 121.603H321.03c-109.8 0-199.469 89.146-199.469 199.38v382.034c0 109.796 89.236 199.38 199.469 199.38h207.397c20.653 0 37.384-16.645 37.384-37.299 0-20.649-16.731-37.296-37.384-37.296H321.03c-68.582 0-124.352-55.77-124.352-124.267V321.421c0-68.496 55.77-124.267 124.352-124.267h382.289c68.582 0 124.352 55.771 124.352 124.267V524.72c0 20.654 16.736 37.299 37.385 37.299 20.654 0 37.384-16.645 37.384-37.299V320.549c-.085-109.8-89.321-198.946-199.121-198.946zm0 0" fill="currentColor"></path></svg></button></div><div class="wl-info"><div class="wl-captcha-container"></div><div class="wl-text-number">0 <!--v-if-->  字</div><button type="button" class="wl-btn">登录</button><button type="submit" class="primary wl-btn" title="Cmd|Ctrl + Enter"><!--[-->提交<!--]--></button></div><div class="wl-gif-popup"><input type="text" placeholder="搜索表情包"><!--v-if--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div></div><div class="wl-emoji-popup"><!--[--><!--]--><!--v-if--></div></div></div><!--v-if--></div><div class="wl-meta-head"><div class="wl-count"><!--v-if--> 评论</div><ul class="wl-sort"><!--[--><li class="active">按正序</li><li class="">按倒序</li><li class="">按热度</li><!--]--></ul></div><div class="wl-cards"><!--[--><!--]--></div><!--[--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div><!--]--><div class="wl-power"> Powered by <a href="https://github.com/walinejs/waline" target="_blank" rel="noopener noreferrer"> Waline </a> v2.15.5</div></div></div><!----><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!----><!--]--></div>
    <script type="module" src="/assets/app-7864f323.js" defer></script>
  </body>
</html>
