---
lang: zh-CN
title: 3. å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
description:
article: false
---

ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ tokenizer å’Œé¢„è®­ç»ƒæ¨¡å‹æ¥è¿›è¡Œæ¨ç†ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼ˆFine-tuningï¼‰ã€‚åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œä½ å°†äº†è§£åˆ°ï¼š

- å¦‚ä½•ä» Hub ä¸­å‡†å¤‡å¤§å‹æ•°æ®é›†
- å¦‚ä½•ä½¿ç”¨ high-level API å¾®è°ƒæ¨¡å‹
- å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹
- å¦‚ä½•åˆ©ç”¨ ğŸ¤— Accelerate åº“åœ¨ä»»ä½•åˆ†å¸ƒå¼è®¾å¤‡ä¸Šè½»æ¾è¿è¡Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹

## 1. å¤„ç†æ•°æ®

:::note
å¦‚æœä½ ä¸æƒ³äº†è§£è¿™äº›ç»†èŠ‚ï¼Œæˆ–è€…æƒ³å…ˆè¿è¡Œæ•°æ®å¤„ç†çš„æ•´ä½“ä»£ç ï¼Œè¯·ç›´æ¥ä» [2](#_2-ä½¿ç”¨-trainer-api-æˆ–-keras-è¿›è¡Œå¾®è°ƒ) å¼€å§‹é˜…è¯»
:::

Hub ä¸­ä¸ä»…æœ‰ modelsï¼Œè¿˜æœ‰å¾ˆå¤š [datasets](https://huggingface.co/datasets).

æˆ‘ä»¬å°†ä½¿ç”¨ [MRPCï¼ˆMicrosoft Research Paraphrase Corpusï¼‰æ•°æ®é›†](https://aclanthology.org/I05-5002.pdf)ï¼Œå®ƒæ˜¯ [GLUE benchmark](https://gluebenchmark.com/) çš„åä¸ªæ•°æ®é›†ä¹‹ä¸€ï¼Œè¯¥ benchmark ç”¨æ¥è¡¡é‡ ML æ¨¡å‹åœ¨ 10 ä¸ªä¸åŒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚MRPC æ•°æ®é›†æœ‰ 5801 ä¸ªå¥å­å¯¹ï¼Œæ¯ä¸ªå¥å­å¯¹æœ‰ä¸€ä¸ªæ ‡ç­¾æ¥æŒ‡æ˜ä¸¤ä¸ªå¥å­æ˜¯å¦åŒä¹‰ã€‚

### 1.1 ä» Hub ä¸­åŠ è½½æ•°æ®é›†
ğŸ¤— Datasets åº“æä¾›äº†ç®€å•æ˜“ç”¨çš„å‘½ä»¤æ¥ä¸‹è½½å¹¶ç¼“å­˜ Hub ä¸­çš„æ•°æ®é›†
```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```
```python:no-line-numbers
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ª `DatasetDict` å¯¹è±¡ï¼Œå®ƒæœ‰ training set, validation set, å’Œ test setã€‚æ¯ä¸€ä¸ªé›†åˆä¸­åŒ…å«è¿™æ ·å‡ åˆ—ï¼šsentence1ã€sentence2ã€labelã€idxï¼Œä»¥åŠè¡Œæ•°ï¼ˆå³æ•°æ®æ•°é‡ï¼‰ã€‚

:::tip
ç¼“å­˜è·¯å¾„ä¸º `~/.cache/huggingface/datasets` ä½ å¯ä»¥é€šè¿‡è®¾ç½® `HF_HOME` ç¯å¢ƒå˜é‡æ¥è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„ã€‚
:::

ä½ å¯ä»¥å…ˆçœ‹çœ‹æ•°æ®ï¼š
```python
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```
```python:no-line-numbers
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
 'label': 1,
 'idx': 0}
```

å¯ä»¥é€šè¿‡æŸ¥çœ‹ raw_train_dataset çš„ `features` æ¥æŸ¥çœ‹ label çš„å«ä¹‰ã€‚0 æ˜¯ not_equivalentï¼Œ1 æ˜¯ equivalentã€‚
```python
raw_train_dataset.features
```
```
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),
 'idx': Value(dtype='int32', id=None)}
```

### 1.2 æ•°æ®é›†é¢„å¤„ç†

æˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬è½¬åŒ–æˆæ•°å­—è¡¨ç¤ºï¼Œè¿™æ ·æ¨¡å‹æ‰èƒ½è¿›è¡Œå¤„ç†ã€‚

```python
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ä¸Šé¢çš„ä»£ç ç¡®å®å°†æ–‡æœ¬è½¬åŒ–æˆäº†æ•°å­—è¡¨ç¤ºï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦ä¼ å…¥å¥å­å¯¹
```python
inputs = tokenizer("This is the first sentence.", "This is the second one.")
print(inputs)

print(tokenizer.convert_ids_to_tokens(inputs["input_ids"]))
```

```python:no-line-numebrs
{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

æˆ‘ä»¬åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ä»‹ç»äº† input_ids å’Œ attention_maskï¼Œæ²¡æœ‰ä»‹ç» token_type_idsã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œtoken_type_ids è¡¨ç¤ºè¾“å…¥çš„å“ªéƒ¨åˆ†æ˜¯ç¬¬ä¸€ä¸ªå¥å­ï¼Œå“ªä¸€ä¸ªæ˜¯ç¬¬äºŒä¸ªå¥å­ã€‚


æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹éœ€è¦çš„è¾“å…¥å½¢å¼æ˜¯ [CLS] sentence1 [SEP] sentence2 [SEP]ï¼ˆä½¿ç”¨ä¸åŒçš„ checkpoints æ—¶è¯¥ç»“æ„ä¼šä¸ä¸€æ ·ï¼‰ï¼Œæ‰€ä»¥ token_type_idsï¼ˆä½¿ç”¨å…¶ä»–çš„ checkpoints æ—¶ï¼Œå¯èƒ½ä¸ä¼šæœ‰ token_type_idsï¼‰ çš„å€¼æ˜¯
```python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

æˆ‘ä»¬å¯ä»¥ä¸º tokenizer æä¾›å¥å­å¯¹åˆ—è¡¨
```python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

è¿™æ˜¯æœ‰ç”¨çš„ï¼Œä½†æ˜¯ä¹Ÿæœ‰ä¸€äº›ä¸è¶³ã€‚tokenization è¿‡ç¨‹ä¸­éœ€è¦åœ¨ RAM ä¸­ä¿å­˜æ•´ä¸ªæ•°æ®é›†ï¼Œå¦‚æœä½ çš„ RAM ç©ºé—´ä¸è¶³å°†ä¼šæœ‰é—®é¢˜ã€‚

æˆ‘ä»¬ä½¿ç”¨ `Dataset.map()` æ–¹æ³•æ¥æ„å»ºæ•°æ®é›†ï¼Œå®ƒä¸ä¼šå°†æ•´ä¸ª dataset éƒ½åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œä¸”ç»“æœä¼šè¢«ç¼“å­˜ï¼Œä¸‹æ¬¡æ‰§è¡Œæ—¶ä¸éœ€è¦é‡å¤è®¡ç®—ã€‚é¦–å…ˆåˆ›å»ºå‡½æ•°å¯¹è¾“å…¥è¿›è¡Œ tokenizationï¼š

```python
def tokenized_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

æˆ‘ä»¬å°† padding å‚æ•°å»æ‰äº†ï¼Œå› ä¸ºå°†æ‰€æœ‰çš„æ•°æ® padding åˆ°æœ€å¤§é•¿åº¦æ•ˆç‡ä¸é«˜ï¼Œæ›´å¥½çš„åšæ³•æ˜¯å½“æˆ‘ä»¬æ„å»ºä¸€ä¸ª batch æ—¶ pad è¯¥ batch ä¸­çš„æ•°æ®ï¼Œè¿™æ ·æˆ‘ä»¬åªéœ€è¦å°†é•¿åº¦å¡«å……ä¸ºè¯¥ batch ä¸­çš„æœ€å¤§é•¿åº¦ã€‚

```python
# è®¾ç½® batched ä¸º Trueï¼Œä½¿å¾—åŒæ—¶å¯¹æ•°æ®é›†ä¸­çš„å¤šä¸ªå…ƒç´ åŒæ—¶åšå¤„ç†ï¼ŒåŠ é€Ÿäº†é¢„å¤„ç†
tokenized_datasets = raw_datasets.map(tokenized_function, batched=True)
tokenized_datasets
```
```python:no-line-numbers
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 1725
    })
})
```

ğŸ¤— Datasets åº“ç”¨ map() å‡½æ•°çš„å¤„ç†æ–¹å¼æ˜¯æƒ³æ•°æ®é›†ä¸­æ·»åŠ æ–°çš„å­—æ®µï¼Œæ–°çš„å­—æ®µå³é¢„å¤„ç†å‡½æ•°è¿”å›çš„å­—å…¸ä¸­çš„æ¯ä¸ªé”®ã€‚

å¯ä»¥é€šè¿‡ä¼ é€’ `num_proc` å‚æ•°ç»™ map() ä»¥å¯åŠ¨å¤šè¿›ç¨‹ã€‚ğŸ¤— Tokenizers åº“å·²ç»ä½¿ç”¨äº†å¤šçº¿ç¨‹ï¼Œäºæ˜¯è¿™é‡Œæˆ‘ä»¬æ²¡æœ‰å¯ç”¨å¤šè¿›ç¨‹ã€‚

æœ€åä¸€é¡¹ä»»åŠ¡å°±æ˜¯åœ¨æ¯ä¸ª batch è¿›è¡Œ paddingï¼Œå³ dynamic padding.

### 1.3 åŠ¨æ€å¡«å……ï¼ˆDynamic Paddingï¼‰

åœ¨æ‰¹å¤„ç†ä¸­è¿™å°†æ•°æ®æ•´ç†åˆ°ä¸€ä¸ª batch çš„å‡½æ•°ç§°ä¸º collate function. å®ƒæ˜¯æ„å»º DataLoader æ—¶çš„ä¸€ä¸ªå‚æ•°ï¼Œé»˜è®¤æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæŠŠä½ çš„æ•°æ®é›†è½¬åŒ–ä¸º Pytorch tensorsï¼Œå¹¶å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥ã€‚

ğŸ¤— Transformers åº“é€šè¿‡ `DataCollatorWithPadding` æä¾›äº† collate functionã€‚å®ƒæ¥æ”¶ä¸€ä¸ª tokenizer (ä»¥è·å– padding tokenã€ç¡®å®šæ˜¯åœ¨è¾“å…¥çš„å·¦ä¾§è¿˜æ˜¯å³ä¾§è¿›è¡Œ padding)ã€‚

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

:::details æˆ‘ä»¬å¯ä»¥éªŒè¯ä¸€ä¸‹ data_collator æ˜¯å¦èƒ½åœ¨ batch ä¸Šè¿›è¡Œæ­£ç¡®çš„ padding
```python
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
# [50, 59, 47, 67, 59, 50, 62, 32]
```
æˆ‘ä»¬å–äº† train set ä¸­å‰ 8 ä¸ªä½œä¸ºä¸€ä¸ª batchï¼Œå»æ‰äº† idxã€sentence1ã€sentence2 å­—æ®µã€‚

input_ids çš„æœ€å¤§é•¿åº¦ä¸º 67ï¼Œåˆ™è¿™ä¸ª batch ç»è¿‡ padding ä¹‹åå°†ä¼šè¢«å¡«å……åˆ° 67

```python
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```
```python:no-line-numbers
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```
:::

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»å°†åŸæ•°æ•°æ®è½¬åŒ–æˆæ¨¡å‹å¯å¤„ç†çš„ batchesï¼Œä¸‹é¢æˆ‘ä»¬è¦è¿›è¡Œå¾®è°ƒäº†ã€‚

## 2. ä½¿ç”¨ Trainer API è¿›è¡Œå¾®è°ƒ

ğŸ¤— Transformers æä¾›äº† `Trainer` ç±»æ¥å¾®è°ƒå„ç§é¢„è®­ç»ƒæ¨¡å‹ã€‚æœ€éš¾çš„æ­¥éª¤å¤§æ¦‚æ˜¯ä¸º `Trainer.train()` é…ç½®è¿è¡Œç¯å¢ƒã€‚

æˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹ä¸Šä¸€éƒ¨åˆ†çš„é¢„å¤„ç†ï¼š
```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_dataset = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### 2.1 è®­ç»ƒï¼ˆTrainingï¼‰

ç¬¬ä¸€æ­¥ï¼Œåœ¨æˆ‘ä»¬å®šä¹‰ `Trainer` ä¹‹å‰æˆ‘ä»¬è¦å…ˆå®šä¹‰ `TrainingArguments` ç±»ï¼Œå®ƒåŒ…å« `Trainer` è®­ç»ƒå’Œè¯„ä¼°æ—¶æ‰€ç”¨çš„å…¨éƒ¨è¶…å‚ã€‚å¿…é¡»æä¾›çš„å”¯ä¸€å‚æ•°æ˜¯è®­ç»ƒæ¨¡å‹çš„å­˜å‚¨è·¯å¾„ï¼Œä¹Ÿæ˜¯ checkpoints çš„è·¯å¾„ã€‚å…¶ä½™çš„å‚æ•°éƒ½å¯ä»¥è®¾ç½®ä¸ºé»˜è®¤å€¼ï¼Œå¯¹äºåŸºç¡€çš„å¾®è°ƒæ¥è¯´è¡¨ç°å¾—ä¹Ÿå¾ˆä¸é”™ã€‚

```python
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

:::tip
å¦‚æœä½ æƒ³åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨ä¸Šä¼ ä½ çš„æ¨¡å‹åˆ° Hub ä¸Šï¼Œå¯ä»¥åœ¨ TrainingArguments ä¸­ä¼ é€’ push_to_hub=Trueã€‚æˆ‘ä»¬å°†åœ¨ [Chapter 4](Chapter4.md) ä¸­è¯¦ç»†ä»‹ç»ã€‚
:::details ğŸ¤— å®˜æ–¹ç¤ºä¾‹ accelerate ç‰ˆæœ¬é”™è¯¯è§£å†³æ–¹æ¡ˆ
åœ¨ CoLab ä¸Šè¿è¡Œ ğŸ¤— å®˜æ–¹ç¤ºä¾‹æ—¶ï¼Œå¦‚æœé‡åˆ°ä¸‹é¢çš„é”™è¯¯ï¼Œ
```:no-line-numbers
ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`
```

å¯ä»¥å°è¯•ä¸‹é¢æ–¹æ³•ï¼Œé¦–å…ˆæ›´æ–° accelerate å’Œ transformers
```
!pip install -U accelerate
!pip install -U transformers
```
ç„¶å Restart runtime
:::


ç¬¬äºŒæ­¥ï¼Œå®šä¹‰æ¨¡å‹ã€‚
```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

åœ¨å®ä¾‹åŒ– model æ—¶ä½ ä¼šçœ‹åˆ° warningï¼Œè¿™æ˜¯å› ä¸º BERT æ²¡æœ‰å¯¹å¥å­å¯¹è¿›è¡Œè¿‡é¢„è®­ç»ƒï¼Œäºæ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„ head è¢«æ›¿æ¢æˆäº†åš sequence classification çš„ headã€‚


ç°åœ¨æˆ‘ä»¬å¯ä»¥å®šä¹‰ `Trainer` äº†ï¼Œå°†æˆ‘ä»¬ä¹‹å‰æ„é€ çš„å¯¹è±¡ï¼ˆmodel, training_args, training & validation datasets, data_collator ä»¥åŠ tokenizerï¼‰ä½œä¸ºå‚æ•°ä¼ é€’ã€‚

```python
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

:::tip æ³¨æ„
å½“åœ¨ Trainer ä¸­ä¼ é€’ tokenizer æ—¶ï¼ŒTrainer ä½¿ç”¨çš„é»˜è®¤ data_collator å’Œæˆ‘ä»¬ä¹‹å‰ä½¿ç”¨ DataCollatorWithPadding å®šä¹‰çš„æ˜¯ä¸€æ ·çš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä¸ä¼ é€’ data_collatorã€‚
:::

è°ƒç”¨ Trainer çš„ `train()` æ–¹æ³•ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹äº†ã€‚
```python
trainer.train()
```

è¿è¡Œä¸Šé¢ä»£ç åï¼Œæˆ‘ä»¬å°†å¼€å§‹å¾®è°ƒï¼Œæ¯ 500 steps ä¼šè¾“å‡ºä¸€æ¬¡ training lossã€‚ä½†æ˜¯å®ƒä¸ä¼šå‘Šè¯‰ä½ è¿™ä¸ªæ¨¡å‹è¡¨ç°å¾—æ€ä¹ˆæ ·ï¼Œå› ä¸ºï¼š
- æˆ‘ä»¬æ²¡æœ‰é…ç½® Trainer è®©å®ƒåœ¨è®­ç»ƒæ—¶è¿›è¡Œè¯„ä¼°ã€‚æƒ³è¦è¿›è¡Œè¯„ä¼°å¯ä»¥è®¾ç½® `evaluation_strategy` ä¸º â€œstepsâ€ï¼ˆæ¯eval_steps è¿›è¡Œè¯„ä¼°ï¼‰ æˆ– â€œepochâ€ï¼ˆåœ¨æ¯ä¸ª epoch ä¹‹åè¿›è¡Œè¯„ä¼°ï¼‰ã€‚
- æˆ‘ä»¬æ²¡æœ‰ä¸º Trainer æä¾›è¯„ä¼°çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥ä¼ é€’é€šè¿‡ `compute_metrics()` å‡½æ•°æä¾›è®¡ç®—æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚æ²¡æœ‰æä¾›è¯¥æ–¹æ³•çš„è¯ï¼Œè¯„ä¼°æ—¶ä¼šç›´æ¥è¾“å‡º lossï¼Œå¹¶ä¸ç›´è§‚ã€‚

### 2.2 è¯„ä¼°ï¼ˆEvaluationï¼‰

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å¦‚ä½•æ„å»º `compute_metrics()` å‡½æ•°å¹¶åœ¨è®­ç»ƒæ—¶ä½¿ç”¨å®ƒã€‚

å¯ä»¥ä½¿ç”¨ `Trainer.predict()` æ–¹æ³•è¿›è¡Œé¢„æµ‹ã€‚
```python
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
# (408, 2) (408,)

import numpy as np
# predictions.predictions çš„è¾“å‡ºæ˜¯ logitsï¼Œä¸ºäº†è·å¾—é¢„æµ‹ç»“æœï¼Œå¯ä»¥å°† logits çš„æœ€å¤§å€¼çš„å–å‡º
preds = np.argmax(predictions.predictions, axis=-1)
```

`Trainer.predict()` çš„è¾“å‡ºæ˜¯ä¸€ä¸ªå‘½åå…ƒç¥–ï¼Œæœ‰ä¸‰ä¸ªå­—æ®µï¼špredictions, label_ids, å’Œ metricsã€‚metrics å­—æ®µåŒ…å« lossã€æ—¶é—´ metricsï¼ˆé¢„æµ‹ç”¨äº†å¤šé•¿æ—¶é—´ï¼Œæ€»è®¡æ—¶é•¿ã€å¹³å‡æ—¶é•¿ï¼‰ã€‚å¦‚æœæˆ‘ä»¬è‡ªå®šä¹‰äº† compute_metrics() å‡½æ•°å¹¶ä¼ é€’ç»™äº† Trainerï¼Œé‚£ä¹ˆè¯¥å­—æ®µè¿˜ä¼šåŒ…æ‹¬ compute_metrics() å‡½æ•°è¿”å›çš„ metricsã€‚

æ„å»º compute_metrics() éœ€è¦ç”¨åˆ° [ğŸ¤— Evaluate åº“](https://github.com/huggingface/evaluate/)ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `evaluate.load()` å‡½æ•°åŠ è½½ä¸ MRPC æ•°æ®é›†æœ‰å…³çš„ metricsï¼Œå®ƒè¿”å›çš„å¯¹è±¡æœ‰ `compute()` æ–¹æ³•ï¼Œå¯ä»¥ç”¨æ¥è¿›è¡Œ metric calculationã€‚

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```
```python:no-line-numbers
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº† accuracy å’Œ f1ã€‚è¿™æ˜¯ç”¨æ¥è¡¡é‡ MRPC çš„ metricsã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥å®šä¹‰ `compute_metrics()` å‡½æ•°äº†ï¼š

```python
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

å¦‚æœæƒ³è¦åœ¨æ¯ä¸ª epoch ä¹‹åè¾“å‡ºè¿™äº› metricsï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ Trainer ä¸­ä¼ é€’ `compute_metrics()` å‡½æ•°

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

è¿™æ¬¡æˆ‘ä»¬å†æ‰§è¡Œ `trainer.train()` æ—¶å°±ä¼šåœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¾“å‡º validation loss å’Œ metricsã€‚

`Trainer` åœ¨å¤š GPU å’Œå¤š TPU ä¸Šå¼€ç®±å³ç”¨ï¼Œä¸”æä¾›äº†å¾ˆå¤šé…ç½®é¡¹ï¼Œæ¯”å¦‚é€šè¿‡é…ç½® `fp16=True` æ¥å¯åŠ¨ mixed-precision è®­ç»ƒã€‚æˆ‘ä»¬ä¼šåœ¨ç¬¬ 10 ç« ä»‹ç»è¿™äº›é…ç½®é¡¹ã€‚


## 3. ä½¿ç”¨ Pytorch è®­ç»ƒ

åœ¨ 2 ä¸­æˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ `Trainer` ç±»è¿›è¡Œå¾®è°ƒã€‚ç°åœ¨æˆ‘ä»¬ä¸ä½¿ç”¨ `Trainer` æ¥è¾¾åˆ°åŒæ ·çš„ç›®çš„ã€‚

æ•°æ®é¢„å¤„ç†çš„æ–¹å¼å’Œä¹‹å‰ä»‹ç»çš„ä¸€æ ·ï¼Œæˆ‘ä»¬å‡å®šä½ å·²ç»å®Œæˆäº†è¿™æ­¥ã€‚

:::details æ•°æ®é¢„å¤„ç†
```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
:::

### 3.1 å‡†å¤‡

ä¹‹å‰æˆ‘ä»¬ç›´æ¥å°† tokenized_datasets ä¼ ç»™ `Trainer` è®©å®ƒè‡ªå·±å¤„ç†ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¤„ç†ï¼š
- tokenized_datasets ä¸­çš„ sentence1, sentence2, idx ä¸æ˜¯ model éœ€è¦çš„è¾“å…¥ï¼Œéœ€è¦åˆ æ‰
- å°†åˆ— label æ”¹ä¸º labels
- å°† dataset çš„æ ¼å¼è®¾ä¸º Pytorch tensor

å¯¹åº”çš„ä»£ç ï¼š
```python
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```
```python:no-line-numbers
['labels', 'input_ids', 'token_type_ids', 'attention_mask']
```

æ¥ä¸‹æ¥åœ¨å®šä¹‰ training loop ä¹‹å‰ï¼Œè¿˜è¦å…ˆå®šä¹‰å‡ ä¸ªå¯¹è±¡ï¼š

#### 3.1.1 æ•°æ®åŠ è½½å™¨ï¼ˆdataloaderï¼‰ï¼šç”¨äºè¿­ä»£æ‰¹æ¬¡
```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

:::details å¿«é€Ÿæ£€éªŒä¸‹æ˜¯å¦æœ‰é”™
```python
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```
```python:no-line-numbers
{'labels': torch.Size([8]),
 'input_ids': torch.Size([8, 76]),
 'token_type_ids': torch.Size([8, 76]),
 'attention_mask': torch.Size([8, 76])}
```
:::

è‡³æ­¤ï¼Œæ•°æ®é¢„å¤„ç†å®Œæˆäº†ã€‚

#### 3.1.2 model

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

:::details å¿«é€Ÿæ£€éªŒä¸‹æ˜¯å¦æœ‰é”™
æˆ‘ä»¬å°†ä¸Šé¢æ£€éªŒ dataloader æ˜¯å¦å‡ºé”™æ—¶ä½¿ç”¨çš„ batch ä¼ é€’ç»™ model
```python
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```
```python:no-line-numbers
tensor(0.6617, grad_fn=<NllLossBackward0>) torch.Size([8, 2])
```
:::

#### 3.1.3 ä¼˜åŒ–å™¨ï¼ˆoptimizerï¼‰

æˆ‘ä»¬ä½¿ç”¨ `Trainer` çš„é»˜è®¤ optimizerï¼š[`AdamW`](https://arxiv.org/abs/1711.05101)ï¼Œå®ƒå’Œ `Adam` ç±»ä¼¼ï¼Œä¸»è¦å·®å¼‚åœ¨äºä»–ä»¬çš„æƒé‡è¡°å‡æ­£åˆ™åŒ–ï¼ˆweight decay regularizationï¼‰ä¸åŒã€‚

```python
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

#### 3.1.4 å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆlearning rate schedulerï¼‰

é»˜è®¤çš„ learning rate scheduler å®ç°çš„æ˜¯ç®€å•çš„ä» 5e-5 åˆ° 0 çš„çº¿æ€§è¡°å‡ã€‚ä¸ºäº†å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“è¦è¿›è¡Œå¤šå°‘ training stepsï¼Œå³ epoch ä¹˜ training batchesï¼ˆtraining dataloader çš„é•¿åº¦ï¼‰ã€‚

```python
from transformers import get_scheduler

# Trainer é»˜è®¤è®­ç»ƒ 3 è½®
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)    # 1377
```

### 3.2 Training Loop

æˆ‘ä»¬å¯ä»¥è®¾ç½® device ä¸º gpu ä»¥è®© modelåœ¨ GPU ä¸Šè¿è¡Œï¼š

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒå•¦ï¼ä¸ºäº†è®©æˆ‘ä»¬çŸ¥é“è®­ç»ƒçš„è¿›åº¦ï¼Œå¯ä»¥ä½¿ç”¨è¿›åº¦æ¡ï¼ˆ`tqdm` åº“ï¼‰ã€‚

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(number_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

```

ä¸‹é¢æˆ‘ä»¬æ·»åŠ ä¸€äº›è¾“å‡ºï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æŸ¥çœ‹è®­ç»ƒæ•ˆæœ

### 3.3 Evaluation Loop

æˆ‘ä»¬ä»ç„¶ä½¿ç”¨ ğŸ¤— Evaluate åº“æä¾›çš„ metricã€‚ä¹‹å‰æˆ‘ä»¬ç”¨è¿‡ metric.compute() æ–¹æ³•äº†ã€‚åœ¨ prediction loop ä¸­ä½¿ç”¨ add_batch() ï¼Œmetrics ä¼šè·Ÿç€ batches ç´¯ç§¯ï¼Œå½“æˆ‘ä»¬å°†å…¨éƒ¨ batch çš„ç»“æœç´¯ç§¯åå°±å¯ä»¥ä½¿ç”¨ metric.compute() å¾—åˆ°æœ€åçš„ç»“æœã€‚

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)
    
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```
```python:no-line-numbers
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

### 3.4 ä½¿ç”¨ ğŸ¤— Accelerate è¿›è¡ŒåŠ é€Ÿ

ä½¿ç”¨ ğŸ¤— Accelerate æˆ‘ä»¬å¯ä»¥åœ¨å¤šä¸ª GPU æˆ– TPU ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚

æˆ‘ä»¬åœ¨ä¹‹å‰çš„ä»£ç ä¸Šè¿›è¡Œç®€å•ä¿®æ”¹å³å¯å®Œæˆï¼š

```python{1,4,9,10,12,13,14,30,33,34}
+ from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
-       batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
-       loss.backward()
+       accelerator.backward(loss)
        

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ğŸ¤— Accelerate ä¼šå¸®ä½ å¤„ç†è®¾å¤‡çš„é—®é¢˜ï¼Œæ‰€ä»¥ä½ å¯ä»¥åˆ é™¤ device é‚£æ®µä»£ç ï¼ˆä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ `accelerator.device` æ¥ä»£æ›¿ `device`ï¼‰ã€‚

:::tip
ä¸ºäº†å……åˆ†åˆ©ç”¨é›†ç¾¤ TPU çš„åŠ é€Ÿï¼Œå»ºè®®æŠŠæ‰€æœ‰çš„æ•°æ®å¡«å……åˆ°å›ºå®šçš„é•¿åº¦ï¼ˆé…ç½® tokenizer çš„ `padding="max_length"`ï¼‰ã€‚
:::

:::details å¦‚æœä½ è¦å¤åˆ¶ç²˜è´´åˆ†å¸ƒå¼è®­ç»ƒçš„ä»£ç ï¼Œè¯·çœ‹è¿™é‡Œ
```python
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```
:::

å°†ä»£ç å­˜åˆ° train.py ä¸­ï¼Œè¯¥è„šæœ¬å¯ä»¥åœ¨ä»»ä½•åˆ†å¸ƒå¼è®¾å¤‡ä¸Šè¿è¡Œã€‚

```shell
accelerate config
```

å›ç­”å¼¹å‡ºçš„é—®é¢˜ï¼Œç„¶åå®ƒä¼šå°†ä½ çš„ç­”æ¡ˆå†™å…¥é…ç½®æ–‡ä»¶ä¸­ã€‚ç„¶åä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤ä½¿ç”¨è¯¥é…ç½®æ–‡ä»¶å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒã€‚
```shell
accelerate launch train.py
```

å¦‚æœä½ æƒ³åœ¨ Notebook ä¸­å°è¯•ï¼Œä½ æŠŠä»£ç è´´åˆ°å‡½æ•°ä¸‹é¢ï¼ˆæ¯”å¦‚ `training_function()` ï¼‰ï¼Œç„¶ååœ¨ cell ä¸­æ‰§è¡Œï¼š
```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

:::info æ›´å¤šç¤ºä¾‹
ä½ å¯ä»¥åœ¨ [ğŸ¤— Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples) ä¸­æŸ¥çœ‹æ›´å¤šç¤ºä¾‹ã€‚
:::

## æ€»ç»“
åœ¨å‰ä¸¤ç« ä¸­ä½ äº†è§£äº† model å’Œ tokenizerï¼Œç°åœ¨ä½ å­¦ä¼šäº†å¦‚ä½•å¾®è°ƒã€‚å›é¡¾æœ¬ç« ï¼š
- åœ¨ Hub ä¸­æŸ¥çœ‹å¹¶ä¸‹è½½ datasets
- å­¦ä¼šäº†å¦‚ä½•åŠ è½½ã€é¢„å¤„ç†æ•°æ®é›†ï¼ŒåŒ…æ‹¬åŠ¨æ€å¡«å……å’Œ collator
- å®ç°å¾®è°ƒä»¥åŠè¯„ä¼°
- è¾ƒåº•å±‚å®ç° training loop
- ä½¿ç”¨ ğŸ¤— Accelerate ä»¥åœ¨ GPU é›†ç¾¤æˆ– TPU é›†ç¾¤ä¸Šè¿›è¡Œè®­ç»ƒ
