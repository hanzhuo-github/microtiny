---
lang: zh-CN
title: 2. ä½¿ç”¨ ğŸ¤— Transformers
article: false
---

Transformer æ¨¡å‹ä¸€èˆ¬éƒ½å¾ˆå¤§ï¼Œè®­ç»ƒæˆ–è€…éƒ¨ç½²æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ã€‚ğŸ¤— Transformers åº“æä¾›äº†ç®€å•çš„APIï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥é€šè¿‡å®ƒæ¥åŠ è½½ã€è®­ç»ƒã€ä¿å­˜æ‰€æœ‰çš„ Transformer æ¨¡å‹ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ model å’Œ tokenizer æ¥å®ç°åœ¨ä¸Šä¸€èŠ‚ä¸­ `pipeline()` å‡½æ•°å®Œæˆçš„ä»»åŠ¡ã€‚ç„¶åä¼šä»‹ç» model APIï¼Œçœ‹ä¸€çœ‹ model ç±»å’Œ configuration ç±»ï¼Œäº†è§£å¦‚ä½•åŠ è½½æ¨¡å‹ã€å®ƒæ˜¯æ€ä¹ˆå¤„ç†æ•°å­—è¾“å…¥å¹¶è¾“å‡ºé¢„æµ‹çš„ã€‚

æ¥ä¸‹æ¥è¿˜æœ‰ tokenizer APIã€‚tokenizer è´Ÿè´£å°†æ–‡æœ¬è½¬æˆæ•°å­—è¡¨ç¤ºï¼ˆä»¥ä½œä¸ºç¥ç»ç½‘ç»œçš„è¾“å…¥ï¼‰ï¼Œå¹¶è´Ÿè´£å°†æ•°å­—è¡¨ç¤ºè½¬åŒ–æˆæ–‡æœ¬ã€‚æˆ‘ä»¬è¿˜ä¼šå±•ç¤ºå¦‚ä½•æ‰¹å¤„ç†å¤šä¸ªå¥å­ã€‚

## 1. Pipeline éƒ½åšäº†ä»€ä¹ˆ

ä¸Šä¸€èŠ‚ä¸­æåˆ°çš„ `pipeline()` å‡½æ•°å®é™…ä¸Šç»è¿‡äº†ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼šé¢„å¤„ç†ã€å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ã€åå¤„ç†

![](/images/huggingface/section1/full_nlp_pipeline.svg)

### 1.1 ä½¿ç”¨ tokenizer è¿›è¡Œé¢„å¤„ç†

Transformer æ¨¡å‹ä¸èƒ½ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œäºæ˜¯è¦å…ˆå°†æ–‡æœ¬è¾“å…¥è½¬æ¢æˆæ•°å­—è¡¨ç¤ºã€‚å®é™…ä¸Šï¼ŒTransformer æ¨¡å‹åªæ¥æ”¶ tensor ä½œä¸ºè¾“å…¥ã€‚

tokenizer çš„å¤„ç†æ­¥éª¤ï¼š
- å°†æ–‡æœ¬åˆ‡åˆ†æˆ `tokens` (å¯èƒ½æ˜¯ words, subwords, æˆ–è€… symbols)
- å°†æ¯ä¸€ä¸ª `token` æ˜ å°„åˆ°ä¸€ä¸ªæ•°å­—ä¸Š
- æ·»åŠ å¯èƒ½å¯¹æ¨¡å‹æœ‰ç”¨çš„å…¶ä»–è¾“å…¥

æ˜¾ç„¶ï¼Œä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹æ—¶ä½¿ç”¨çš„ä¸Šè¿°æ“ä½œåº”è¯¥å’Œé¢„è®­ç»ƒæ—¶çš„æ“ä½œä¸€è‡´ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `AutoTokenizer` ç±»ä»¥åŠå®ƒçš„ `from_pretrained()` å‡½æ•°æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä½¿ç”¨æ¨¡å‹çš„ checkpoints åç§°ï¼Œå®ƒä¼šä¸‹è½½å¯¹åº”æ¨¡å‹çš„ tokenizer å¹¶ç¼“å­˜ä¸‹æ¥ã€‚


åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† sentiment-analysis
```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

ä¸Šé¢æ¨¡å‹çš„é»˜è®¤ checkpoints æ˜¯ sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-englishï¼Œä½¿ç”¨ AutoTokenizer åˆ›å»º tokenizer å¯¹è±¡

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

å°†æ–‡æœ¬ä¼ é€’ç»™ tokenizer
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]

# padding, truncation ä¼šåœ¨ä¹‹åä»‹ç»ï¼›return_tensors ä¸º pt, å³ pytorch
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

```:no-line-numbers
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

### 1.2 model

ä¸ `AutoTokenizer` ç±»ä¼¼ï¼ŒğŸ¤— Transformers åº“è¿˜æä¾›äº† `AutoModel` classï¼Œå®ƒä¹Ÿæœ‰ `from_pretrained()` æ–¹æ³•ã€‚

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

ä¸Šé¢çš„ä»£ç ä¸‹è½½äº† distilbert-base-uncased-finetuned-sst-2-english çš„ checkpoints ï¼ˆå¦‚æœè¿˜åœ¨åŒä¸€ç¯å¢ƒä¸­ï¼Œé‚£ä¹ˆå®ƒåœ¨ä¹‹å‰å·²ç»è¢«ç¼“å­˜äº†ï¼‰ï¼Œå¹¶å®ä¾‹åŒ–äº†å¯¹åº”çš„æ¨¡å‹ã€‚

è¿™ä¸ªæ¶æ„åªåŒ…æ‹¬æœ€åŸºæœ¬çš„ Transformer æ¨¡å—ï¼Œå³ä¸‹å›¾ä¸­ Transformer Network éƒ¨åˆ†ã€‚

![](/images/huggingface/section1/transformer_and_head.svg)

:::info æ³¨æ„
Transformer Network å³æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­è°ˆåˆ°çš„ Transformer æ¶æ„ï¼Œè¿™é‡Œåªæ˜¯ç”»æˆ Embedding + Layers
:::

å½“ç„¶ ğŸ¤— Transformers è¿˜æä¾›äº†ä¸åŒçš„æ¶æ„ã€‚ä¸‹é¢åˆ—ä¸¾äº†ä¸€éƒ¨åˆ†
- *Model (retrieve the hidden states)
- *ForCausalLM
- *ForMaskedLM
- *ForMultipleChoice
- *ForQuestionAnswering
- *ForSequenceClassification
- *ForTokenClassification
- and others ğŸ¤—

#### 1.2.1 Model è¾“å‡ºï¼ˆhidden states æˆ– featuresï¼‰ï¼šé«˜ç»´å¼ é‡

Transformer æ¨¡å—è¾“å‡ºçš„å¼ é‡é€šå¸¸å¾ˆå¤§ï¼Œå®ƒæœ‰ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦ï¼š
- Batch size: æ¯æ¬¡å¤„ç†çš„åºåˆ—é•¿åº¦ï¼ˆä¸Šè¿°ä¾‹å­ä¸­ä¸º2ï¼‰
- Sequence length: åºåˆ—çš„æ•°å­—è¡¨ç¤ºçš„é•¿åº¦ï¼ˆä¸Šè¿°ä¾‹å­ä¸­ä¸º16ï¼‰
- Hidden size: æ¯ä¸ªæ¨¡å‹è¾“å…¥çš„å¼ é‡ç»´åº¦ã€‚é€šå¸¸å¾ˆå¤§ï¼ˆå°æ¨¡å‹å¯èƒ½æ˜¯768ï¼Œåœ¨å¤§ä¸€äº›çš„æ¨¡å‹ä¸­å¯èƒ½æ˜¯3072ç”šè‡³æ›´å¤§ï¼‰

æˆ‘ä»¬å°†ä¸Šé¢ä½¿ç”¨ tokenizer å¾—åˆ°çš„è¾“å…¥ä¼ é€’ç»™ modelï¼Œçœ‹çœ‹å®ƒçš„è¾“å‡º
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```:no-line-numbers
torch.Size([2, 16, 768])
```

#### 1.2.2 Model heads

Model heads é€šå¸¸æœ‰ä¸€å±‚æˆ–å¤šå±‚çº¿æ€§å±‚ç»„æˆï¼Œä»¥ hidden states ä½œä¸ºè¾“å…¥ï¼Œå°†è¿™äº›é«˜ç»´å¼ é‡æ˜ å°„åˆ°ä¸åŒçš„ç»´åº¦ä¸Šã€‚

æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæœ‰åºåˆ—åˆ†ç±»ï¼ˆsequence classificationï¼‰head çš„æ¨¡å‹ï¼Œäºæ˜¯æˆ‘ä»¬ä¸ç”¨ `AutoModel` ç±»ï¼Œæˆ‘ä»¬ä½¿ç”¨ `AutoModelForSequenceClassification`ã€‚

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)

print(outputs.logits.shape)
```

```:no-line-numbers
torch.Size([2, 2])
```

æˆ‘ä»¬åˆ†æçš„æ˜¯ä¸¤ä¸ªå¥å­ï¼Œåˆ¤æ–­æ¯ä¸ªå¥å­æ˜¯ positive è¿˜æ˜¯ negativeï¼Œæ‰€ä»¥è¾“å‡ºç»´åº¦æ˜¯ 2x2

### 1.3 åå¤„ç†

```python
print(outputs.logits)
```

```:no-line-numbers
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```

å¯¹äºç¬¬ä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬é¢„æµ‹çš„æ˜¯ [-1.5607,  1.6123]ï¼Œè¿™æ˜¯logitsï¼ˆæ‰€æœ‰çš„ ğŸ¤— Transformers æ¨¡å‹è¾“å‡ºçš„éƒ½æ˜¯ logitsï¼‰ã€‚å®é™…ä¸Šæˆ‘ä»¬æ›´å¸Œæœ›å¾—åˆ°ç±»ä¼¼äºæ¦‚ç‡çš„ç»“æœï¼Œäºæ˜¯æˆ‘ä»¬å°†å®ƒè¾“å…¥è‡³ SoftMax å±‚ä¸­ã€‚

```python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```

```:no-line-numbers
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹ model config çš„ id2label å±æ€§æ¥æŸ¥çœ‹å¯¹åº”çš„ label 

```python
model.config.id2label
```

```:no-line-numbers
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

åˆ°æ­¤ä¸ºæ­¢ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæ­¥éª¤ï¼ˆä½¿ç”¨ tokenizer è¿›è¡Œé¢„å¤„ç†ï¼Œå°†è¾“å…¥ä¼ é€’ç»™ modelï¼Œåå¤„ç†ï¼‰å¾—åˆ°æœ€ç»ˆçš„ç»“è®ºï¼š
- ç¬¬ä¸€ä¸ªå¥å­: NEGATIVE: 0.0402, POSITIVE: 0.9598
- ç¬¬äºŒä¸ªå¥å­: NEGATIVE: 0.9995, POSITIVE: 0.0005


## 2. Models

æˆ‘ä»¬è¯¦ç»†ä»‹ç»ä¸‹ modelã€‚`AutoModel` ç±»å¯ä»¥æ ¹æ® checkpoint æ¥å®ä¾‹åŒ–ä»»ä½•æ¨¡å‹ã€‚å®ƒæ ¹æ® checkpoint æ¥ç¡®å®šæ¨¡å‹ç»“æ„ï¼Œå¹¶å®ä¾‹åŒ–æ¨¡å‹ã€‚å¦‚æœä½ ç¡®åˆ‡çŸ¥é“ä½ æƒ³ä½¿ç”¨ä»€ä¹ˆç±»å‹çš„æ¨¡å‹ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨å¯¹åº”çš„ model ç±»ã€‚

ä¸‹é¢å°†ä½¿ç”¨ BERT modelã€‚

### 2.1 åˆ›å»º Transformer

åˆå§‹åŒ– BERT æ¨¡å‹çš„ç¬¬ä¸€æ­¥æ˜¯åŠ è½½é…ç½®å¯¹è±¡ã€‚

```python
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)
```

é…ç½®ä¸­åŒ…å«å¾ˆå¤šå»ºç«‹æ¨¡å‹è¦ç”¨åˆ°çš„å±æ€§ã€‚
:::details æ‰“å° config çš„å†…å®¹
```:no-line-numbers{5,7,9-12}
BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.29.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}
```
:::

#### 2.1.1 ä¸åŒçš„åŠ è½½æ–¹æ³•

ä½¿ç”¨é»˜è®¤çš„é…ç½®æ¥åˆ›å»º model æ—¶ï¼Œmodel ä¼šè¢«éšæœºåˆå§‹åŒ–ã€‚

ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ modelï¼Œä¸è¿‡ä½¿ç”¨æ•ˆæœè‚¯å®šå¾ˆå·®ï¼Œè€Œé‡æ–°è®­ç»ƒåˆéœ€è¦å¤§é‡çš„æ—¶é—´å’Œæ•°æ®ã€‚æˆ‘ä»¬ä¸å¦¨åŠ è½½å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿™è¦ç”¨åˆ° `from_pretrained()` æ–¹æ³•ã€‚

```python
from transformers import BertModel

model = BertModel.from_trained("bert-base-cased")
```

å½“ç„¶ä½ ä¹Ÿå¯ä»¥å°† BertModel æ›¿æ¢ä¸º AutoModel 
```python
from transformers import AutoModel

model = AutoModel.from_trained("bert-base-cased")
```

ä½ å¯ä»¥åœ¨è¿™ä¸ª [model card](https://huggingface.co/bert-base-cased) ä¸­æŸ¥çœ‹ BERT æ¨¡å‹çš„æ›´å¤šç»†èŠ‚ã€‚

:::tip
ç¼“å­˜è·¯å¾„ä¸º ~/.cache/huggingface/modules ä½ å¯ä»¥é€šè¿‡è®¾ç½® `HF_HOME` ç¯å¢ƒå˜é‡æ¥è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„ã€‚
:::

#### 2.1.2 ä¿å­˜æ–¹æ³•

ä½¿ç”¨ `save_pretrained()` æ–¹æ³•æ¥ä¿å­˜æ¨¡å‹
```python
model.save_pretrained("directory_on_my_computer")
```

è¿™ä¼šä¿å­˜ä¸¤ä¸ªæ–‡ä»¶
```
ls directory_on_my_computer
config.json pytorch_model.bin
```

ä½ å¯ä»¥åœ¨ config.json ä¸­çœ‹åˆ°å»ºç«‹æ¨¡å‹æ‰€éœ€çš„å±æ€§ã€‚è¯¥æ–‡ä»¶ä¸­ä¹Ÿæœ‰ä¸€äº› metadataï¼Œæ¯”å¦‚ checkpoint çš„æ¥æºæˆ–è·¯å¾„ã€ä½ æœ€åä¸€æ¬¡ä¿å­˜ checkpoint æ—¶ä½¿ç”¨çš„ ğŸ¤— Transformers ç‰ˆæœ¬

python_model.bin è¢«ç§°ä¸ºçŠ¶æ€å­—å…¸ï¼ˆstate dictionaryï¼‰ã€‚å…¶ä¸­è®°å½•äº†æ¨¡å‹çš„æƒé‡ã€‚

è¿™ä¸¤ä¸ªæ–‡ä»¶ç›¸è¾…ç›¸æˆï¼Œconfig.json æä¾›äº†æ¨¡å‹çš„æ¶æ„ä¿¡æ¯ï¼Œpython_model.bin æä¾›äº†æ¨¡å‹æƒé‡ã€‚


### 2.2 ä½¿ç”¨ Transformer è¿›è¡Œæ¨ç†ï¼ˆinferenceï¼‰

åœ¨ 2.1 ä¸­ä½ å·²ç»çœ‹åˆ°äº†å¦‚ä½•åŠ è½½ä»¥åŠä¿å­˜ä½¿ç”¨æ¨¡å‹ï¼Œä¸‹é¢æˆ‘ä»¬æ¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚

åœ¨ 1.1 ä¸­ï¼Œæˆ‘ä»¬å·²ç»è¿‡å¦‚ä½•ä½¿ç”¨ tokenizer å°†æ–‡æœ¬è½¬åŒ–ä¸ºå¼ é‡ï¼Œå®ƒå°†è¾“å…¥çš„æ–‡æœ¬è½¬åŒ–ä¸ºæ•°å­—ï¼š

:::details è·å¾— input IDs: model_input
ç»™å®šæ–‡æœ¬ï¼š
```python:no-line-numbers
sequences = ["Hello!", "Cool.", "Nice!"]
```

ç» tokenizer è·å¾— input IDs:
```python:no-line-numbers
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

Transformer åªæ¥æ”¶ tensorï¼Œå°†ä¸Šé¢çš„ list è½¬åŒ–æˆ tensorï¼š
```python
import torch

model_inputs = torch.tensor(encoded_sequences)
```
:::

ç°åœ¨å¯ä»¥å°† model_input ä¼ é€’ç»™ model äº†

```python
output = model(model_inputs)
```

model å¯ä»¥æ¥æ”¶å¾ˆå¤šå‚æ•°ï¼Œå…¶ä¸­ input IDs åªå¿…ä¼ çš„ã€‚æˆ‘ä»¬å°†åœ¨æœªæ¥åœ¨è®¨è®ºå…¶ä»–å‚æ•°ã€‚


## 3. Tokenizers

æ¨¡å‹åªèƒ½å¤„ç†æ•°å­—ï¼Œtokenizer çš„ä½œç”¨æ˜¯å°†æ–‡æœ¬è½¬åŒ–ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°å­—ã€‚å®ƒçš„ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€æœ‰æ„ä¹‰çš„è¡¨ç¤ºï¼Œå¹¶å°½å¯èƒ½å°ã€‚

ä¸‹é¢ä»‹ç»å‡ ç§ tokenization ç®—æ³•ã€‚

### 3.1 tokenization ç®—æ³•

#### 3.1.1 Word-based

å¯ä»¥ä½¿ç”¨ç©ºæ ¼æ¥å°†å¥å­åˆ‡åˆ†ä¸ºå­—è¯ï¼š
```python
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
# res
# ['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

ä¹Ÿæœ‰é’ˆå¯¹æ ‡ç‚¹ç¬¦å·å¢åŠ äº†é¢å¤–è§„åˆ™çš„ tokenizerã€‚

ä½¿ç”¨ word-based tokenizerï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°ä¸€ä¸ªéå¸¸å¤§çš„è¯æ±‡è¡¨ï¼Œæ­¤è¡¨çš„å¤§å°ç”±è¯­æ–™ä¸­çš„ç‹¬ç«‹ token æ•°å†³å®šã€‚æ¯ä¸ªå­—è¯éƒ½è¢«åˆ†é…äº†ä¸€ä¸ª IDï¼Œä» 0 åˆ°æ•´ä¸ªè¯è¡¨å¤§å°ã€‚æ¨¡å‹ä½¿ç”¨è¿™äº› ID æ¥è¡¨ç¤ºæ¯ä¸ªå­—è¯ã€‚

å¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨è¿™ç§ tokenizer æ¥è¦†ç›–æŸé—¨è¯­è¨€ï¼Œé‚£å°†ä¼šç”Ÿæˆå¤§é‡ tokenã€‚ä¾‹å¦‚ï¼Œè‹±è¯­ä¸­æœ‰ 500,000 ä¸ªå•è¯ï¼Œäºæ˜¯æ„å»ºæ¯ä¸ªå•è¯åˆ° input_id çš„æ˜ å°„è¦æœ‰ 500,000 ä¸ªã€‚é™¤æ­¤ä¹‹å¤–ï¼Œ'dog' å’Œ 'dogs', 'run' å’Œ 'running' ä¼šè¢«åˆ†åˆ«æ„å»ºä¸åŒçš„ input_id, æ²¡æœ‰ä½“ç°å‡ºå®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼ä¸è”ç³»ã€‚

å¦å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è‡ªå®šä¹‰ä¸€ä¸ª token æ¥è¡¨ç¤ºä¸åœ¨è¯è¡¨ä¸­çš„å­—è¯ï¼Œä¹Ÿå°±æ˜¯ 'unknown' tokenã€‚ä¸€èˆ¬ç”¨ '[UNK]' æˆ– '' è¡¨ç¤ºã€‚å¦‚æœæŸä¸ª tokenizer äº§ç”Ÿäº†å¤§é‡ unknown tokenï¼Œè¿™æ„å‘³ç€å®ƒæ— æ³•æ£€ç´¢åˆ°ä¸€ä¸ªè¯çš„åˆç†è¡¨ç¤ºï¼Œä¸”ä½ åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä¸¢å¤±äº†ä¿¡æ¯ã€‚æˆ‘ä»¬å¸Œæœ› tokenizer ä¼šå°†å°½é‡å°‘çš„å­—è¯æ ‡è®°ä¸º unknown tokenã€‚

ä¸‹é¢ä»‹ç» character-based tokenizerï¼Œå®ƒå¯ä»¥å‡å°‘ unknown token çš„äº§ç”Ÿã€‚

#### 3.1.2 Character-based

Character-based tokenizer å°†æ–‡æœ¬åˆ‡åˆ†æˆ charactersï¼Œè€Œä¸æ˜¯ wordsã€‚è¿™æ ·åšæœ‰ä¸¤ç‚¹å¥½å¤„ï¼š
- è¯è¡¨å°†æœ‰æ•ˆåœ°å‡å°
- unknown tokens ä¹Ÿä¼šå‡å°‘ï¼ˆå› ä¸ºæ¯ä¸ªå­—è¯éƒ½æ˜¯é€šè¿‡ character æ„å»ºçš„ï¼‰

å½“ç„¶ï¼Œè¿™ä¹Ÿä¼šäº§ç”Ÿä¸€äº›é—®é¢˜ã€‚é¦–å…ˆï¼Œcharacter å¯èƒ½æœ¬èº«æ²¡æœ‰å«ä¹‰ï¼ˆç›¸å¯¹äº word æ¥è¯´ã€‚ä½†ä¹Ÿå› è¯­è¨€è€Œå¼‚ï¼Œæ¯”å¦‚ä¸­æ–‡å­—ç¬¦ä¼šæ¯”æ‹‰ä¸ç³»è¯­è¨€çš„å­—ç¬¦æºå¸¦æ›´å¤šä¿¡æ¯ï¼‰ã€‚å¦å¤–ï¼Œæ¨¡å‹éœ€è¦å¤„ç†å¤§é‡ tokenï¼ˆå¯¹äº word-based æ¥è¯´çš„ä¸€ä¸ª word åªéœ€è¦ 1 ä¸ª tokenï¼Œè€Œä½¿ç”¨ character-basedï¼Œå¯èƒ½æœ‰åå‡ ä¸ª tokens è¦å¤„ç†ï¼‰ã€

è€ƒè™‘åˆ°ä¸Šé¢ä¸¤ç§æŠ€æœ¯ï¼Œåˆæå‡ºäº†ç¬¬ä¸‰ç§æ–¹å¼ï¼šsubword tokenization

#### 3.1.3 Subword tokenization

subword tokenization çš„åŸåˆ™æ˜¯ï¼šç»å¸¸ä½¿ç”¨çš„è¯ä¸åº”è¯¥å†è¢«åˆ‡åˆ†ä¸ºæ›´å°çš„å­è¯ï¼Œæ¯”è¾ƒå°‘ç”¨çš„è¯å¯ä»¥åˆ†è§£ä¸ºæœ‰æ„ä¹‰çš„å­è¯ã€‚

æ¯”å¦‚ï¼šannoyingly å¯èƒ½è¢«åˆ‡åˆ†æˆ annoyã€ingã€ly

ä»¥ä¸‹ tokenization æ˜¯ subword tokenizationï¼š
- Byte-level BPE, GPT-2 ä½¿ç”¨è¿™ç§æ–¹å¼
- WordPiece, BERT ä½¿ç”¨è¿™ç§æ–¹å¼
- SentencePiece or Unigram, åœ¨ä¸€äº›å¤šè¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨
- ...

### 3.2 åŠ è½½ & ä¿å­˜

åŠ è½½å’Œä¿å­˜ tokenizer åˆ†åˆ«ä½¿ç”¨æ–¹æ³• `from_pretrained()` å’Œ `save_pretrained()`. ä½¿ç”¨è¯¥æ–¹æ³•ä¼šåŠ è½½æˆ–ä¿å­˜ tokenizer ä½¿ç”¨çš„ç®—æ³•ï¼ˆç±»ä¼¼äº model çš„ architectureï¼‰ï¼Œè¿˜ä¼šåŠ è½½æˆ–ä¿å­˜å¯¹åº”çš„è¯è¡¨ï¼ˆç±»ä¼¼äº model çš„ weightï¼‰

åŠ è½½ BERT çš„ tokenizer
```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

ä¹Ÿå¯ä»¥ä½¿ç”¨ `AutoTokenizer`
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ç„¶åæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ tokenizer å°†æ–‡æœ¬è½¬åŒ–æˆ input_ids
```python
tokenizer("Using a Transformer network is simple")
# result:
# {'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
#  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
#  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ä¿å­˜ tokenizer
```python
tokenizer.save_pretrained("directory_on_my_computer")
```

æˆ‘ä»¬å°†åœ¨ [Chapter 3](./Chapter3.md) ä¸­æ¥è®¨è®º token_type_idsã€‚å¹¶åœ¨ [4.3](#_4-3-attention-masks) ä¸­è®¨è®º attention_maskã€‚ä¸‹é¢æˆ‘ä»¬å…ˆä»‹ç»ä¸‹ input_ids å¦‚ä½•ç”Ÿæˆï¼Œä¸ºæ­¤æˆ‘ä»¬è¦æŸ¥çœ‹ tokenizer çš„ä¸­é—´æ–¹æ³•ã€‚

### 3.3 ç¼–ç ï¼ˆEncodingï¼‰

å°†æ–‡æœ¬è½¬æ¢æˆæ•°å­—çš„è¿‡ç¨‹å«åšç¼–ç ï¼ˆEncodingï¼‰ã€‚Encoding çš„åˆ†ä¸ºä¸¤æ­¥ï¼štokenizationï¼Œç„¶åè½¬åŒ–ä¸º input IDs.

ç¬¬ä¸€æ­¥æ˜¯å°†æ–‡æœ¬åˆ‡åˆ†ä¸º tokensã€‚å®ç°è¿™ä¸€æ­¥æœ‰ä¸åŒçš„è§„åˆ™ï¼ˆè§ 3.1ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç”¨æˆ‘ä»¬æ‰€é€‰çš„æ¨¡å‹çš„åç§°æ¥å®ä¾‹åŒ– tokenizerï¼Œä»¥ç¡®ä¿ä½¿ç”¨å’Œé¢„è®­ç»ƒæ—¶ç›¸åŒçš„è§„åˆ™ã€‚

ç¬¬äºŒæ­¥æ˜¯å°† tokens è½¬åŒ–ä¸ºæ•°å­—è¡¨ç¤ºï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç”¨å®ƒä»¬æ„å»ºå¼ é‡å¹¶æŠŠå¼ é‡æä¾›ç»™æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€æ­¥éª¤ï¼Œtokenizer éœ€è¦ä¸€ä¸ªè¯è¡¨ï¼ˆvocabularyï¼‰ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨ `from_pretrained()` æ¥å®ä¾‹åŒ– tokenizer çš„æ—¶å€™å·²ç»ä¸‹è½½å¥½äº†ã€‚åŒæ ·åœ°ï¼Œè¿™ä¸ªè¯è¡¨å’Œé¢„è®­ç»ƒæ—¶çš„è¯è¡¨æ˜¯ç›¸åŒçš„ã€‚

ä¸‹é¢åˆ†åˆ«ä»‹ç»è¿™ä¸¤æ­¥ã€‚æ³¨æ„ï¼Œåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ç›´æ¥è°ƒç”¨ tokenizer å°±å¯ä»¥ï¼Œä¸‹é¢çš„åˆ†æ­¥è°ƒç”¨åªæ˜¯ä¸ºäº†è®©å¤§å®¶æ›´æ¸…æ¥š encoding çš„ä¸¤ä¸ªæ­¥éª¤åˆ†åˆ«åšäº†ä»€ä¹ˆã€‚

#### 3.3.1 Tokenization

Tokenization è¿‡ç¨‹å¯ä»¥ä½¿ç”¨ tokenizer çš„ `tokenize()` æ–¹æ³•å®ç°ï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-based-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)
print(tokens)
# result:
# ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

#### 3.3.2 å°† tokens è½¬æ¢ä¸º input IDs

è¯¥è¿‡ç¨‹é€šè¿‡ `convert_tokens_to_ids()` å®ç°ã€‚

```python
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
# result:
# [7993, 170, 11303, 1200, 2443, 1110, 3014]
```

### 3.4 è§£ç ï¼ˆDecodingï¼‰

è§£ç ï¼ˆdecodingï¼‰å’Œç¼–ç ï¼ˆencodingï¼‰çš„è¯¥è¿‡ç¨‹ç›¸åï¼Œå°†è¯è¡¨ç´¢å¼•è½¬åŒ–æˆå­—ç¬¦ä¸²ï¼Œå¯ä»¥ä½¿ç”¨ `decode()` æ–¹æ³•æ¥å®ç°

```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
# result:
# 'Using a Transformer network is simple'
```

æ³¨æ„ï¼Œ`decode()` æ–¹ä¸ä»…å°†ç´¢å¼•è½¬åŒ–ä¸ºäº† tokensï¼Œè¿˜å°†åŒä¸€ä¸ªè¯ä¸­çš„ tokens ç»„åˆåœ¨ä¸€èµ·äº†ã€‚

### 3.5 å°ç»“

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ åº”è¯¥äº†è§£ tokenizer çš„åŸå­æ“ä½œï¼štokenizationã€å°† tokens è½¬åŒ–æˆ input_idsã€å°† ids è½¬åŒ–ä¸ºå­—ç¬¦ä¸²ã€‚

## 4. å¤„ç†å¤šä¸ªåºåˆ—

åœ¨ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯¹å°é•¿åº¦çš„åºåˆ—è¿›è¡Œäº†å¤„ç†ã€‚æˆ‘ä»¬éœ€è¦æ€è€ƒä»¥ä¸‹é—®é¢˜ï¼š
- å¦‚ä½•å¤„ç†å¤šä¸ªåºåˆ—
- å¦‚ä½•å¤„ç†*ä¸åŒé•¿åº¦*çš„å¤šä¸ªåºåˆ—
- è¯æ±‡è¡¨ç´¢å¼•æ˜¯å”¯ä¸€èƒ½å¤Ÿä½¿æ¨¡å‹æ­£å¸¸å·¥ä½œçš„è¾“å…¥å—
- æ˜¯å¦å­˜åœ¨åºåˆ—è¿‡é•¿çš„é—®é¢˜

### 4.1 æ‰¹å¤„ç†

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
# IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```

ä¸ºä»€ä¹ˆä¼šå‡ºé”™å‘¢ï¼ŸğŸ¤— Transformers æ¨¡å‹é»˜è®¤æ¥æ”¶å¤šä¸ªå¥å­ä½œä¸ºè¾“å…¥ï¼Œä½†æˆ‘ä»¬åªä¼ é€’æ¥ä¸€ä¸ªåºåˆ—ã€‚

åœ¨æ­¤ä¹‹å‰æˆ‘ä»¬ç›´æ¥è°ƒç”¨ tokenizer æ—¶ï¼Œåœ¨é¡¶éƒ¨åŠ äº†ä¸€ä¸ªç»´åº¦ï¼š
```python
tokenized_input = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
# result:
# tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
#           2607,  2026,  2878,  2166,  1012,   102]])
```

:::details tokenizer å¯ä»¥å°†åºåˆ—è½¬åŒ–æˆç‰¹å®šç»“æ„çš„ tensorï¼Œé€šè¿‡ return_tensors å‚æ•°è®¾ç½®
```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```
:::

æˆ‘ä»¬å¯¹èµ·åˆçš„ä»£ç è¿›è¡Œä¿®æ”¹ï¼š
```python{13}
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor([ids])
print(input_ids)
# tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]])

output = model(input_ids)
print(output.logits)
# tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)
```

`Batching` æ˜¯æŒ‡ä¸€æ¬¡å‘æ¨¡å‹ä¼ é€’å¤šä¸ªå¥å­

```python
batched_ids = [ids, ids]
```

## 4.2 å¡«å……ï¼ˆPaddingï¼‰

å½“è¿›è¡Œæ‰¹å¤„ç†æ—¶ï¼Œå¦‚æœä¸¤ä¸ªåºåˆ—çš„é•¿åº¦ä¸ä¸€æ ·æ€ä¹ˆåŠï¼ˆå¯¹äº tensor æ¥è¯´ï¼Œå®ƒå¿…é¡»æ˜¯çŸ©é˜µï¼Œå³æ¯ä¸ªåºåˆ—çš„è¡¨ç¤ºåº”è¯¥æ˜¯ä¸€æ ·é•¿çš„ï¼‰ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†å¡«å……ï¼ˆpadï¼‰è¾“å…¥ã€‚

å¯¹äºçŸ­çš„åºåˆ—ï¼Œæˆ‘ä»¬ä½¿ç”¨ padding token æ¥å¡«å……ï¼Œä½¿å…¶å’Œæœ€é•¿çš„åºåˆ—ä¸€æ ·é•¿ã€‚

```python
# æˆ‘ä»¬æ— æ³•å°†è¿™ä¸ª batch è½¬åŒ–æˆ tensor
bached_id_origin = [
    [200, 200, 200],
    [200, 200]
]

padding_id = 100

# ä½¿ç”¨ padding 
batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id]
]
```

å¯ä»¥åœ¨ `tokenizer.pad_token_id` ä¸­è·å– padding token idã€‚

```python
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```
```:no-line-numbers
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)
tensor([[ 1.5694, -1.3895],
        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)
```

è§‚å¯Ÿä¸Šé¢çš„ç»“æœï¼Œbatched_ids çš„ç»“æœçš„ç¬¬äºŒè¡Œä¸ sequence2_ids çš„ç»“æœä¸ä¸€æ ·ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åº”è¯¥çš„ã€‚

é€ æˆä¸ä¸€è‡´æ˜¯å› ä¸ºï¼ŒTransformer ä¸­çš„ attention layers å°†æ¯ä¸ª token éƒ½ä½œä¸ºä¸Šä¸‹æ–‡è€ƒè™‘è¿›å»äº†ã€‚é‚£ä¹ˆ padding tokens ä¹Ÿä¼šè¢«è€ƒè™‘è¿›å»ã€‚å¦‚æœæƒ³è®©ä¸¤æ¬¡çš„ç»“æœç›¸åŒï¼Œéœ€è¦å‘Šè¯‰ attention layer å¿½ç•¥ padding tokensã€‚è¿™è¦é€šè¿‡ attention mask æ¥å®ç°ã€‚

### 4.3 Attention Masks

attention mask ä¹Ÿæ˜¯ tensorï¼Œå®ƒå’Œ input IDs tensor ç»“æ„ç›¸åŒï¼Œå…ƒç´ åªæœ‰ 0 å’Œ 1ï¼š0 è¡¨ç¤ºè¯¥ä½ç½®æ˜¯ padding tokensï¼Œattention layers åº”è¯¥å¿½ç•¥å®ƒã€‚

æˆ‘ä»¬å°† attention_mask ä½œä¸ºå‚æ•°ä¼ é€’ç»™ modelï¼Œå†è§‚å¯Ÿç»“æœï¼Œå’Œ sequence2_ids çš„ç»“æœä¸€è‡´äº†ï¼š
```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]
attention_mask = [
    [1, 1, 1],
    [1, 1, 0]
]
print(model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)).logits)
# tensor([[ 1.5694, -1.3895],
#         [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)
```

```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
print(model_inputs)

# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")
print(model_inputs)

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")
print(model_inputs)

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
print(model_inputs)
```

:::details è¾“å‡º
```python:no-line-numbers
{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}

{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}

{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}

{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]]}
```
:::
### 4.4 é•¿åºåˆ—

Transformer æ¨¡å‹èƒ½å¤„ç†çš„åºåˆ—é•¿åº¦æ˜¯æœ‰é™çš„ï¼Œå¤§éƒ½åœ¨ 512 è‡³ 1024 ä¸ª tokens ä¹‹é—´ã€‚å¦‚æœä¼ å…¥äº†å¤§äºæœ€å¤§é™åº¦çš„åºåˆ—ä¼šå´©æºƒï¼Œæœ‰ä¸¤ç§æ–¹å¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š
- æ¢ç”¨æ”¯æŒæ›´é•¿åºåˆ—çš„æ¨¡å‹ã€‚åƒ [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) å’Œ [LED](https://huggingface.co/docs/transformers/model_doc/led) å°±èƒ½å¤„ç†æ¯”è¾ƒé•¿çš„åºåˆ—ã€‚
- æˆªæ–­åºåˆ—ã€‚
```python
sequence = sequence[:max_sequence_length]
```

å¯ä»¥ä½¿ç”¨ tokenizer è¿›è¡Œæˆªæ–­
```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)
print(model_inputs)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
print(model_inputs)
```
```python:no-line-numbers
{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}
{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}
```

## 5. Tokenizer API

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ tokenizer æ¥ paddingã€truncate åºåˆ—ï¼Œä¹Ÿå¯ä»¥æŒ‡å®š return_tensors çš„ç±»å‹ï¼ˆ4 ä¸­å·²æœ‰å¯¹åº”ç¤ºä¾‹ï¼‰ã€‚

### 5.1 ç‰¹æ®Š token

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```
```python:no-line-numbers
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

é¦–ä½å„æ·»åŠ äº†ä¸€ä¸ª token IDï¼Œæˆ‘ä»¬å°†å…¶è¿›è¡Œ decodeï¼š
```python
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```
```python:no-line-numbers
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

tokenizer æ·»åŠ äº†ä¸¤ä¸ªç‰¹æ®Šè¯ â€œ[CLS]â€ å’Œ â€œ[SEP]â€ã€‚è¿™æ˜¯åœ¨é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨ä½¿ç”¨è¯¥æ¨¡å‹åšæ¨ç†çš„æ—¶å€™ä¹Ÿåº”è¯¥åœ¨é¦–å°¾åŠ ä¸Šå®ƒä»¬ã€‚ä¸åŒçš„æ¨¡å‹ä¼šä½¿ç”¨ä¸åŒçš„ç‰¹æ®Šè¯ï¼Œæœ‰äº›æ¨¡å‹ä¸ç”¨ç‰¹æ®Šè¯ï¼Œæœ‰äº›æ¨¡å‹åªåœ¨å¥é¦–åŠ ç‰¹æ®Šè¯ï¼Œæœ‰äº›æ¨¡å‹åªåœ¨å¥å°¾åŠ ç‰¹æ®Šè¯ã€‚ä¸è®ºå¦‚ä½•ï¼Œtokenizer æ€»æ˜¯çŸ¥é“åº”è¯¥æ˜¯æ€æ ·çš„ï¼Œå¹¶ä¼šä¸ºä½ å¤„ç†å¥½å®ƒã€‚

### 5.2 å°ç»“

æœ€åè®©æˆ‘ä»¬çœ‹ä¸€çœ‹å¦‚ä½•ä½¿ç”¨ tokenizer å’Œ model æ¥è¿›è¡Œ inferenceï¼Œæˆ‘ä»¬ä½¿ç”¨äº† paddingï¼ˆä¸ºäº†æ‰¹å¤„ç†ï¼‰ï¼Œä½¿ç”¨äº† truncateï¼ˆä¸ºäº†å¤„ç†é•¿åºåˆ—ï¼‰ã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```

## æ€»ç»“
- Transformer çš„åŸºæœ¬ç»“æ„
- tokenization pipeline çš„ç»„æˆ
- å¦‚ä½•ä½¿ç”¨ Transformer model
- å¦‚ä½•åˆ©ç”¨ tokenizer å°†æ–‡æœ¬è½¬åŒ–ä¸º tensor
- ä½¿ç”¨ tokenizer å’Œ model æ¥è¿›è¡Œæ¨ç†
- input IDs çš„é™åˆ¶ï¼Œäº†è§£ attention masks
- å°è¯•äº†å¤šç§å¯é…ç½®çš„ tokenizer æ–¹æ³•