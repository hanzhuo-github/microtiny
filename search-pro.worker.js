const nt="ENTRIES",T="KEYS",V="VALUES",F="";class D{constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===F)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==F).join("")}value(){return E(this._path).node.get(F)}result(){switch(this._type){case V:return this.value();case T:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],ot=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return W(e,t,s,n,i,1,o,""),n},W=(e,t,s,n,o,u,i,r)=>{const h=u*i;t:for(const c of e.keys())if(c===F){const a=o[h-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let l=0;l<c.length;++l,++a){const p=c[l],f=i*a,g=f-i;let d=o[f];const m=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let _=m;_<y;++_){const b=p!==t[_],z=o[g+_]+ +b,A=o[g+_+1]+1,w=o[f+_]+1,L=o[f+_+1]=Math.min(z,A,w);L<d&&(d=L)}if(d>s)continue t}W(e.get(c),t,s,n,o,a,i,r+c)}};class C{constructor(t=new Map,s=""){this._size=void 0,this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=I(n);for(const i of o.keys())if(i!==F&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ut(this._tree,t)}entries(){return new D(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ot(this._tree,t,s)}get(t){const s=O(this._tree,t);return s!==void 0?s.get(F):void 0}has(t){const s=O(this._tree,t);return s!==void 0&&s.has(F)}keys(){return new D(this,T)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,M(this._tree,t).set(F,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=M(this._tree,t);return n.set(F,s(n.get(F))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=M(this._tree,t);let o=n.get(F);return o===void 0&&n.set(F,o=s()),o}values(){return new D(this,V)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==F&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},O=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==F&&t.startsWith(s))return O(e.get(s),t.slice(s.length))},M=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==F&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const h=e.get(u);if(r===u.length)e=h;else{const c=new Map;c.set(u.slice(r),h),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ut=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(F),s.size===0)R(n);else if(s.size===1){const[o,u]=s.entries().next().value;$(n,o,u)}}},R=e=>{if(e.length===0)return;const[t,s]=I(e);if(t.delete(s),t.size===0)R(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==F&&$(e.slice(0,-1),n,o)}},$=(e,t,s)=>{if(e.length===0)return;const[n,o]=I(e);n.set(o+t,s),n.delete(o)},I=e=>e[e.length-1],it=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,S="or",P="and",rt="and_not",ct=(e,t)=>{e.includes(t)||e.push(t)},q=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},G=({score:e},{score:t})=>t-e,lt=()=>new Map,k=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},N=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[S]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),q(n.terms,u)}}return e},[P]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);q(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[rt]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},at=(e,t,s,n,o,u)=>{const{k:i,b:r,d:h}=u;return Math.log(1+(s-t+.5)/(t+.5))*(h+e*(i+1)/(e+i*(1-r+r*n/o)))},dt=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},ft={k:1.2,b:.7,d:.5},gt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(it),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof(console==null?void 0:console[e])=="function"&&console[e](t)},autoVacuum:!0},H={combineWith:S,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:ft},mt={combineWith:P,prefix:(e,t,s)=>t===s.length-1},pt={batchSize:1e3,batchWait:10},J={minDirtFactor:.1,minDirtCount:20},Ft={...pt,...J};class _t{constructor(t){if((t==null?void 0:t.fields)==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?Ft:t.autoVacuum;this._options={...gt,...t,autoVacuum:s,searchOptions:{...H,...t.searchOptions||{}},autoSuggestOptions:{...mt,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=J,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const yt=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},U=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},At=(e,t,s,n)=>{if(!e._index.has(n)){U(e,s,t,n);return}const o=e._index.fetch(n,lt),u=o.get(t);u==null||u.get(s)==null?U(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},Y=(e,t=S)=>{if(e.length===0)return new Map;const s=t.toLowerCase();return e.reduce(ht[s])||new Map},B=(e,t,s,n,o,u,i,r,h=new Map)=>{if(o==null)return h;for(const c of Object.keys(u)){const a=u[c],l=e._fieldIds[c],p=o.get(l);if(p==null)continue;let f=p.size;const g=e._avgFieldLength[l];for(const d of p.keys()){if(!e._documentIds.has(d)){At(e,l,d,s),f-=1;continue}const m=i?i(e._documentIds.get(d),s,e._storedFields.get(d)):1;if(!m)continue;const y=p.get(d),_=e._fieldLength.get(d)[l],b=at(y,f,e._documentCount,_,g,r),z=n*a*m*b,A=h.get(d);if(A){A.score+=z,ct(A.terms,t);const w=N(A.match,s);w?w.push(c):A.match[s]=[c]}else h.set(d,{score:z,terms:[t],match:{[s]:[c]}})}}return h},Ct=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((d,m)=>({...d,[m]:N(n.boost,m)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:h}=n,{fuzzy:c,prefix:a}={...H.weights,...i},l=e._index.get(t.term),p=B(e,t.term,t.term,1,l,o,u,h);let f,g;if(t.prefix&&(f=e._index.atPrefix(t.term)),t.fuzzy){const d=t.fuzzy===!0?.2:t.fuzzy,m=d<1?Math.min(r,Math.round(t.term.length*d)):d;m&&(g=e._index.fuzzyGet(t.term,m))}if(f)for(const[d,m]of f){const y=d.length-t.term.length;if(!y)continue;g==null||g.delete(d);const _=a*d.length/(d.length+.3*y);B(e,t.term,d,_,m,o,u,h,p)}if(g)for(const d of g.keys()){const[m,y]=g.get(d);if(!y)continue;const _=c*d.length/(d.length+y);B(e,t.term,d,_,m,o,u,h,p)}return p},K=(e,t,s={})=>{if(typeof t!="string"){const a={...s,...t,queries:void 0},l=t.queries.map(p=>K(e,p,a));return Y(l,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:h}=i,c=r(t).flatMap(a=>h(a)).filter(a=>!!a).map(dt(i)).map(a=>Ct(e,a,i));return Y(c,i.combineWith)},X=(e,t,s={})=>{const n=K(e,t,s),o=[];for(const[u,{score:i,terms:r,match:h}]of n){const c=r.length,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(h),match:h};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return o.sort(G),o},Et=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of X(e,t,s)){const r=i.join(" "),h=n.get(r);h!=null?(h.score+=u,h.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:h}]of n)o.push({suggestion:u,terms:r,score:i/h});return o.sort(G),o},zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:h,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const l=new _t(a);l._documentCount=t,l._nextId=s,l._documentIds=k(n),l._idToShortId=new Map,l._fieldIds=o,l._fieldLength=k(u),l._avgFieldLength=i,l._storedFields=k(r),l._dirtCount=h||0,l._index=new C;for(const[p,f]of l._documentIds)l._idToShortId.set(f,p);for(const[p,f]of e){const g=new Map;for(const d of Object.keys(f)){let m=f[d];c===1&&(m=m.ds),g.set(parseInt(d,10),k(m))}l._index.set(p,g)}return l},Q=Object.entries,wt=Object.fromEntries,j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let l="";i===0?l=c.length>20?`… ${c.slice(-20)}`:c:a?l=c.length+i>100?`${c.slice(0,100-i)}… `:c:l=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,l&&o.push(l),i+=l.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let h=s.indexOf(n,u);if(h===-1)return null;for(;h>=0;){const c=h+n.length;if(r(e.slice(u,h)),u=c,i>100)break;h=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},Z=/[\u4e00-\u9fa5]/g,tt=(e={})=>({fuzzy:.2,prefix:!0,processTerm:t=>{const s=t.match(Z)||[],n=t.replace(Z,"").toLowerCase();return n?[n,...s]:[...s]},...e}),xt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),kt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),et=(e,t,s={})=>{const n={};return X(t,e,tt({boost:{h:2,t:1,c:4},...s})).forEach(o=>{const{id:u,terms:i,score:r}=o,h=u.includes("@"),c=u.includes("#"),[a,l]=u.split(/[#@]/),{contents:p}=n[a]??={title:"",contents:[]};if(h)p.push([{type:"customField",key:a,index:l,display:i.map(f=>o.c.map(g=>j(g,f))).flat().filter(f=>f!==null)},r]);else{const f=i.map(g=>j(o.h,g)).filter(g=>g!==null);if(f.length&&p.push([{type:c?"heading":"title",key:a,...c&&{anchor:l},display:f},r]),"t"in o)for(const g of o.t){const d=i.map(m=>j(g,m)).filter(m=>m!==null);d.length&&p.push([{type:"text",key:a,...c&&{anchor:l},display:d},r])}}}),Q(n).sort(([,o],[,u])=>"max"==="total"?xt(o,u):kt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=yt(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},st=(e,t,s={})=>Et(t,e,tt(s)).map(({suggestion:n})=>n),v=wt(Q(JSON.parse("{\"/en/\":{\"documentCount\":0,\"nextId\":0,\"documentIds\":{},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{},\"averageFieldLength\":[],\"storedFields\":{},\"dirtCount\":0,\"index\":[],\"serializationVersion\":2},\"/\":{\"documentCount\":100,\"nextId\":100,\"documentIds\":{\"0\":\"v-8daa1a0e\",\"1\":\"v-7bf9caeb\",\"2\":\"v-652d8e04\",\"3\":\"v-9f1cb90a\",\"4\":\"v-9f1cb90a#_1-概念\",\"5\":\"v-9f1cb90a#_2-notation\",\"6\":\"v-1c3d7e88\",\"7\":\"v-b398ee9c\",\"8\":\"v-b398ee9c#_1-准确性-accuracy\",\"9\":\"v-b398ee9c#_2-精确度-precision\",\"10\":\"v-b398ee9c#_3-召回率-recall\",\"11\":\"v-b398ee9c#_4-f1-分数-f1-score\",\"12\":\"v-668a7906\",\"13\":\"v-668a7906#_1-nlp-介绍\",\"14\":\"v-668a7906#_1-1-术语-architectures-vs-checkpoints\",\"15\":\"v-668a7906#_2-transformers-能做什么\",\"16\":\"v-668a7906#_2-1-快速体验-🤗-transformers-库\",\"17\":\"v-668a7906#_2-2-局限性-偏见\",\"18\":\"v-668a7906#_3-transformer-背景知识\",\"19\":\"v-668a7906#_3-1-transformer-是语言模型-language-model\",\"20\":\"v-668a7906#_3-2-transformer-是大模型\",\"21\":\"v-668a7906#_3-3-迁移学习-transfer-learning\",\"22\":\"v-668a7906#_4-transformer-结构\",\"23\":\"v-668a7906#_4-1-注意力层-attention-layers\",\"24\":\"v-668a7906#_4-2-原始模型\",\"25\":\"v-668a7906#_5-小结\",\"26\":\"v-6320c7c8\",\"27\":\"v-6320c7c8#_1-pipeline-都做了什么\",\"28\":\"v-6320c7c8#_1-1-使用-tokenizer-进行预处理\",\"29\":\"v-6320c7c8#_1-2-model\",\"30\":\"v-6320c7c8#_1-2-1-model-输出-hidden-states-或-features-高维张量\",\"31\":\"v-6320c7c8#_1-2-2-model-heads\",\"32\":\"v-6320c7c8#_1-3-后处理\",\"33\":\"v-6320c7c8#_2-models\",\"34\":\"v-6320c7c8#_2-1-创建-transformer\",\"35\":\"v-6320c7c8#_2-1-1-不同的加载方法\",\"36\":\"v-6320c7c8#_2-1-2-保存方法\",\"37\":\"v-6320c7c8#_2-2-使用-transformer-进行推理-inference\",\"38\":\"v-6320c7c8#_3-tokenizers\",\"39\":\"v-6320c7c8#_3-1-tokenization-算法\",\"40\":\"v-6320c7c8#_3-1-1-word-based\",\"41\":\"v-6320c7c8#_3-1-2-character-based\",\"42\":\"v-6320c7c8#_3-1-3-subword-tokenization\",\"43\":\"v-6320c7c8#_3-2-加载-保存\",\"44\":\"v-6320c7c8#_3-3-编码-encoding\",\"45\":\"v-6320c7c8#_3-3-1-tokenization\",\"46\":\"v-6320c7c8#_3-3-2-将-tokens-转换为-input-ids\",\"47\":\"v-6320c7c8#_3-4-解码-decoding\",\"48\":\"v-6320c7c8#_3-5-小结\",\"49\":\"v-6320c7c8#_4-处理多个序列\",\"50\":\"v-6320c7c8#_4-1-批处理\",\"51\":\"v-6320c7c8#_4-2-填充-padding\",\"52\":\"v-6320c7c8#_4-3-attention-masks\",\"53\":\"v-6320c7c8#_4-4-长序列\",\"54\":\"v-6320c7c8#_5-tokenizer-api\",\"55\":\"v-6320c7c8#_5-1-特殊-token\",\"56\":\"v-6320c7c8#_5-2-小结\",\"57\":\"v-6320c7c8#总结\",\"58\":\"v-5fb7168a\",\"59\":\"v-5fb7168a#_1-处理数据\",\"60\":\"v-5fb7168a#_1-1-从-hub-中加载数据集\",\"61\":\"v-5fb7168a#_1-2-数据集预处理\",\"62\":\"v-5fb7168a#_1-3-动态填充-dynamic-padding\",\"63\":\"v-5fb7168a#_2-使用-trainer-api-进行微调\",\"64\":\"v-5fb7168a#_2-1-训练-training\",\"65\":\"v-5fb7168a#_2-2-评估-evaluation\",\"66\":\"v-5fb7168a#_3-使用-pytorch-训练\",\"67\":\"v-5fb7168a#_3-1-准备\",\"68\":\"v-5fb7168a#_3-1-1-数据加载器-dataloader-用于迭代批次\",\"69\":\"v-5fb7168a#_3-1-2-model\",\"70\":\"v-5fb7168a#_3-1-3-优化器-optimizer\",\"71\":\"v-5fb7168a#_3-1-4-学习率调度器-learning-rate-scheduler\",\"72\":\"v-5fb7168a#_3-2-training-loop\",\"73\":\"v-5fb7168a#_3-3-evaluation-loop\",\"74\":\"v-5fb7168a#_3-4-使用-🤗-accelerate-进行加速\",\"75\":\"v-5fb7168a#总结\",\"76\":\"v-5c4d654c\",\"77\":\"v-5c4d654c#_1-使用预训练模型\",\"78\":\"v-5c4d654c#_2-共享预训练模型\",\"79\":\"v-5c4d654c#_2-1-创建模型仓库\",\"80\":\"v-5c4d654c#_2-1-1-使用-push-to-hub-api\",\"81\":\"v-5c4d654c#_2-1-2-使用-huggingface-hub-python-库\",\"82\":\"v-5c4d654c#_2-1-3-使用-web-页面\",\"83\":\"v-5c4d654c#_2-2-上传-model-files\",\"84\":\"v-5c4d654c#_2-3-1-upload-file-方法\",\"85\":\"v-5c4d654c#_2-3-2-repository-类\",\"86\":\"v-5c4d654c#_2-3-3-git-based-方法\",\"87\":\"v-5c4d654c#_3-建立-model-card\",\"88\":\"v-5c4d654c#model-card-metadata\",\"89\":\"v-2d69903a\",\"90\":\"v-2d69903a#_1-处理不在-hugging-face-hub-上的数据集\",\"91\":\"v-2d69903a#_1-1-加载本地数据集\",\"92\":\"v-2d69903a#_1-2-加载远程数据集\",\"93\":\"v-2d69903a#_2-切片\",\"94\":\"v-2d69903a#_2-1-slicing-and-dicing-数据\",\"95\":\"v-2d69903a#_2-2-创建新列\",\"96\":\"v-2d69903a#_2-3-map-方法\",\"97\":\"v-2d0a776e\",\"98\":\"v-1340a590\",\"99\":\"v-1340a552\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1],\"1\":[1,12],\"2\":[2,9],\"3\":[1,2],\"4\":[2,85],\"5\":[2,67],\"6\":[1,1],\"7\":[1,25],\"8\":[3,14],\"9\":[3,16],\"10\":[3,16],\"11\":[4,68],\"12\":[3],\"13\":[3,27],\"14\":[5,34],\"15\":[3,8],\"16\":[6,71],\"17\":[3,49],\"18\":[3,23],\"19\":[7,24],\"20\":[4,2],\"21\":[5,15],\"22\":[3,59],\"23\":[6,25],\"24\":[3,49],\"25\":[2,25],\"26\":[4,31],\"27\":[3,6],\"28\":[4,115],\"29\":[3,59],\"30\":[9,35],\"31\":[4,46],\"32\":[3,66],\"33\":[2,15],\"34\":[4,61],\"35\":[3,39],\"36\":[3,36],\"37\":[6,47],\"38\":[2,9],\"39\":[4],\"40\":[4,69],\"41\":[5,39],\"42\":[4,26],\"43\":[4,69],\"44\":[4,37],\"45\":[3,28],\"46\":[7,18],\"47\":[5,34],\"48\":[3,12],\"49\":[2,7],\"50\":[3,110],\"51\":[5,74],\"52\":[4,93],\"53\":[2,80],\"54\":[3,13],\"55\":[4,60],\"56\":[3,50],\"57\":[1,22],\"58\":[2,21],\"59\":[2,33],\"60\":[4,83],\"61\":[3,144],\"62\":[6,81],\"63\":[5,42],\"64\":[5,147],\"65\":[4,127],\"66\":[4,43],\"67\":[3,47],\"68\":[5,40],\"69\":[4,31],\"70\":[5,22],\"71\":[8,42],\"72\":[4,56],\"73\":[3,56],\"74\":[6,116],\"75\":[1,26],\"76\":[5,21],\"77\":[2,14],\"78\":[2,19],\"79\":[3],\"80\":[7,107],\"81\":[7,62],\"82\":[6,3],\"83\":[4,17],\"84\":[6,36],\"85\":[4,62],\"86\":[5,126],\"87\":[4,41],\"88\":[3,30],\"89\":[4,39],\"90\":[6,32],\"91\":[2,69],\"92\":[3,26],\"93\":[2],\"94\":[6,403],\"95\":[2,73],\"96\":[4,16],\"97\":[1],\"98\":[1],\"99\":[1]},\"averageFieldLength\":[3.64,47.75408269665131],\"storedFields\":{\"0\":{\"h\":\"首页\"},\"1\":{\"h\":\"介绍\",\"t\":[\"本模块用于记录 Hugging Face NLP Course 的学习过程。\",\"主要围绕以下四部分展开：\",\"🤗 Transformers\",\"🤗 DataSets\",\"🤗 Tokenizers\",\"🤗 Accelerate\"]},\"2\":{\"h\":\"读书笔记-统计学习导论\",\"t\":[\"书是开源的，访问 An Introduction to Statistical Learning 获得。\"]},\"3\":{\"h\":\"简介\",\"t\":[\"本篇文章记录了一些相关概念以及 notation\"]},\"4\":{\"h\":\"1. 概念\",\"t\":[\"error term: 误差项，has mean zero\",\"lease squares 最小二乘法 -> linear regression -> continuous or quantitative values\",\"linear discriminant analysis 线性判别分析, logistic regression -> non-numerical value: categorical or qualitative values\",\"generalized liner model: entire class of statistical learning methods that include both linear and regression\",\"classification and regression trees, generalized additive models 广义加性模型\",\"neural network, support vector machine\",\"scalar 标量\",\"K-nearest neighbor classifier\",\"cross-validation, bootstrap\",\"stepwise selection, ridge regression, principal components regression, lasso\",\"non-linear statistical learning\",\"non-linear additive models\",\"tree-based methods: bagging, boosting, random forests\",\"support vector machines: a set of approaches for performing both linear and non-linear classification\",\"deep learning\",\"survival analysis\",\"unsupervised\",\"principal components analysis, K-means clustering, hierarchical clustering\",\"multiple hypothesis testing 多重假设检验\"]},\"5\":{\"h\":\"2. Notation\",\"t\":[\"n: The number of distinct data points/observations\",\"p: the number of variables that are available for use in making predictions\",\"xi​j: the value of the jth variable for the ith observation\",\"X: n×p matrix\",\"X=​x11​x21​⋮xn1​​x12​x22​⋮xn2​​⋯⋯⋱⋯​x1p​x1p​⋮xnp​​​\",\"xi​: the rows of X. A vector of length p\",\"xi​=​xi​1xi​2⋮xi​p​​\",\"X=​x1T​x2T​⋮xnT​​​\",\"xj​: the column of X\",\"xj​=​x1​jx2​j⋮xn​j​​\",\"X=(x1​​x1​​⋯​xp​​)\",\"yi​:\",\"y=​y1​y2​⋮yn​​​\",\"长度为 n 的向量：小写加粗\",\"长度非 n 的向量：小写\",\"标量：小写\",\"矩阵：大写粗体\",\"随机变量：大写\",\"一些标记：\",\"标量：a∈R\",\"a vector of length k：a∈Rk\",\"r×s 矩阵：A∈Rk×s\",\"input: predictors, independent variables, features, variables\",\"output: response, dependent variable\"]},\"6\":{\"h\":\"jjj\",\"t\":[\"dfsjh\"]},\"7\":{\"h\":\"机器学习的评价指标\",\"t\":[\"在评价一个二分类的机器学习模型的性能时，我们通常可以选择 Accuracy、Precision、Recall、F1 Score 等指标。\",\"使用分类器做预测后，可以绘出混淆矩阵（Confusion Matrix）：\",\"True Positive (TP): 把正样本成功预测为正。\",\"True Negative (TN)：把负样本成功预测为负。\",\"False Positive (FP)：把负样本错误地预测为正。\",\"False Negative (FN)：把正样本错误的预测为负。\"]},\"8\":{\"h\":\"1. 准确性 Accuracy\",\"t\":[\"Accuracy=TP+TN+FP+FNTP+TN​\",\"from sklearn.metric import accuracy_score print('Accuracy: %.3f' % accuracy_score(y_test, y_pred)) \"]},\"9\":{\"h\":\"2. 精确度 Precision\",\"t\":[\"Precision=TP+FPTP​\",\"在预测为Positive的所有数据中，真实Positve的数据到底占多少\",\"from sklearn.metric import precision_score print('Precision: %.3f' % precision_score(y_test, y_pred)) \"]},\"10\":{\"h\":\"3. 召回率 Recall\",\"t\":[\"Recall=TP+FNTP​\",\"在所有的Positive数据中，到底有多少数据被成功预测为Positive\",\"from sklearn.metric import recall_score print('Recall: %.3f' % recall_score(y_test, y_pred)) \"]},\"11\":{\"h\":\"4. F1-分数 F1 Score\",\"t\":[\"F1−score=Precision+Recall2×Precision×Recall​\",\"F1 Score 是 Precision 和 Recall 的综合考量。它赋予Precision score和Recall Score相同的权重，以衡量其准确性方面的性能，使其成为准确性指标的替代方案。\",\"from sklearn.metric import f1_score print('Recall: %.3f' % f1_score(y_test, y_pred)) \",\"Recall v.s. Precision[^first]\",\"Recall > Precision:\",\"当 FN 的成本代价很高，希望尽量避免产生 FN 时，应该着重考虑提高Recall指标。 例如，癌症诊断场景中，False Negative 是得了癌症的病人没有被诊断出癌症，这种情况是最应该避免的。我们宁可把健康人误诊为癌症 (FP)，也不能让真正患病的人检测不出癌症 (FN) 而耽误治疗离世。所以，癌症诊断系统的目标是：尽可能提高Recall值，哪怕牺牲一部分Precision。\",\"Precision > Recall:\",\"当 FP 的成本代价很高，希望尽量避免产生 FP 时，应该着重考虑提高Precision指标。 例如，垃圾邮件分类时，垃圾邮件为Positive，正常邮件为 Negative，False Positive 是把正常邮件识别为垃圾邮件，这种情况是最应该避免的。我们宁可把垃圾邮件标记为正常邮件 (FN)，也不能让正常邮件直接进垃圾箱 (FP)。于是，垃圾邮件分类的目标是：尽可能提高Precision值，哪怕牺牲一部分recall。\",\"继续以癌症诊断场景为，FP 是将没有患癌症的人诊断为癌症，虽然这不致命，但是会给患者带来麻烦\",\"参考：多分类模型Accuracy, Precision, Recall和F1-score的超级无敌深入探讨\"]},\"12\":{\"h\":\"1. Transformer Models\"},\"13\":{\"h\":\"1. NLP 介绍\",\"t\":[\"NLP 的任务不仅仅是理解单个字词的含义，而是要理解上下文的含义。\",\"NLP 任务有很多，比如：\",\"对整个句子进行分类：获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关\",\"对句子中的每个词语进行分类：识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）\",\"生成上下文：用自动生成的文本完成提示，用屏蔽词填充文本中的空白\",\"从文本中提取答案：给定问题和上下文，根据上下文中提供的信息提取问题的答案\",\"根据输入文本生成新的句子：将文本翻译成另一种语言，总结文本\"]},\"14\":{\"h\":\"1.1 术语：Architectures vs. checkpoints\",\"t\":[\"在接下来的学习中，你将会看到 architectures、checkpoints，还有 models 这些术语。\",\"Architecture: 模型框架。每一层的定义、模型中发生的每个操作的定义。\",\"Checkpoints: 对于一个给定 architecture 的权重。\",\"Model: 范语，可能是指 architecture，也可能是指 checkpoints。\",\"如：BERT 是一个 architecture。bert-base-cased 是由 Google 团队为 BERT 训练的初始权重，它是 checkpoints。我们可以说 BERT model，也可以说 bert-base-cased model.\"]},\"15\":{\"h\":\"2. Transformers 能做什么\",\"t\":[\"你可以使用🤗 Transformers 库来创建并使用公开的模型。你可以在模型中心中查找预训练模型。你也可以在 Hub 中上传你自己的模型。\"]},\"16\":{\"h\":\"2.1 快速体验 🤗 Transformers 库\",\"t\":[\"🤗 Transformers 库提供了 pipeline() 函数，它聚合了预训练模型和对应的文本预处理。使用该函数可以直接根据输入返回目标输出。\",\"from transformers import pipeline # 选择任务 sentiment-analysis，创建分类器对象 # 没有指定 model，则会使用默认 model classifier = pipeline(\\\"sentiment-analysis\\\") # 1 传入一个句子 classifier(\\\"I've been waiting for a HuggingFace course my whole life.\\\") # 结果：[{'label': 'POSITIVE', 'score': 0.9598048329353333}] # 2 传入多个句子 classifier( [\\\"I've been waiting for a HuggingFace course my whole life.\\\", \\\"I hate this so much!\\\"] ) # 结果 # [{'label': 'POSITIVE', 'score': 0.9598048329353333}, # {'label': 'NEGATIVE', 'score': 0.9994558691978455}] \",\"目前支持的 pipeline 见 Model 中心。 如果不想使用默认模型，可通过 model 参数传递对应的模型名称。\",\"from transformers import pipeline generator = pipeline(\\\"text-generation\\\", model=\\\"distilgpt2\\\") generator( \\\"In this course, we will teach you how to\\\", max_length=30, num_return_sequences=2, ) \"]},\"17\":{\"h\":\"2.2 局限性 & 偏见\",\"t\":[\"为了在大规模数据上进行预训练，研究员们会收集尽可能多的数据，这其中可能会夹杂一些意识形态或者价值观的刻板印象。\",\"from transformers import pipeline unmasker = pipeline(\\\"fill-mask\\\", model=\\\"bert-base-uncased\\\") result = unmasker(\\\"This man works as a [MASK].\\\") print([r[\\\"token_str\\\"] for r in result]) result = unmasker(\\\"This woman works as a [MASK].\\\") print([r[\\\"token_str\\\"] for r in result]) \",\"['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic'] ['nurse', 'waitress', 'teacher', 'maid', 'prostitute'] \",\"观察结果，有明显的性别相关性，妓女成为了“女性工作”相关的前五名答案之一。\",\"注意\",\"原始模型中很容易掺杂性别歧视、种族歧视等问题，在模型上进一步微调并不会消除这种偏差。\"]},\"18\":{\"h\":\"3. Transformer 背景知识\",\"t\":[\"Transformer 架构于 2017 年 6 月推出。大体上可以将 Transformer 模型分为三类：\",\"GPT-like (自回归（auto-regressive）Transformer 模型)\",\"BERT-like (自编码（auto-encoding）Transformer 模型)\",\"BART/T5-like (序列到序列（sequence-to-sequence）Transformer 模型)\"]},\"19\":{\"h\":\"3.1 Transformer 是语言模型（language model）\",\"t\":[\"包括 GPT、BERT、BART、T5 等 Transformer 模型都是语言模型，即他们已经以自监督学习（self-supervised）的方式在大量文本上进行了训练。\",\"这类模型在其进行训练的语料上进行了理解，但是对于具体问题，它就没那么有针对性了，于是我们需要进行迁移学习（transfer learning）。在迁移学习时，对于具体问题，我们使用人工标注的数据以有监督的方式进行精调（fine-tune）。\"]},\"20\":{\"h\":\"3.2 Transformer 是大模型\",\"t\":[\"实现更好性能的一般策略是增加模型的大小以及预训练的数据量。\"]},\"21\":{\"h\":\"3.3 迁移学习（Transfer Learning）\",\"t\":[\"预训练（Pretraining）指从头开始训练模型。这往往需要使用大规模语料，花费长达数周的时间。\",\"微调（Fine-tuning）是在预训练好的模型上进行进一步的训练。要进行微调，你需要使用预训练模型以及针对特定任务的数据集再次进行训练。进行微调可以有效降低时间、设备成本，使用更小的数据集完成。\"]},\"22\":{\"h\":\"4. Transformer 结构\",\"t\":[\"扩展阅读\",\"推荐 The Illustrated Transformer 这篇文章。在该文章中，作者使用动图清晰地描述了 Transformer 的结构和原理。\",\"Transformer 主要由两部分组成：\",\"Encoders (编码器): 编码器接收输入并构建其表示（即特征）。这意味着对模型进行了优化，以从输入中获得理解。\",\"Decoders (解码器): 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。\",\"这两部分可以单独使用，这取决于你要做什么任务：\",\"Encoder-only 模型（auto-encoding models）：适用于需要理解输入的任务，如句子分类和命名实体识别。\",\"这类模型有 ALBERT, BERT, DistillBERT, ELECTRA, RoBERTa\",\"Decoder-only 模型（auto-regressive models）：适用于生成任务，如文本生成。\",\"这类模型有 CTRL, GPT, GPT-2, Transformer XL\",\"Encoder-decoder 模型（sequence-to-sequence models）：适用于需要根据输入进行生成的任务，如翻译或摘要。预训练这类模型可以使用 encode 或 decoder 的目标。\",\"这类模型有 BART, mBART, Marian, T5\"]},\"23\":{\"h\":\"4.1 注意力层（Attention Layers）\",\"t\":[\"注意力层使得模型对不同位置的字词有着不同的关注程度。\",\"比如，在做文本翻译任务时，将 \\\"I like eating apples\\\" 翻译成中文，在翻译 like 时，模型需要关注 I 和 eating 来获得正确的翻译，而对 apples 的关注度可能小一些；翻译 \\\"It feels like a soft blanket\\\" 时，关注 feels 会帮助模型获得正确的翻译。\"]},\"24\":{\"h\":\"4.2 原始模型\",\"t\":[\"Transformer 最开始是为了翻译任务而设计的。\",\"在训练过程中，encoder 和 decoder 分别接收两种语言的同一个句子。encoder 使用注意力层，可以“看到”该句子中的全部字词。而 decoder 只能看到已经翻译好的字词（即在正在被翻译的字词之前已经生成的部分）。 比如 decoder 已经生成了3个单词，在生成第4个单词时，我们会把前三个单词也作为输入，连同 encoder 输出的部分一起作为 decoder 的输入来生成第4个单词。\",\"为了加快训练，我们会喂给 decoder 完整的目标，但是不允许它使用没有预测的词汇。例如，我们正在预测第4个单词，但是模型看到了目标中的第4个单词，显然这样的模型在实际中不会获得好的效果。\",\"最初的 Transformer 结构如下：\",\"注意，在 decoder 中，第一个注意力层关注所有 decoder 的过去的输入，第二个注意力层，使用了来自 encoder 的输出。因此它能够获得完整的输入句子来对当前词语进行最佳预测。\",\"我们还可以使用注意力遮罩层（attention mask）以使得模型关注某些表示。比如，在批处理句子时，会使用填充的方式使句子长度保持一致，填充的内容无意义，我们不希望模型关注它。\"]},\"25\":{\"h\":\"5. 小结\",\"t\":[\"本节内容介绍了 NLP 任务以及如何使用 🤗 Transformers 中的 pipeline() 函数来执行不同的 NLP 任务。你可以在模型中心中查找模型，按照 Model Card 中的说明或者使用页面上的 inference API 进行使用。\",\"我们简单介绍了 Transformer 的结构，如果你想做进一步了解，推荐阅读 The Illustrated Transformer。\"]},\"26\":{\"h\":\"2. 使用 🤗 Transformers\",\"t\":[\"Transformer 模型一般都很大，训练或者部署是一项复杂的任务。🤗 Transformers 库提供了简单的API，使得用户可以通过它来加载、训练、保存所有的 Transformer 模型。\",\"我们将使用 model 和 tokenizer 来实现在上一节中 pipeline() 函数完成的任务。然后会介绍 model API，看一看 model 类和 configuration 类，了解如何加载模型、它是怎么处理数字输入并输出预测的。\",\"接下来还有 tokenizer API。tokenizer 负责将文本转成数字表示（以作为神经网络的输入），并负责将数字表示转化成文本。我们还会展示如何批处理多个句子。\"]},\"27\":{\"h\":\"1. Pipeline 都做了什么\",\"t\":[\"上一节中提到的 pipeline() 函数实际上经过了以下几个步骤：预处理、将输入传递给模型、后处理\"]},\"28\":{\"h\":\"1.1 使用 tokenizer 进行预处理\",\"t\":[\"Transformer 模型不能直接处理原始文本，于是要先将文本输入转换成数字表示。实际上，Transformer 模型只接收 tensor 作为输入。\",\"tokenizer 的处理步骤：\",\"将文本切分成 tokens (可能是 words, subwords, 或者 symbols)\",\"将每一个 token 映射到一个数字上\",\"添加可能对模型有用的其他输入\",\"显然，使用模型进行预测时使用的上述操作应该和预训练时的操作一致。我们可以使用 AutoTokenizer 类以及它的 from_pretrained() 函数来实现这一点。使用模型的 checkpoints 名称，它会下载对应模型的 tokenizer 并缓存下来。\",\"在上一节中，我们使用了 sentiment-analysis\",\"from transformers import pipeline classifier = pipeline(\\\"sentiment-analysis\\\") classifier( [ \\\"I've been waiting for a HuggingFace course my whole life.\\\", \\\"I hate this so much!\\\", ] ) \",\"上面模型的默认 checkpoints 是 sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-english，使用 AutoTokenizer 创建 tokenizer 对象\",\"from transformers import AutoTokenizer checkpoint = \\\"distilbert-base-uncased-finetuned-sst-2-english\\\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) \",\"将文本传递给 tokenizer\",\"raw_inputs = [ \\\"I've been waiting for a HuggingFace course my whole life.\\\", \\\"I hate this so much!\\\", ] # padding, truncation 会在之后介绍；return_tensors 为 pt, 即 pytorch inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\\\"pt\\\") print(inputs) \",\"{ 'input_ids': tensor([ [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0] ]), 'attention_mask': tensor([ [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] ]) } \"]},\"29\":{\"h\":\"1.2 model\",\"t\":[\"与 AutoTokenizer 类似，🤗 Transformers 库还提供了 AutoModel class，它也有 from_pretrained() 方法。\",\"from transformers import AutoModel checkpoint = \\\"distilbert-base-uncased-finetuned-sst-2-english\\\" model = AutoModel.from_pretrained(checkpoint) \",\"上面的代码下载了 distilbert-base-uncased-finetuned-sst-2-english 的 checkpoints （如果还在同一环境中，那么它在之前已经被缓存了），并实例化了对应的模型。\",\"这个架构只包括最基本的 Transformer 模块，即下图中 Transformer Network 部分。\",\"注意\",\"Transformer Network 即我们在上一节中谈到的 Transformer 架构，这里只是画成 Embedding + Layers\",\"当然 🤗 Transformers 还提供了不同的架构。下面列举了一部分\",\"*Model (retrieve the hidden states)\",\"*ForCausalLM\",\"*ForMaskedLM\",\"*ForMultipleChoice\",\"*ForQuestionAnswering\",\"*ForSequenceClassification\",\"*ForTokenClassification\",\"and others 🤗\"]},\"30\":{\"h\":\"1.2.1 Model 输出（hidden states 或 features）：高维张量\",\"t\":[\"Transformer 模块输出的张量通常很大，它有以下三个维度：\",\"Batch size: 每次处理的序列长度（上述例子中为2）\",\"Sequence length: 序列的数字表示的长度（上述例子中为16）\",\"Hidden size: 每个模型输入的张量维度。通常很大（小模型可能是768，在大一些的模型中可能是3072甚至更大）\",\"我们将上面使用 tokenizer 得到的输入传递给 model，看看它的输出\",\"outputs = model(**inputs) print(outputs.last_hidden_state.shape) \",\"torch.Size([2, 16, 768]) \"]},\"31\":{\"h\":\"1.2.2 Model heads\",\"t\":[\"Model heads 通常有一层或多层线性层组成，以 hidden states 作为输入，将这些高维张量映射到不同的维度上。\",\"我们需要一个有序列分类（sequence classification）head 的模型，于是我们不用 AutoModel 类，我们使用 AutoModelForSequenceClassification。\",\"from transformers import AutoModelForSequenceClassification checkpoint = \\\"distilbert-base-uncased-finetuned-sst-2-english\\\" model = AutoModelForSequenceClassification.from_pretrained(checkpoint) outputs = model(**inputs) print(outputs.logits.shape) \",\"torch.Size([2, 2]) \",\"我们分析的是两个句子，判断每个句子是 positive 还是 negative，所以输出维度是 2x2\"]},\"32\":{\"h\":\"1.3 后处理\",\"t\":[\"print(outputs.logits) \",\"tensor([[-1.5607, 1.6123], [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>) \",\"对于第一个句子，我们预测的是 [-1.5607, 1.6123]，这是logits（所有的 🤗 Transformers 模型输出的都是 logits）。实际上我们更希望得到类似于概率的结果，于是我们将它输入至 SoftMax 层中。\",\"import torch predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) print(predictions) \",\"tensor([[4.0195e-02, 9.5980e-01], [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>) \",\"我们可以通过查看 model config 的 id2label 属性来查看对应的 label\",\"model.config.id2label \",\"{0: 'NEGATIVE', 1: 'POSITIVE'} \",\"到此为止，我们通过三个步骤（使用 tokenizer 进行预处理，将输入传递给 model，后处理）得到最终的结论：\",\"第一个句子: NEGATIVE: 0.0402, POSITIVE: 0.9598\",\"第二个句子: NEGATIVE: 0.9995, POSITIVE: 0.0005\"]},\"33\":{\"h\":\"2. Models\",\"t\":[\"我们详细介绍下 model。AutoModel 类可以根据 checkpoint 来实例化任何模型。它根据 checkpoint 来确定模型结构，并实例化模型。如果你确切知道你想使用什么类型的模型，你可以直接使用对应的 model 类。\",\"下面将使用 BERT model。\"]},\"34\":{\"h\":\"2.1 创建 Transformer\",\"t\":[\"初始化 BERT 模型的第一步是加载配置对象。\",\"from transformers import BertConfig, BertModel config = BertConfig() model = BertModel(config) \",\"配置中包含很多建立模型要用到的属性。\",\"打印 config 的内容\",\"BertConfig { \\\"attention_probs_dropout_prob\\\": 0.1, \\\"classifier_dropout\\\": null, \\\"hidden_act\\\": \\\"gelu\\\", \\\"hidden_dropout_prob\\\": 0.1, \\\"hidden_size\\\": 768, \\\"initializer_range\\\": 0.02, \\\"intermediate_size\\\": 3072, \\\"layer_norm_eps\\\": 1e-12, \\\"max_position_embeddings\\\": 512, \\\"model_type\\\": \\\"bert\\\", \\\"num_attention_heads\\\": 12, \\\"num_hidden_layers\\\": 12, \\\"pad_token_id\\\": 0, \\\"position_embedding_type\\\": \\\"absolute\\\", \\\"transformers_version\\\": \\\"4.29.1\\\", \\\"type_vocab_size\\\": 2, \\\"use_cache\\\": true, \\\"vocab_size\\\": 30522 } \"]},\"35\":{\"h\":\"2.1.1 不同的加载方法\",\"t\":[\"使用默认的配置来创建 model 时，model 会被随机初始化。\",\"你可以直接使用随机初始化的 model，不过使用效果肯定很差，而重新训练又需要大量的时间和数据。我们不妨加载已经训练好的模型，这要用到 from_pretrained() 方法。\",\"from transformers import BertModel model = BertModel.from_trained(\\\"bert-base-cased\\\") \",\"当然你也可以将 BertModel 替换为 AutoModel\",\"from transformers import AutoModel model = AutoModel.from_trained(\\\"bert-base-cased\\\") \",\"你可以在这个 model card 中查看 BERT 模型的更多细节。\",\"提示\",\"缓存路径为 ~/.cache/huggingface/modules 你可以通过设置 HF_HOME 环境变量来自定义缓存路径。\"]},\"36\":{\"h\":\"2.1.2 保存方法\",\"t\":[\"使用 save_pretrained() 方法来保存模型\",\"model.save_pretrained(\\\"directory_on_my_computer\\\") \",\"这会保存两个文件\",\"ls directory_on_my_computer config.json pytorch_model.bin \",\"你可以在 config.json 中看到建立模型所需的属性。该文件中也有一些 metadata，比如 checkpoint 的来源或路径、你最后一次保存 checkpoint 时使用的 🤗 Transformers 版本\",\"python_model.bin 被称为状态字典（state dictionary）。其中记录了模型的权重。\",\"这两个文件相辅相成，config.json 提供了模型的架构信息，python_model.bin 提供了模型权重。\"]},\"37\":{\"h\":\"2.2 使用 Transformer 进行推理（inference）\",\"t\":[\"在 2.1 中你已经看到了如何加载以及保存使用模型，下面我们来使用模型进行预测。\",\"在 1.1 中，我们已经过如何使用 tokenizer 将文本转化为张量，它将输入的文本转化为数字：\",\"获得 input IDs: model_input\",\"给定文本：\",\"sequences = [\\\"Hello!\\\", \\\"Cool.\\\", \\\"Nice!\\\"] \",\"经 tokenizer 获得 input IDs:\",\"encoded_sequences = [ [101, 7592, 999, 102], [101, 4658, 1012, 102], [101, 3835, 999, 102], ] \",\"Transformer 只接收 tensor，将上面的 list 转化成 tensor：\",\"import torch model_inputs = torch.tensor(encoded_sequences) \",\"现在可以将 model_input 传递给 model 了\",\"output = model(model_inputs) \",\"model 可以接收很多参数，其中 input IDs 只必传的。我们将在未来在讨论其他参数。\"]},\"38\":{\"h\":\"3. Tokenizers\",\"t\":[\"模型只能处理数字，tokenizer 的作用是将文本转化为模型可以处理的数字。它的目标是找到最有意义的表示，并尽可能小。\",\"下面介绍几种 tokenization 算法。\"]},\"39\":{\"h\":\"3.1 tokenization 算法\"},\"40\":{\"h\":\"3.1.1 Word-based\",\"t\":[\"可以使用空格来将句子切分为字词：\",\"tokenized_text = \\\"Jim Henson was a puppeteer\\\".split() print(tokenized_text) # res # ['Jim', 'Henson', 'was', 'a', 'puppeteer'] \",\"也有针对标点符号增加了额外规则的 tokenizer。\",\"使用 word-based tokenizer，我们最终会得到一个非常大的词汇表，此表的大小由语料中的独立 token 数决定。每个字词都被分配了一个 ID，从 0 到整个词表大小。模型使用这些 ID 来表示每个字词。\",\"如果我们想使用这种 tokenizer 来覆盖某门语言，那将会生成大量 token。例如，英语中有 500,000 个单词，于是构建每个单词到 input_id 的映射要有 500,000 个。除此之外，'dog' 和 'dogs', 'run' 和 'running' 会被分别构建不同的 input_id, 没有体现出它们之间的相似与联系。\",\"另外，我们还需要自定义一个 token 来表示不在词表中的字词，也就是 'unknown' token。一般用 '[UNK]' 或 '' 表示。如果某个 tokenizer 产生了大量 unknown token，这意味着它无法检索到一个词的合理表示，且你在这个过程中丢失了信息。我们希望 tokenizer 会将尽量少的字词标记为 unknown token。\",\"下面介绍 character-based tokenizer，它可以减少 unknown token 的产生。\"]},\"41\":{\"h\":\"3.1.2 Character-based\",\"t\":[\"Character-based tokenizer 将文本切分成 characters，而不是 words。这样做有两点好处：\",\"词表将有效地减小\",\"unknown tokens 也会减少（因为每个字词都是通过 character 构建的）\",\"当然，这也会产生一些问题。首先，character 可能本身没有含义（相对于 word 来说。但也因语言而异，比如中文字符会比拉丁系语言的字符携带更多信息）。另外，模型需要处理大量 token（对于 word-based 来说的一个 word 只需要 1 个 token，而使用 character-based，可能有十几个 tokens 要处理）、\",\"考虑到上面两种技术，又提出了第三种方式：subword tokenization\"]},\"42\":{\"h\":\"3.1.3 Subword tokenization\",\"t\":[\"subword tokenization 的原则是：经常使用的词不应该再被切分为更小的子词，比较少用的词可以分解为有意义的子词。\",\"比如：annoyingly 可能被切分成 annoy、ing、ly\",\"以下 tokenization 是 subword tokenization：\",\"Byte-level BPE, GPT-2 使用这种方式\",\"WordPiece, BERT 使用这种方式\",\"SentencePiece or Unigram, 在一些多语言模型中使用\",\"...\"]},\"43\":{\"h\":\"3.2 加载 & 保存\",\"t\":[\"加载和保存 tokenizer 分别使用方法 from_pretrained() 和 save_pretrained(). 使用该方法会加载或保存 tokenizer 使用的算法（类似于 model 的 architecture），还会加载或保存对应的词表（类似于 model 的 weight）\",\"加载 BERT 的 tokenizer\",\"from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(\\\"bert-base-cased\\\") \",\"也可以使用 AutoTokenizer\",\"from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-cased\\\") \",\"然后我们就可以使用 tokenizer 将文本转化成 input_ids\",\"tokenizer(\\\"Using a Transformer network is simple\\\") # result: # {'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], # 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], # 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]} \",\"保存 tokenizer\",\"tokenizer.save_pretrained(\\\"directory_on_my_computer\\\") \",\"我们将在 Chapter 3 中来讨论 token_type_ids。并在 4.3 中讨论 attention_mask。下面我们先介绍下 input_ids 如何生成，为此我们要查看 tokenizer 的中间方法。\"]},\"44\":{\"h\":\"3.3 编码（Encoding）\",\"t\":[\"将文本转换成数字的过程叫做编码（Encoding）。Encoding 的分为两步：tokenization，然后转化为 input IDs.\",\"第一步是将文本切分为 tokens。实现这一步有不同的规则（见 3.1），所以我们需要用我们所选的模型的名称来实例化 tokenizer，以确保使用和预训练时相同的规则。\",\"第二步是将 tokens 转化为数字表示，所以我们可以用它们构建张量并把张量提供给模型。为了实现这一步骤，tokenizer 需要一个词表（vocabulary），我们在使用 from_pretrained() 来实例化 tokenizer 的时候已经下载好了。同样地，这个词表和预训练时的词表是相同的。\",\"下面分别介绍这两步。注意，在使用过程中直接调用 tokenizer 就可以，下面的分步调用只是为了让大家更清楚 encoding 的两个步骤分别做了什么。\"]},\"45\":{\"h\":\"3.3.1 Tokenization\",\"t\":[\"Tokenization 过程可以使用 tokenizer 的 tokenize() 方法实现：\",\"from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\\\"bert-based-cased\\\") sequence = \\\"Using a Transformer network is simple\\\" tokens = tokenizer.tokenize(sequence) print(tokens) # result: # ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple'] \"]},\"46\":{\"h\":\"3.3.2 将 tokens 转换为 input IDs\",\"t\":[\"该过程通过 convert_tokens_to_ids() 实现。\",\"ids = tokenizer.convert_tokens_to_ids(tokens) print(ids) # result: # [7993, 170, 11303, 1200, 2443, 1110, 3014] \"]},\"47\":{\"h\":\"3.4 解码（Decoding）\",\"t\":[\"解码（decoding）和编码（encoding）的该过程相反，将词表索引转化成字符串，可以使用 decode() 方法来实现\",\"decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014]) print(decoded_string) # result: # 'Using a Transformer network is simple' \",\"注意，decode() 方不仅将索引转化为了 tokens，还将同一个词中的 tokens 组合在一起了。\"]},\"48\":{\"h\":\"3.5 小结\",\"t\":[\"通过本章的学习，你应该了解 tokenizer 的原子操作：tokenization、将 tokens 转化成 input_ids、将 ids 转化为字符串。\"]},\"49\":{\"h\":\"4. 处理多个序列\",\"t\":[\"在之前的例子中，我们对小长度的序列进行了处理。我们需要思考以下问题：\",\"如何处理多个序列\",\"如何处理不同长度的多个序列\",\"词汇表索引是唯一能够使模型正常工作的输入吗\",\"是否存在序列过长的问题\"]},\"50\":{\"h\":\"4.1 批处理\",\"t\":[\"import torch from transformers import AutoTokenizer, AutoModelForSequenceClassification checkpoint = \\\"distilbert-base-uncased-finetuned-sst-2-english\\\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequence = \\\"I've been waiting for a HuggingFace course my whole life.\\\" tokens = tokenizer.tokenize(sequence) ids = tokenizer.convert_tokens_to_ids(tokens) input_ids = torch.tensor(ids) # This line will fail. model(input_ids) # IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1) \",\"为什么会出错呢？🤗 Transformers 模型默认接收多个句子作为输入，但我们只传递来一个序列。\",\"在此之前我们直接调用 tokenizer 时，在顶部加了一个维度：\",\"tokenized_input = tokenizer(sequence, return_tensors=\\\"pt\\\") print(tokenized_inputs[\\\"input_ids\\\"]) # result: # tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, # 2607, 2026, 2878, 2166, 1012, 102]]) \",\"tokenizer 可以将序列转化成特定结构的 tensor，通过 return_tensors 参数设置\",\"sequences = [\\\"I've been waiting for a HuggingFace course my whole life.\\\", \\\"So have I!\\\"] # Returns PyTorch tensors model_inputs = tokenizer(sequences, padding=True, return_tensors=\\\"pt\\\") # Returns TensorFlow tensors model_inputs = tokenizer(sequences, padding=True, return_tensors=\\\"tf\\\") # Returns NumPy arrays model_inputs = tokenizer(sequences, padding=True, return_tensors=\\\"np\\\") \",\"我们对起初的代码进行修改：\",\"import torch from transformers import AutoTokenizer, AutoModelForSequenceClassification checkpoint = \\\"distilbert-base-uncased-finetuned-sst-2-english\\\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequence = \\\"I've been waiting for a HuggingFace course my whole life.\\\" tokens = tokenizer.tokenize(sequence) ids = tokenizer.convert_tokens_to_ids(tokens) input_ids = torch.tensor([ids]) print(input_ids) # tensor([[ 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]]) output = model(input_ids) print(output.logits) # tensor([[-2.7276, 2.8789]], grad_fn=<AddmmBackward0>) \",\"Batching 是指一次向模型传递多个句子\",\"batched_ids = [ids, ids] \"]},\"51\":{\"h\":\"4.2 填充（Padding）\",\"t\":[\"当进行批处理时，如果两个序列的长度不一样怎么办（对于 tensor 来说，它必须是矩阵，即每个序列的表示应该是一样长的），为了解决这个问题，我们将填充（pad）输入。\",\"对于短的序列，我们使用 padding token 来填充，使其和最长的序列一样长。\",\"# 我们无法将这个 batch 转化成 tensor bached_id_origin = [ [200, 200, 200], [200, 200] ] padding_id = 100 # 使用 padding batched_ids = [ [200, 200, 200], [200, 200, padding_id] ] \",\"可以在 tokenizer.pad_token_id 中获取 padding token id。\",\"model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequence1_ids = [[200, 200, 200]] sequence2_ids = [[200, 200]] batched_ids = [ [200, 200, 200], [200, 200, tokenizer.pad_token_id], ] print(model(torch.tensor(sequence1_ids)).logits) print(model(torch.tensor(sequence2_ids)).logits) print(model(torch.tensor(batched_ids)).logits) \",\"tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>) tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>) tensor([[ 1.5694, -1.3895], [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>) \",\"观察上面的结果，batched_ids 的结果的第二行与 sequence2_ids 的结果不一样，这显然是不应该的。\",\"造成不一致是因为，Transformer 中的 attention layers 将每个 token 都作为上下文考虑进去了。那么 padding tokens 也会被考虑进去。如果想让两次的结果相同，需要告诉 attention layer 忽略 padding tokens。这要通过 attention mask 来实现。\"]},\"52\":{\"h\":\"4.3 Attention Masks\",\"t\":[\"attention mask 也是 tensor，它和 input IDs tensor 结构相同，元素只有 0 和 1：0 表示该位置是 padding tokens，attention layers 应该忽略它。\",\"我们将 attention_mask 作为参数传递给 model，再观察结果，和 sequence2_ids 的结果一致了：\",\"batched_ids = [ [200, 200, 200], [200, 200, tokenizer.pad_token_id], ] attention_mask = [ [1, 1, 1], [1, 1, 0] ] print(model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)).logits) # tensor([[ 1.5694, -1.3895], # [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>) \",\"sequences = [\\\"I've been waiting for a HuggingFace course my whole life.\\\", \\\"So have I!\\\"] model_inputs = tokenizer(sequences) print(model_inputs) # Will pad the sequences up to the maximum sequence length model_inputs = tokenizer(sequences, padding=\\\"longest\\\") print(model_inputs) # Will pad the sequences up to the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer(sequences, padding=\\\"max_length\\\") print(model_inputs) # Will pad the sequences up to the specified max length model_inputs = tokenizer(sequences, padding=\\\"max_length\\\", max_length=8) print(model_inputs) \",\"输出\",\"{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]} {'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]} {'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]} {'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]]} \"]},\"53\":{\"h\":\"4.4 长序列\",\"t\":[\"Transformer 模型能处理的序列长度是有限的，大都在 512 至 1024 个 tokens 之间。如果传入了大于最大限度的序列会崩溃，有两种方式来解决这个问题：\",\"换用支持更长序列的模型。像 Longformer 和 LED 就能处理比较长的序列。\",\"截断序列。\",\"sequence = sequence[:max_sequence_length] \",\"可以使用 tokenizer 进行截断\",\"sequences = [\\\"I've been waiting for a HuggingFace course my whole life.\\\", \\\"So have I!\\\"] # Will truncate the sequences that are longer than the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer(sequences, truncation=True) print(model_inputs) # Will truncate the sequences that are longer than the specified max length model_inputs = tokenizer(sequences, max_length=8, truncation=True) print(model_inputs) \",\"{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]} {'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]} \"]},\"54\":{\"h\":\"5. Tokenizer API\",\"t\":[\"我们可以使用 tokenizer 来 padding、truncate 序列，也可以指定 return_tensors 的类型（4 中已有对应示例）。\"]},\"55\":{\"h\":\"5.1 特殊 token\",\"t\":[\"sequence = \\\"I've been waiting for a HuggingFace course my whole life.\\\" model_inputs = tokenizer(sequence) print(model_inputs[\\\"input_ids\\\"]) tokens = tokenizer.tokenize(sequence) ids = tokenizer.convert_tokens_to_ids(tokens) print(ids) \",\"[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102] [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012] \",\"首位各添加了一个 token ID，我们将其进行 decode：\",\"print(tokenizer.decode(model_inputs[\\\"input_ids\\\"])) print(tokenizer.decode(ids)) \",\"\\\"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\\\" \\\"i've been waiting for a huggingface course my whole life.\\\" \",\"tokenizer 添加了两个特殊词 “[CLS]” 和 “[SEP]”。这是在预训练时使用的，所以我们在使用该模型做推理的时候也应该在首尾加上它们。不同的模型会使用不同的特殊词，有些模型不用特殊词，有些模型只在句首加特殊词，有些模型只在句尾加特殊词。不论如何，tokenizer 总是知道应该是怎样的，并会为你处理好它。\"]},\"56\":{\"h\":\"5.2 小结\",\"t\":[\"最后让我们看一看如何使用 tokenizer 和 model 来进行 inference，我们使用了 padding（为了批处理），使用了 truncate（为了处理长序列）。\",\"import torch from transformers import AutoTokenizer, AutoModelForSequenceClassification checkpoint = \\\"distilbert-base-uncased-finetuned-sst-2-english\\\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequences = [\\\"I've been waiting for a HuggingFace course my whole life.\\\", \\\"So have I!\\\"] tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\\\"pt\\\") output = model(**tokens) \"]},\"57\":{\"h\":\"总结\",\"t\":[\"Transformer 的基本结构\",\"tokenization pipeline 的组成\",\"如何使用 Transformer model\",\"如何利用 tokenizer 将文本转化为 tensor\",\"使用 tokenizer 和 model 来进行推理\",\"input IDs 的限制，了解 attention masks\",\"尝试了多种可配置的 tokenizer 方法\"]},\"58\":{\"h\":\"3. 微调预训练模型\",\"t\":[\"上一篇文章中介绍了如何使用 tokenizer 和预训练模型来进行推理。接下来我们将介绍如何在自己的数据集上进行微调（Fine-tuning）。在本篇文章中，你将了解到：\",\"如何从 Hub 中准备大型数据集\",\"如何使用 high-level API 微调模型\",\"如何使用自定义训练过程\",\"如何利用 🤗 Accelerate 库在任何分布式设备上轻松运行自定义训练过程\"]},\"59\":{\"h\":\"1. 处理数据\",\"t\":[\"注\",\"如果你不想了解这些细节，或者想先运行数据处理的整体代码，请直接从 2 开始阅读\",\"Hub 中不仅有 models，还有很多 datasets.\",\"我们将使用 MRPC（Microsoft Research Paraphrase Corpus）数据集，它是 GLUE benchmark 的十个数据集之一，该 benchmark 用来衡量 ML 模型在 10 个不同文本分类任务中的性能。MRPC 数据集有 5801 个句子对，每个句子对有一个标签来指明两个句子是否同义。\"]},\"60\":{\"h\":\"1.1 从 Hub 中加载数据集\",\"t\":[\"🤗 Datasets 库提供了简单易用的命令来下载并缓存 Hub 中的数据集\",\"from datasets import load_dataset raw_datasets = load_dataset(\\\"glue\\\", \\\"mrpc\\\") raw_datasets \",\"DatasetDict({ train: Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx'], num_rows: 3668 }) validation: Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx'], num_rows: 408 }) test: Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx'], num_rows: 1725 }) }) \",\"我们得到了一个 DatasetDict 对象，它有 training set, validation set, 和 test set。每一个集合中包含这样几列：sentence1、sentence2、label、idx，以及行数（即数据数量）。\",\"提示\",\"缓存路径为 ~/.cache/huggingface/datasets 你可以通过设置 HF_HOME 环境变量来自定义缓存路径。\",\"你可以先看看数据：\",\"raw_train_dataset = raw_datasets[\\\"train\\\"] raw_train_dataset[0] \",\"{'sentence1': 'Amrozi accused his brother , whom he called \\\" the witness \\\" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \\\" the witness \\\" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0} \",\"可以通过查看 raw_train_dataset 的 features 来查看 label 的含义。0 是 not_equivalent，1 是 equivalent。\",\"raw_train_dataset.features \",\"{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)} \"]},\"61\":{\"h\":\"1.2 数据集预处理\",\"t\":[\"我们需要将文本转化成数字表示，这样模型才能进行处理。\",\"from transformers import AutoTokenizer checkpoint = \\\"bert-base-uncased\\\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) tokenized_sentences_1 = tokenizer(raw_datasets[\\\"train\\\"][\\\"sentence1\\\"]) tokenized_sentences_2 = tokenizer(raw_datasets[\\\"train\\\"][\\\"sentence2\\\"]) \",\"上面的代码确实将文本转化成了数字表示，但是我们需要传入句子对\",\"inputs = tokenizer(\\\"This is the first sentence.\\\", \\\"This is the second one.\\\") print(inputs) print(tokenizer.convert_ids_to_tokens(inputs[\\\"input_ids\\\"])) \",\"{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} ['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]'] \",\"我们在上一篇文章中介绍了 input_ids 和 attention_mask，没有介绍 token_type_ids。在这个例子中，token_type_ids 表示输入的哪部分是第一个句子，哪一个是第二个句子。\",\"我们可以看到模型需要的输入形式是 [CLS] sentence1 [SEP] sentence2 [SEP]（使用不同的 checkpoints 时该结构会不一样），所以 token_type_ids（使用其他的 checkpoints 时，可能不会有 token_type_ids） 的值是\",\"['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]'] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] \",\"我们可以为 tokenizer 提供句子对列表\",\"tokenized_dataset = tokenizer( raw_datasets[\\\"train\\\"][\\\"sentence1\\\"], raw_datasets[\\\"train\\\"][\\\"sentence2\\\"], padding=True, truncation=True, ) \",\"这是有用的，但是也有一些不足。tokenization 过程中需要在 RAM 中保存整个数据集，如果你的 RAM 空间不足将会有问题。\",\"我们使用 Dataset.map() 方法来构建数据集，它不会将整个 dataset 都加载到内存中，且结果会被缓存，下次执行时不需要重复计算。首先创建函数对输入进行 tokenization：\",\"def tokenized_function(example): return tokenizer(example[\\\"sentence1\\\"], example[\\\"sentence2\\\"], truncation=True) \",\"我们将 padding 参数去掉了，因为将所有的数据 padding 到最大长度效率不高，更好的做法是当我们构建一个 batch 时 pad 该 batch 中的数据，这样我们只需要将长度填充为该 batch 中的最大长度。\",\"# 设置 batched 为 True，使得同时对数据集中的多个元素同时做处理，加速了预处理 tokenized_datasets = raw_datasets.map(tokenized_function, batched=True) tokenized_datasets \",\"DatasetDict({ train: Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], num_rows: 3668 }) validation: Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], num_rows: 408 }) test: Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], num_rows: 1725 }) }) \",\"🤗 Datasets 库用 map() 函数的处理方式是想数据集中添加新的字段，新的字段即预处理函数返回的字典中的每个键。\",\"可以通过传递 num_proc 参数给 map() 以启动多进程。🤗 Tokenizers 库已经使用了多线程，于是这里我们没有启用多进程。\",\"最后一项任务就是在每个 batch 进行 padding，即 dynamic padding.\"]},\"62\":{\"h\":\"1.3 动态填充（Dynamic Padding）\",\"t\":[\"在批处理中这将数据整理到一个 batch 的函数称为 collate function. 它是构建 DataLoader 时的一个参数，默认是一个函数，它把你的数据集转化为 Pytorch tensors，并将它们拼接起来。\",\"🤗 Transformers 库通过 DataCollatorWithPadding 提供了 collate function。它接收一个 tokenizer (以获取 padding token、确定是在输入的左侧还是右侧进行 padding)。\",\"from transformers import DataCollatorWithPadding data_collator = DataCollatorWithPadding(tokenizer=tokenizer) \",\"我们可以验证一下 data_collator 是否能在 batch 上进行正确的 padding\",\"samples = tokenized_datasets[\\\"train\\\"][:8] samples = {k: v for k, v in samples.items() if k not in [\\\"idx\\\", \\\"sentence1\\\", \\\"sentence2\\\"]} [len(x) for x in samples[\\\"input_ids\\\"]] # [50, 59, 47, 67, 59, 50, 62, 32] \",\"我们取了 train set 中前 8 个作为一个 batch，去掉了 idx、sentence1、sentence2 字段。\",\"input_ids 的最大长度为 67，则这个 batch 经过 padding 之后将会被填充到 67\",\"batch = data_collator(samples) {k: v.shape for k, v in batch.items()} \",\"{'attention_mask': torch.Size([8, 67]), 'input_ids': torch.Size([8, 67]), 'token_type_ids': torch.Size([8, 67]), 'labels': torch.Size([8])} \",\"现在，我们已经将原数数据转化成模型可处理的 batches，下面我们要进行微调了。\"]},\"63\":{\"h\":\"2. 使用 Trainer API 进行微调\",\"t\":[\"🤗 Transformers 提供了 Trainer 类来微调各种预训练模型。最难的步骤大概是为 Trainer.train() 配置运行环境。\",\"我们快速回顾一下上一部分的预处理：\",\"from datasets import load_dataset from transformers import AutoTokenizer, DataCollatorWithPadding raw_dataset = load_dataset(\\\"glue\\\", \\\"mrpc\\\") checkpoint = \\\"bert-base-uncased\\\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) def tokenize_function(example): return tokenizer(example[\\\"sentence1\\\"], example[\\\"sentence2\\\"], truncation=True) tokenized_datasets = raw_dataset.map(tokenize_function, batched=True) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) \"]},\"64\":{\"h\":\"2.1 训练（Training）\",\"t\":[\"第一步，在我们定义 Trainer 之前我们要先定义 TrainingArguments 类，它包含 Trainer 训练和评估时所用的全部超参。必须提供的唯一参数是训练模型的存储路径，也是 checkpoints 的路径。其余的参数都可以设置为默认值，对于基础的微调来说表现得也很不错。\",\"from transformers import TrainingArguments training_args = TrainingArguments(\\\"test-trainer\\\") \",\"提示\",\"如果你想在训练过程中自动上传你的模型到 Hub 上，可以在 TrainingArguments 中传递 push_to_hub=True。我们将在 Chapter 4 中详细介绍。\",\"🤗 官方示例 accelerate 版本错误解决方案\",\"在 CoLab 上运行 🤗 官方示例时，如果遇到下面的错误，\",\"ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U` \",\"可以尝试下面方法，首先更新 accelerate 和 transformers\",\"!pip install -U accelerate !pip install -U transformers \",\"然后 Restart runtime\",\"第二步，定义模型。\",\"from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) \",\"在实例化 model 时你会看到 warning，这是因为 BERT 没有对句子对进行过预训练，于是预训练模型的 head 被替换成了做 sequence classification 的 head。\",\"现在我们可以定义 Trainer 了，将我们之前构造的对象（model, training_args, training & validation datasets, data_collator 以及 tokenizer）作为参数传递。\",\"from transformers import Trainer trainer = Trainer( model, training_args, train_dataset=tokenized_datasets[\\\"train\\\"], eval_dataset=tokenized_datasets[\\\"validation\\\"], data_collator=data_collator, tokenizer=tokenizer, ) \",\"注意\",\"当在 Trainer 中传递 tokenizer 时，Trainer 使用的默认 data_collator 和我们之前使用 DataCollatorWithPadding 定义的是一样的。所以我们可以不传递 data_collator。\",\"调用 Trainer 的 train() 方法，我们就可以在自己的数据集上微调模型了。\",\"trainer.train() \",\"运行上面代码后，我们将开始微调，每 500 steps 会输出一次 training loss。但是它不会告诉你这个模型表现得怎么样，因为：\",\"我们没有配置 Trainer 让它在训练时进行评估。想要进行评估可以设置 evaluation_strategy 为 “steps”（每eval_steps 进行评估） 或 “epoch”（在每个 epoch 之后进行评估）。\",\"我们没有为 Trainer 提供评估的方法。我们可以传递通过 compute_metrics() 函数提供计算模型性能的方法。没有提供该方法的话，评估时会直接输出 loss，并不直观。\"]},\"65\":{\"h\":\"2.2 评估（Evaluation）\",\"t\":[\"我们来看一下如何构建 compute_metrics() 函数并在训练时使用它。\",\"可以使用 Trainer.predict() 方法进行预测。\",\"predictions = trainer.predict(tokenized_datasets[\\\"validation\\\"]) print(predictions.predictions.shape, predictions.label_ids.shape) # (408, 2) (408,) import numpy as np # predictions.predictions 的输出是 logits，为了获得预测结果，可以将 logits 的最大值的取出 preds = np.argmax(predictions.predictions, axis=-1) \",\"Trainer.predict() 的输出是一个命名元祖，有三个字段：predictions, label_ids, 和 metrics。metrics 字段包含 loss、时间 metrics（预测用了多长时间，总计时长、平均时长）。如果我们自定义了 compute_metrics() 函数并传递给了 Trainer，那么该字段还会包括 compute_metrics() 函数返回的 metrics。\",\"构建 compute_metrics() 需要用到 🤗 Evaluate 库。我们可以使用 evaluate.load() 函数加载与 MRPC 数据集有关的 metrics，它返回的对象有 compute() 方法，可以用来进行 metric calculation。\",\"import evaluate metric = evaluate.load(\\\"glue\\\", \\\"mrpc\\\") metric.compute(predictions=preds, references=predictions.label_ids) \",\"{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542} \",\"我们最终得到了 accuracy 和 f1。这是用来衡量 MRPC 的 metrics。\",\"现在我们可以定义 compute_metrics() 函数了：\",\"def compute_metrics(eval_preds): metric = evaluate.load(\\\"glue\\\", \\\"mrpc\\\") logits, labels = eval_preds predictions = np.argmax(logits, axis=-1) return metric.compute(predictions=predictions, references=labels) \",\"如果想要在每个 epoch 之后输出这些 metrics，我们可以在 Trainer 中传递 compute_metrics() 函数\",\"training_args = TrainingArguments(\\\"test-trainer\\\", evaluation_strategy=\\\"epoch\\\") model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) trainer = Trainer( model, training_args, train_dataset=tokenized_datasets[\\\"train\\\"], eval_dataset=tokenized_datasets[\\\"validation\\\"], data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics, ) \",\"这次我们再执行 trainer.train() 时就会在每个 epoch 结束时输出 validation loss 和 metrics。\",\"Trainer 在多 GPU 和多 TPU 上开箱即用，且提供了很多配置项，比如通过配置 fp16=True 来启动 mixed-precision 训练。我们会在第 10 章介绍这些配置项。\"]},\"66\":{\"h\":\"3. 使用 Pytorch 训练\",\"t\":[\"在 2 中我们介绍了如何使用 Trainer 类进行微调。现在我们不使用 Trainer 来达到同样的目的。\",\"数据预处理的方式和之前介绍的一样，我们假定你已经完成了这步。\",\"数据预处理\",\"from datasets import load_dataset from transformers import AutoTokenizer, DataCollatorWithPadding raw_datasets = load_dataset(\\\"glue\\\", \\\"mrpc\\\") checkpoint = \\\"bert-base-uncased\\\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) def tokenize_function(example): return tokenizer(example[\\\"sentence1\\\"], example[\\\"sentence2\\\"], truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) \"]},\"67\":{\"h\":\"3.1 准备\",\"t\":[\"之前我们直接将 tokenized_datasets 传给 Trainer 让它自己处理，现在我们需要手动处理：\",\"tokenized_datasets 中的 sentence1, sentence2, idx 不是 model 需要的输入，需要删掉\",\"将列 label 改为 labels\",\"将 dataset 的格式设为 Pytorch tensor\",\"对应的代码：\",\"tokenized_datasets = tokenized_datasets.remove_columns([\\\"sentence1\\\", \\\"sentence2\\\", \\\"idx\\\"]) tokenized_datasets = tokenized_datasets.rename_column(\\\"label\\\", \\\"labels\\\") tokenized_datasets.set_format(\\\"torch\\\") tokenized_datasets[\\\"train\\\"].column_names \",\"['labels', 'input_ids', 'token_type_ids', 'attention_mask'] \",\"接下来在定义 training loop 之前，还要先定义几个对象：\"]},\"68\":{\"h\":\"3.1.1 数据加载器（dataloader）：用于迭代批次\",\"t\":[\"from torch.utils.data import DataLoader train_dataloader = DataLoader( tokenized_datasets[\\\"train\\\"], shuffle=True, batch_size=8, collate_fn=data_collator ) eval_dataloader = DataLoader( tokenized_datasets[\\\"validation\\\"], batch_size=8, collate_fn=data_collator ) \",\"快速检验下是否有错\",\"for batch in train_dataloader: break {k: v.shape for k, v in batch.items()} \",\"{'labels': torch.Size([8]), 'input_ids': torch.Size([8, 76]), 'token_type_ids': torch.Size([8, 76]), 'attention_mask': torch.Size([8, 76])} \",\"至此，数据预处理完成了。\"]},\"69\":{\"h\":\"3.1.2 model\",\"t\":[\"from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) \",\"快速检验下是否有错\",\"我们将上面检验 dataloader 是否出错时使用的 batch 传递给 model\",\"outputs = model(**batch) print(outputs.loss, outputs.logits.shape) \",\"tensor(0.6617, grad_fn=<NllLossBackward0>) torch.Size([8, 2]) \"]},\"70\":{\"h\":\"3.1.3 优化器（optimizer）\",\"t\":[\"我们使用 Trainer 的默认 optimizer：AdamW，它和 Adam 类似，主要差异在于他们的权重衰减正则化（weight decay regularization）不同。\",\"from transformers import AdamW optimizer = AdamW(model.parameters(), lr=5e-5) \"]},\"71\":{\"h\":\"3.1.4 学习率调度器（learning rate scheduler）\",\"t\":[\"默认的 learning rate scheduler 实现的是简单的从 5e-5 到 0 的线性衰减。为了定义学习率调度器，我们需要知道要进行多少 training steps，即 epoch 乘 training batches（training dataloader 的长度）。\",\"from transformers import get_scheduler # Trainer 默认训练 3 轮 num_epochs = 3 num_training_steps = num_epochs * len(train_dataloader) lr_scheduler = get_scheduler( \\\"linear\\\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) print(num_training_steps) # 1377 \"]},\"72\":{\"h\":\"3.2 Training Loop\",\"t\":[\"我们可以设置 device 为 gpu 以让 model在 GPU 上运行：\",\"import torch device = torch.device(\\\"cuda\\\") if torch.cuda.is_available() else torch.device(\\\"cpu\\\") model.to(device) \",\"现在可以开始训练啦！为了让我们知道训练的进度，可以使用进度条（tqdm 库）。\",\"from tqdm.auto import tqdm progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(number_epochs): for batch in train_dataloader: batch = {k: v.to(device) for k, v in batch.items()} outputs = model(**batch) loss = outputs.loss loss.backward() optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) \",\"下面我们添加一些输出，以在训练过程中查看训练效果\"]},\"73\":{\"h\":\"3.3 Evaluation Loop\",\"t\":[\"我们仍然使用 🤗 Evaluate 库提供的 metric。之前我们用过 metric.compute() 方法了。在 prediction loop 中使用 add_batch() ，metrics 会跟着 batches 累积，当我们将全部 batch 的结果累积后就可以使用 metric.compute() 得到最后的结果。\",\"import evaluate metric = evaluate.load(\\\"glue\\\", \\\"mrpc\\\") model.eval() for batch in eval_dataloader: batch = {k: v.to(device) for k, v in batch.items()} with torch.no_grad(): outputs = model(**batch) logits = outputs.logits predictions = torch.argmax(logits, dim=-1) metric.add_batch(predictions=predictions, references=batch[\\\"labels\\\"]) metric.compute() \",\"{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535} \"]},\"74\":{\"h\":\"3.4 使用 🤗 Accelerate 进行加速\",\"t\":[\"使用 🤗 Accelerate 我们可以在多个 GPU 或 TPU 上进行分布式训练。\",\"我们在之前的代码上进行简单修改即可完成：\",\"+ from accelerate import Accelerator from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler + accelerator = Accelerator() model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) optimizer = AdamW(model.parameters(), lr=3e-5) - device = torch.device(\\\"cuda\\\") if torch.cuda.is_available() else torch.device(\\\"cpu\\\") - model.to(device) + train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare( + train_dataloader, eval_dataloader, model, optimizer + ) num_epochs = 3 num_training_steps = num_epochs * len(train_dataloader) lr_scheduler = get_scheduler( \\\"linear\\\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(num_epochs): for batch in train_dataloader: - batch = {k: v.to(device) for k, v in batch.items()} outputs = model(**batch) loss = outputs.loss - loss.backward() + accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) \",\"🤗 Accelerate 会帮你处理设备的问题，所以你可以删除 device 那段代码（你也可以使用 accelerator.device 来代替 device）。\",\"提示\",\"为了充分利用集群 TPU 的加速，建议把所有的数据填充到固定的长度（配置 tokenizer 的 padding=\\\"max_length\\\"）。\",\"如果你要复制粘贴分布式训练的代码，请看这里\",\"from accelerate import Accelerator from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler accelerator = Accelerator() model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) optimizer = AdamW(model.parameters(), lr=3e-5) train_dl, eval_dl, model, optimizer = accelerator.prepare( train_dataloader, eval_dataloader, model, optimizer ) num_epochs = 3 num_training_steps = num_epochs * len(train_dl) lr_scheduler = get_scheduler( \\\"linear\\\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(num_epochs): for batch in train_dl: outputs = model(**batch) loss = outputs.loss accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) \",\"将代码存到 train.py 中，该脚本可以在任何分布式设备上运行。\",\"accelerate config \",\"回答弹出的问题，然后它会将你的答案写入配置文件中。然后你可以使用下面的命令使用该配置文件启动分布式训练。\",\"accelerate launch train.py \",\"如果你想在 Notebook 中尝试，你把代码贴到函数下面（比如 training_function() ），然后在 cell 中执行：\",\"from accelerate import notebook_launcher notebook_launcher(training_function) \",\"更多示例\",\"你可以在 🤗 Accelerate repo 中查看更多示例。\"]},\"75\":{\"h\":\"总结\",\"t\":[\"在前两章中你了解了 model 和 tokenizer，现在你学会了如何微调。回顾本章：\",\"在 Hub 中查看并下载 datasets\",\"学会了如何加载、预处理数据集，包括动态填充和 collator\",\"实现微调以及评估\",\"较底层实现 training loop\",\"使用 🤗 Accelerate 以在 GPU 集群或 TPU 集群上进行训练\"]},\"76\":{\"h\":\"4. 共享 Models 和 Tokenizers\",\"t\":[\"Hugging Face Hub 是主网站，我们可以在里面找到各种最新的模型和数据集，也可以上传自己的模型和数据集。\",\"其中的模型不局限于 🤗 Transformers 或者 NLP。你可以自己去探索。\",\"模型都用 Git 进行托管，允许版本控制和重现。另外，在 Hub 上共享模型会自动为该模型部署托管的推理 API。\"]},\"77\":{\"h\":\"1. 使用预训练模型\",\"t\":[\"如我们要使用 camembert-base checkpoints.\",\"提示\",\"使用预训练模型时，可以在 model card 中查看它是如何训练的，在哪些数据集上训练的，局限性和 bias 。\"]},\"78\":{\"h\":\"2. 共享预训练模型\",\"t\":[\"创建模型仓库的三种方法：\",\"使用 push_to_hub API\",\"使用 huggingface_hub python 库\",\"在 web 页面上创建\",\"创建好仓库后，就可以通过 git 或者 git-lfs 上传文件了。\"]},\"79\":{\"h\":\"2.1 创建模型仓库\"},\"80\":{\"h\":\"2.1.1 使用 push_to_hub API\",\"t\":[\"你需要身份令牌一遍 huggingface_hub 知道你的权限。\",\"如果你使用 Trainer API 训练模型，将模型上传至 Hub 最简单的方式就是在定义 TrainerArguments 时配置 push_to_hub=True\",\"from transformers import TrainerArguments training_args = TrainerArguments( \\\"bert-finetuned-mrpc\\\", save_strategy=\\\"epoch\\\", push_to_hub=True ) \",\"当你调用 trainer.train() 时，Trainer 会在每次保存 model 时（按照上面的配置，是每个 epoch）将你的 model 上传到 Hub 中对应的仓库上。仓库名称为你选择的输出路径（如上面的 bert-finetuned-mrpc），你也可以用 hub_model_id=\\\"a_different_name\\\" 来设置不同的名称。如果要将 model 上传到你所在的组织下，你可以使用 hub_model_id=\\\"my_organization/mu_repo_name\\\"。\",\"训练结束后，使用 trainer.push_to_hub() 上传最后一版 model。它会生成 model card。\",\"在较低层的实现中，我们可以直接通过 models、tokenizers、configuration 对象的 push_to_hub() 方法来访问 Model Hub。这种方式既可以创建仓库，又能将 model 和 tokenizer 文件直接推到仓库中。\",\"首先创建 model 和 tokenizer。\",\"from transformers import AutoModelForMaskedLM, AutoTokenizer checkpoint = \\\"camembert-base\\\" model = AutoModelForMaskedLM.from_pretrained(checkpoint) tokenizer = AutoTokenizer.from_pretrained(checkpoint) \",\"你可以训练模型、对模型进行微调、向 tokenizer 中增加 tokens。做完你想做的事情时候，你可以使用 push_to_hub() 将 model 推到仓库中\",\"model.push_to_hub(\\\"dummy-model\\\") \",\"这将会创建名为 dummy-model 的仓库，其中会填上你的 model 文件。\",\"同样，对 tokenizer 也可做同样的操作。现在你的仓库中有了全部所需的文件。\",\"tokenizer.push_to_hub(\\\"dummy-model\\\") \",\"如果你想将仓库放到组织下：\",\"tokenizer.push_to_hub(\\\"dummy-model\\\", organization=\\\"huggingface\\\") \",\"如果你想使用某个特定的 Hugging Face token：\",\"tokenizer.push_to_hub(\\\"dummy-model\\\", organization=\\\"huggingface\\\", use_auth_token=\\\"<TOKEN>\\\") \"]},\"81\":{\"h\":\"2.1.2 使用 huggingface_hub python 库\",\"t\":[\"你需要使用 CLI 的登录命令\",\"huggingface-cli login \",\"huggingface_hub 库提供了很多方法和类。下面是和仓库创建、删除等有关的方法\",\"from huggingface_hub import ( # User management login, logout, whoami, # Repository creation and management create_repo, delete_repo, update_repo_visibility, # And some methods to retrieve/change information about the content list_models, list_datasets, list_metrics, list_repo_files, upload_file, delete_file, ) \",\"# 创建仓库 from huggingface_hub import create_repo create_repo(\\\"dummy-model\\\") # 可以指定 organization # create_repo(\\\"dummy-model\\\", organization=\\\"huggingface\\\") \",\"除了可以指定 organization，还有一些参数：\",\"private: 是否对其他人可见\",\"token: 是否想用给定的 token 覆盖缓存中的 token\",\"repo_type: 是都要创建 dataset 或 space（而非创建 model）。接受的值可以是 “dataset” 或 “space”\"]},\"82\":{\"h\":\"2.1.3 使用 web 页面\",\"t\":[\"这里不展开介绍，按照页面提示进行即可。\"]},\"83\":{\"h\":\"2.2 上传 model files\",\"t\":[\"Hugging Face Hub 的文件管理系统基于 git（对于 regular files）和 git-lfs（对于大文件，large file storage）。\",\"下面我们将介绍三种上传文件到 Hub 的方法。\"]},\"84\":{\"h\":\"2.3.1 upload_file 方法\",\"t\":[\"使用 upload_file() 不需要 git 或 git-lfs，它使用 http post 请求将文件直接传到 🤗 Hub。但是它没有办法处理 5GB 以上的文件。\",\"from huggingface_hub import upload_file upload_file( \\\"<path_to_file>/config.json\\\", path_in_repo=\\\"config.json\\\", repo_id=\\\"<namespace>/dummy-model\\\", ) \",\"还有一些其他的参数：\",\"token\",\"repo_type\"]},\"85\":{\"h\":\"2.3.2 Repository 类\",\"t\":[\"Repository 类以于 git 的方式管理本地仓库。使用该类需要安装 git 和 git-lfs\",\"安装 git-lfs\",\"参考 Git Large File Storage\",\":::\",\"我们使用刚刚建好的仓库。首先我们克隆远端仓库：\",\"from huggingface_hub import Repository repo = Repository(\\\"<path_to_dummy_folder>\\\", clone_from=\\\"<namespace>/dummy-model\\\") \",\"这将在本地创建文件夹<path_to_dummy_folder>。该文件夹中包含 .gitattributes 文件。\",\"我们还会使用一些传统的 git 方法，参考文档：\",\"repo.git_pull() repo.git_add() repo.git_commit() repo.git_push() repo.git_tag() \",\"现在我们有想要推到 Hub 上的 model 和 tokenizer，并成功 clone 了仓库。\",\"首先，确保我们本地 clone 的版本是最新的：\",\"repo.git_pull() \",\"然后我们就可以保存 model 和 tokenizer files 了：\",\"model.save_pretrained(\\\"<path_to_dummy_folder>\\\") tokenizer.save_pretrained(\\\"<path_to_dummy_folder>\\\") \",\"目前，<path_to_dummy_folder> 中包含了全部的 model 和 tokenizer files。接下来可以使用传统的 git 工作流将他们推到远端 hub：\",\"repo.git_add() repo.git_commit(\\\"Add model and tokenizer files\\\") repo.git_push() \"]},\"86\":{\"h\":\"2.3.3 git-based 方法\",\"t\":[\"直接使用 git 和 git-lfs 来上传文件。请确保安装了 git 和 git-lfs。\",\"首先，初始化 git-lfs\",\"git lfs install \",\"接下来，第一步是克隆 model 仓库：\",\"git clone https://huggingface.co/<namespace>/<your-model-id> \",\"例如，我的 username 是 hanzhuo，使用的 model name 是 dummy-model\",\"git clone https://huggingface.co/hanzhuo/dummy-model \",\"现在我的工作路径中有一个 dummy-model 文件夹，\",\"cd dummy-model && ls \",\"可以使用 git 来添加小文件，对于大文件，需要使用 git-lfs。\",\"回顾一下之前获得 model 和 tokenizer 的方式：\",\"from transformers import AutoModelForMaskedLM, AutoTokenizer checkpoint = \\\"camembert-base\\\" model = AutoModelForMaskedLM.from_pretrained(checkpoint) tokenizer = AutoTokenizer.from_pretrained(checkpoint) # Do whatever with the model, train it, fine-tune it... model.save_pretrained(\\\"<path_to_dummy_folder>\\\") tokenizer.save_pretrained(\\\"<path_to_dummy_folder>\\\") \",\"我们看一下 dummy-model 下的文件目录：\",\"config.json pytorch_model.bin README.md sentencepiece.bpe.model special_tokens_map.json tokenizer_config.json tokenizer.json \",\"如果使用 ls -lh 命令，可以发现 pytorch_model.bin 的大小超过了 400MB。\",\"接下来使用常规的 git 命令：\",\"git add . git status \",\"On branch main Your branch is up to date with 'origin/main'. Changes to be committed: (use \\\"git restore --staged <file>...\\\" to unstage) modified: .gitattributes new file: config.json new file: pytorch_model.bin new file: sentencepiece.bpe.model new file: special_tokens_map.json new file: tokenizer.json new file: tokenizer_config.json \",\"再看一下 git-lfs ：\",\"git lfs status \",\"On branch main Objects to be pushed to origin/main: Objects to be committed: config.json (Git: bc20ff2) pytorch_model.bin (LFS: 35686c2) sentencepiece.bpe.model (LFS: 988bc5a) special_tokens_map.json (Git: cb23931) tokenizer.json (Git: 851ff3e) tokenizer_config.json (Git: f0f7783) Objects not staged for commit: \",\"可以观察到 pytorch_model.bin 和 sentencepiece.bpe.model 使用的 LFS，其余的都是 Git。\",\"最后，commit 并 push\",\"git commit -m \\\"First model version\\\" git push \"]},\"87\":{\"h\":\"3. 建立 model card\",\"t\":[\"建立 model card 是通过 README.md 来完成的。为了理解 model card 的重要作用，你可以阅读 Model Cards for Model Reporting。\",\"model card 通常开篇为简短的概述说明其用途，然后是以下几部分：\",\"Model description 描述\",\"Intended uses & limitations 预期用途和限制\",\"How to use 如何使用\",\"Limitations and bias 局限性和偏见\",\"Training data 训练数据\",\"Training procedure 训练过程\",\"Variable & metrics 评估指标\",\"Evaluation results 评估结果\"]},\"88\":{\"h\":\"Model card metadata\",\"t\":[\"在 Hugging Face Hub 中，有的 model 属于特定的类型，你可以通过 tasks, languages, libraries 等等来筛选。\",\"请查看 camembert-base model card，你能看到在 model card header 中有如下信息：\",\"--- language: fr license: mit datasets: - oscar --- \",\"具体配置可查看 full model card specification。\"]},\"89\":{\"h\":\"5. 🤗 Datasets 库\",\"t\":[\"在第三章中我们初步体验了 🤗 Datasets 库，了解 fine-tune 的基本步骤：\",\"从 Hugging Face Hub 上加载数据集\",\"使用 Dataset.map() 预处理数据\",\"加载并计算 metrics\",\"在本章内容中，我们将深入了解 🤗 Datasets 库，你将能够回答以下问题：\",\"数据集不在 hub 上应该怎么做\",\"如何对数据集进行切片（如果你确实需要使用 Pandas 怎么办）\",\"如果你的数据集很大，会撑爆你的 RAM 应该怎么办\",\"Memory Mapping、Apache Arrow 是什么\",\"如何创建自己的数据集并将其推至 Hub\"]},\"90\":{\"h\":\"1. 处理不在 Hugging Face Hub 上的数据集\",\"t\":[\"🤗 Datasets 提供了加载本地和远程数据集的方法，支持下列格式：\",\"Data format\",\"Loading script\",\"Example\",\"CSV & TSV\",\"csv\",\"load_dataset(\\\"csv\\\", data_files=\\\"my_file.csv\\\")\",\"Text files\",\"text\",\"load_dataset(\\\"text\\\", data_files=\\\"my_file.txt\\\")\",\"JSON & JSON Lines\",\"json\",\"load_dataset(\\\"json\\\", data_files=\\\"my_file.jsonl\\\")\",\"Pickled DataFrames\",\"pandas\",\"load_dataset(\\\"pandas\\\", data_files=\\\"my_dataframe.pkl\\\")\"]},\"91\":{\"h\":\"1.1 加载本地数据集\",\"t\":[\"我们使用 SQuAD-it dataset，它是大规模意大利语问答数据集。\",\"Ubuntu 下载并解压\",\"!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz !wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz !gzip -dkv SQuAD_it-*.json.gz \",\"from datasets import load_dataset squad_it_dataset = load_dataset(\\\"json\\\", data_files=\\\"SQuAD_it-train.json\\\", field=\\\"data\\\") squad_it_dataset \",\"DatasetDict({ train: Dataset({ features: ['paragraphs', 'title'], num_rows: 442 }) }) \",\"加载本地文件会创建一个带有 train 的 DatasetDict 对象。我们可以通过下标查看几个示例如：squad_it_dataset[\\\"train\\\"][0]。\",\"如何获得同时有 train 和 test 的 DatasetDict 对象呢？\",\"data_files = {\\\"train\\\": \\\"SQuAD_it-train.json\\\", \\\"test\\\": \\\"SQuAD_it-test.json\\\"} squad_it_dataset = load_dataset(\\\"json\\\", data_files=data_files, field=\\\"data\\\") squad_it_dataset \",\"DatasetDict({ train: Dataset({ features: ['paragraphs', 'title'], num_rows: 442 }) test: Dataset({ features: ['paragraphs', 'title'], num_rows: 48 }) }) \",\"提示\",\"data_files 参数很灵活，可以是单个文件路径、文件路径 list、映射名称路径的字典，还可以使用 Unix shell 的匹配规则选择多有满足规则的文件（如 data_files=\\\"*.json\\\" 匹配所有的 json 文件）。\",\"🤗 Datasets 的 loading script 支持自动解压，所以我们可以跳过自己解压的过程，直接使用下面的代码加载数据：\",\"data_files = {\\\"train\\\": \\\"SQuAD_it-train.json.gz\\\", \\\"test\\\": \\\"SQuAD_it-test.json.gz\\\"} squad_it_dataset = load_dataset(\\\"json\\\", data_files=data_files, field=\\\"data\\\") \"]},\"92\":{\"h\":\"1.2 加载远程数据集\",\"t\":[\"将 data_files 设置为 url 即可。\",\"url = \\\"https://github.com/crux82/squad-it/raw/master/\\\" data_files = { \\\"train\\\": url + \\\"SQuAD_it-train.json.gz\\\", \\\"test\\\": url + \\\"SQuAD_it-test.json.gz\\\", } squad_it_dataset = load_dataset(\\\"json\\\", data_files=data_files, field=\\\"data\\\") \"]},\"93\":{\"h\":\"2. 切片\"},\"94\":{\"h\":\"2.1 Slicing and dicing 数据\",\"t\":[\"和 Pandas 类似，🤗 Datasets 也提供了一些函数处理 Dataset 和 DatasetDict 对象。在第三章中我们介绍了 Dataset.map()，本章我们将介绍其他函数。\",\"接下来我们使用的数据集为 Drug Review Dataset。\",\"TSV 是 CSV 的变体，它和 CSV 的区别在于 CSV 用逗号作为分割符，而 TSV 使用制表符作为分隔符。所以我们可以使用 csv 加载的方式并指定 delimiter。\",\"from datasets import load_dataset data_files = {\\\"train\\\": \\\"drugLibTrain_raw.tsv\\\", \\\"test\\\": \\\"drugLibTest_raw.tsv\\\"} drug_dataset = load_dataset(\\\"csv\\\", data_files=data_files, delimiter=\\\"\\\\t\\\") \",\"我们可以抽取一些样本来观察，以对数据有一个直观的认识。可以使用 Dataset.shuffle() 和 Dataset.select() 来随机抽取样本。\",\"drug_sample = drug_dataset[\\\"train\\\"].shuffle().select(range(1000)) # 选取前面几个样本 drug_sample[:3] \",\"运行结果\",\"{'Unnamed: 0': [1468, 3422, 1444], 'urlDrugName': ['cymbalta', 'tazorac', 'tirosint'], 'rating': [9, 5, 4], 'effectiveness': ['Highly Effective', 'Moderately Effective', 'Moderately Effective'], 'sideEffects': ['No Side Effects', 'Severe Side Effects', 'Moderate Side Effects'], 'condition': ['sever depression', 'acne', 'thyroid/total thyroidectomy due to cancer'], 'benefitsReview': [\\\"This medication saved my life. The depression had gotten so sever that I was unable to function properly. It has made me feel like a 'real' person again. It has not done much for the anxiety, panic, or OCD. The Xanax helps with that area. I will be going to the psychiatrist in 2 weeks for the anxiety, panic, and OCD. Hoping to stay on the Cymbalta. I was on Lexapro 30mg from 2000-2006. Then switched to Celexa (cost reasons) 60mg from 2007-2008. The Celexa just about costed me my life. It was ineffective for the Depression. Try not to take Celexa for cost reasons, Lexapro shows much more promise in its effectiveness.\\\", 'It exfoliated my skin.', 'I started taking Tirosint 125mcg 6 weeks ago due to a gluten and caseine allergy. I previously was taking synthroid however, the company couldnt verify the inactive ingredients. So to avoid gluten, caseine, and some really nasty anxiety symptms I switched to the Tirosint. \\\\r\\\\r\\\\n\\\\r\\\\r\\\\nThe anxiety symptoms subsided from 1 attack per day to none. This was great!, but then new symptoms started after three weeks.'], 'sideEffectsReview': ['Weight gain, which is to be expected when you \\\"feel better\\\"', 'My skin became extremely dry, irritated, red, and would peel.', 'I felt dperessed, tired and very sore. My finger joints hurt so bad and to the touch. My back, and legs ached. Then my lower legs, ankles and feet started to swell. it is so bad that I cannot walk upon waking in the morning. If Im on my feet for more than an hour I have to elevate my legs due to the pain and swelling. I also gained alot of weight 8lbs. I\\\\'m a healthy 37 year old woman, I am active with two small boys, and eat an extremley healthy diet due to my Celiacs and Caseine allergies. \\\\r\\\\r\\\\n\\\\r\\\\r\\\\nI recently had my blood levels checked and to my surprise I was taking way too much Tirosint. My Endo said that the swelling and pain wasnt from the Tirosint and that I should see my Primary doctor...REALLY?? I was shocked due to my \\\"hypo\\\" symptoms. Im thinking of returning to Synthroid and switching doctors...'], 'commentsReview': ['Depression and Anxiety', 'Use once daily, at night. Wash face, use toner and leave to dry (10 minutes). Then apply pea size amount of cream all over face, excluding eye area. Let soak in (15 minutes), then layer with moisturizer. Must wear sunscreen daily.', '125 mcg Tirosint per day and doubled on Sat and Sun. \\\\r\\\\r\\\\n\\\\r\\\\r\\\\nListen to your body and be persistent with your doctors.']} \",\"Dataset.select() 需要传入一个可迭代的索引，这里我们传入了 range(1000) 从随机打乱的数据集中选取前 1000 个样本。我们可以看出数据集的一些特点了：\",\"Unnamed: 0: 可能是患者的 ID\",\"condition: 描述健康状况的标签，有大写有小写\",\"各类 review: 长短不一，混有 Python 行分隔符（\\\\r\\\\n）、html 字符编码（见 🤗 官方示例，如&#039;）\",\"下面我们验证一下 Unnamed 0 是患者 ID 的猜想，这里会用到 Dataset.unique()：\",\"for split in drug_dataset.keys(): assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\\\"Unnamed: 0\\\")) \",\"上面的代码没抛出 AssertionError，看来是患者 ID 的这个想法是正确的。我们将该列重命名为 patient_id，这里会用到 DatasetDict.rename_column():\",\"drug_dataset = drug_dataset.rename_column( original_column_name=\\\"Unnamed: 0\\\", new_column_name=\\\"patient_id\\\" ) \",\"我们使用 Dataset.map() 标准化所有的 condition：\",\"def lowercase_condition(example): return {\\\"condition\\\": example[\\\"condition\\\"].lower()} drug_dataset.map(lowercase_condition) \",\"AttributeError: 'NoneType' object has no attribute 'lower' \",\"看来我们还需要把 condition 为 None 的数据过滤掉（使用 Dataset.filter()）：\",\"drug_dataset = drug_dataset.filter(lambda x: x[\\\"condition\\\"] is not None) \"]},\"95\":{\"h\":\"2.2 创建新列\",\"t\":[\"在处理 review 字段时，最好是要统计一下字数。我们就简单基于空格来进行词数统计。\",\"def compute_review_length(example): return {\\\"benefit_review_length\\\": len(example[\\\"benefitsReview\\\"].split())} drug_dataset = drug_dataset.map(compute_review_length) \",\"这样，我们就增加了 benefit_review_length 列。我们还可以使用 Dataset.sort() 以该列为基准做排序\",\"drug_datasets[\\\"train\\\"].sort(\\\"benefit_review_length\\\") \",\"`Dataset.add_column()`\",\"我们还可以使用 Dataset.add_column() 增加列。可以传入 Python list 或 numpy。\",\"review 的词数较少时（极端情况比如只有一个词）对于预测没有提供相对有用的信息。下面我将使用 Dataset.filter() 将少于 30 个词的 review 去掉。\",\"drug_dataset = drug_dataset.filter(lambda x: x[\\\"benefit_review_length\\\"] > 30) print(drug_dataset.num_rows) \",\"{'train': 1445, 'test': 473} \",\"review 中还有一些 html 编码，可以使用 Python 的 html module 来解码：\",\"import html text = \\\"I&#039;m a transformer called BERT\\\" html.unescape(text) \",\"\\\"I'm a transformer called BERT\\\" \",\"drug_dataset = drug_dataset.map(lambda x: { \\\"benefitsReview\\\": html.unescape(x[\\\"benefitsReview\\\"])}) \"]},\"96\":{\"h\":\"2.3 map() 方法\",\"t\":[\"Dataset.map() 有一个参数是 batched。将其设为 True 后，map 函数将一次处理多个数据（成为一批 a batch），batch size 可以配置，默认情况下为 1000.\"]},\"97\":{\"h\":\"Ai\"},\"98\":{\"h\":\"Section1\"},\"99\":{\"h\":\"Section2\"}},\"dirtCount\":0,\"index\":[[\"成为一批\",{\"1\":{\"96\":1}}],[\"后\",{\"1\":{\"96\":1}}],[\"后处理\",{\"0\":{\"32\":1},\"1\":{\"27\":1,\"32\":1}}],[\"去掉\",{\"1\":{\"95\":1}}],[\"去掉了\",{\"1\":{\"62\":1}}],[\"极端情况比如只有一个词\",{\"1\":{\"95\":1}}],[\"增加列\",{\"1\":{\"95\":1}}],[\"列\",{\"1\":{\"95\":1}}],[\"标准化所有的\",{\"1\":{\"94\":1}}],[\"标量\",{\"1\":{\"4\":1,\"5\":2}}],[\"字符编码\",{\"1\":{\"94\":1}}],[\"字段时\",{\"1\":{\"95\":1}}],[\"字段包含\",{\"1\":{\"65\":1}}],[\"字段\",{\"1\":{\"62\":1}}],[\"行分隔符\",{\"1\":{\"94\":1}}],[\"混有\",{\"1\":{\"94\":1}}],[\"各类\",{\"1\":{\"94\":1}}],[\"运行结果\",{\"1\":{\"94\":1}}],[\"运行上面代码后\",{\"1\":{\"64\":1}}],[\"选取前面几个样本\",{\"1\":{\"94\":1}}],[\"选择任务\",{\"1\":{\"16\":1}}],[\"切片\",{\"0\":{\"93\":1}}],[\"支持自动解压\",{\"1\":{\"91\":1}}],[\"支持下列格式\",{\"1\":{\"90\":1}}],[\"匹配所有的\",{\"1\":{\"91\":1}}],[\"映射名称路径的字典\",{\"1\":{\"91\":1}}],[\"映射到一个数字上\",{\"1\":{\"28\":1}}],[\"怎么办\",{\"1\":{\"89\":1}}],[\"具体配置可查看\",{\"1\":{\"88\":1}}],[\"属于特定的类型\",{\"1\":{\"88\":1}}],[\"属性来查看对应的\",{\"1\":{\"32\":1}}],[\"描述健康状况的标签\",{\"1\":{\"94\":1}}],[\"描述\",{\"1\":{\"87\":1}}],[\"建立\",{\"0\":{\"87\":1},\"1\":{\"87\":1}}],[\"建议把所有的数据填充到固定的长度\",{\"1\":{\"74\":1}}],[\"再看一下\",{\"1\":{\"86\":1}}],[\"再观察结果\",{\"1\":{\"52\":1}}],[\"命令\",{\"1\":{\"86\":2}}],[\"我的\",{\"1\":{\"86\":1}}],[\"我们就增加了\",{\"1\":{\"95\":1}}],[\"我们就简单基于空格来进行词数统计\",{\"1\":{\"95\":1}}],[\"我们就可以在自己的数据集上微调模型了\",{\"1\":{\"64\":1}}],[\"我们看一下\",{\"1\":{\"86\":1}}],[\"我们仍然使用\",{\"1\":{\"73\":1}}],[\"我们假定你已经完成了这步\",{\"1\":{\"66\":1}}],[\"我们最终得到了\",{\"1\":{\"65\":1}}],[\"我们最终会得到一个非常大的词汇表\",{\"1\":{\"40\":1}}],[\"我们来看一下如何构建\",{\"1\":{\"65\":1}}],[\"我们没有为\",{\"1\":{\"64\":1}}],[\"我们没有配置\",{\"1\":{\"64\":1}}],[\"我们快速回顾一下上一部分的预处理\",{\"1\":{\"63\":1}}],[\"我们已经将原数数据转化成模型可处理的\",{\"1\":{\"62\":1}}],[\"我们已经过如何使用\",{\"1\":{\"37\":1}}],[\"我们取了\",{\"1\":{\"62\":1}}],[\"我们在之前的代码上进行简单修改即可完成\",{\"1\":{\"74\":1}}],[\"我们在上一篇文章中介绍了\",{\"1\":{\"61\":1}}],[\"我们在使用\",{\"1\":{\"44\":1}}],[\"我们得到了一个\",{\"1\":{\"60\":1}}],[\"我们无法将这个\",{\"1\":{\"51\":1}}],[\"我们对起初的代码进行修改\",{\"1\":{\"50\":1}}],[\"我们对小长度的序列进行了处理\",{\"1\":{\"49\":1}}],[\"我们需要知道要进行多少\",{\"1\":{\"71\":1}}],[\"我们需要将文本转化成数字表示\",{\"1\":{\"61\":1}}],[\"我们需要思考以下问题\",{\"1\":{\"49\":1}}],[\"我们需要一个有序列分类\",{\"1\":{\"31\":1}}],[\"我们希望\",{\"1\":{\"40\":1}}],[\"我们不妨加载已经训练好的模型\",{\"1\":{\"35\":1}}],[\"我们不希望模型关注它\",{\"1\":{\"24\":1}}],[\"我们详细介绍下\",{\"1\":{\"33\":1}}],[\"我们通过三个步骤\",{\"1\":{\"32\":1}}],[\"我们通常可以选择\",{\"1\":{\"7\":1}}],[\"我们预测的是\",{\"1\":{\"32\":1}}],[\"我们分析的是两个句子\",{\"1\":{\"31\":1}}],[\"我们将该列重命名为\",{\"1\":{\"94\":1}}],[\"我们将深入了解\",{\"1\":{\"89\":1}}],[\"我们将上面检验\",{\"1\":{\"69\":1}}],[\"我们将上面使用\",{\"1\":{\"30\":1}}],[\"我们将开始微调\",{\"1\":{\"64\":1}}],[\"我们将其进行\",{\"1\":{\"55\":1}}],[\"我们将\",{\"1\":{\"52\":1,\"61\":1}}],[\"我们将填充\",{\"1\":{\"51\":1}}],[\"我们将在\",{\"1\":{\"43\":1,\"64\":1}}],[\"我们将在未来在讨论其他参数\",{\"1\":{\"37\":1}}],[\"我们将使用\",{\"1\":{\"26\":1,\"59\":1}}],[\"我们使用刚刚建好的仓库\",{\"1\":{\"85\":1}}],[\"我们使用\",{\"1\":{\"31\":1,\"51\":1,\"61\":1,\"70\":1,\"91\":1,\"94\":1}}],[\"我们使用了\",{\"1\":{\"28\":1,\"56\":1}}],[\"我们使用人工标注的数据以有监督的方式进行精调\",{\"1\":{\"19\":1}}],[\"我们可以看出数据集的一些特点了\",{\"1\":{\"94\":1}}],[\"我们可以看到模型需要的输入形式是\",{\"1\":{\"61\":1}}],[\"我们可以抽取一些样本来观察\",{\"1\":{\"94\":1}}],[\"我们可以通过下标查看几个示例如\",{\"1\":{\"91\":1}}],[\"我们可以通过查看\",{\"1\":{\"32\":1}}],[\"我们可以直接通过\",{\"1\":{\"80\":1}}],[\"我们可以设置\",{\"1\":{\"72\":1}}],[\"我们可以在里面找到各种最新的模型和数据集\",{\"1\":{\"76\":1}}],[\"我们可以在多个\",{\"1\":{\"74\":1}}],[\"我们可以在\",{\"1\":{\"65\":1}}],[\"我们可以传递通过\",{\"1\":{\"64\":1}}],[\"我们可以验证一下\",{\"1\":{\"62\":1}}],[\"我们可以为\",{\"1\":{\"61\":1}}],[\"我们可以使用\",{\"1\":{\"28\":1,\"54\":1,\"65\":1}}],[\"我们可以说\",{\"1\":{\"14\":1}}],[\"我们还可以使用\",{\"1\":{\"95\":2}}],[\"我们还可以使用注意力遮罩层\",{\"1\":{\"24\":1}}],[\"我们还会使用一些传统的\",{\"1\":{\"85\":1}}],[\"我们还会展示如何批处理多个句子\",{\"1\":{\"26\":1}}],[\"我们还需要自定义一个\",{\"1\":{\"40\":1}}],[\"我们简单介绍了\",{\"1\":{\"25\":1}}],[\"我们正在预测第4个单词\",{\"1\":{\"24\":1}}],[\"我们会在第\",{\"1\":{\"65\":1}}],[\"我们会喂给\",{\"1\":{\"24\":1}}],[\"我们会把前三个单词也作为输入\",{\"1\":{\"24\":1}}],[\"我们宁可把垃圾邮件标记为正常邮件\",{\"1\":{\"11\":1}}],[\"我们宁可把健康人误诊为癌症\",{\"1\":{\"11\":1}}],[\"仓库\",{\"1\":{\"86\":1}}],[\"仓库名称为你选择的输出路径\",{\"1\":{\"80\":1}}],[\"直接使用下面的代码加载数据\",{\"1\":{\"91\":1}}],[\"直接使用\",{\"1\":{\"86\":1}}],[\"工作流将他们推到远端\",{\"1\":{\"85\":1}}],[\"目前\",{\"1\":{\"85\":1}}],[\"目前支持的\",{\"1\":{\"16\":1}}],[\"确保我们本地\",{\"1\":{\"85\":1}}],[\"确定是在输入的左侧还是右侧进行\",{\"1\":{\"62\":1}}],[\"确定句子在语法上是否正确或两个句子在逻辑上是否相关\",{\"1\":{\"13\":1}}],[\"安装\",{\"1\":{\"85\":1}}],[\"<file>\",{\"1\":{\"86\":1}}],[\"<your\",{\"1\":{\"86\":1}}],[\"<namespace>\",{\"1\":{\"84\":1,\"85\":1,\"86\":1}}],[\"<path\",{\"1\":{\"84\":1,\"85\":4,\"86\":2}}],[\"<token>\",{\"1\":{\"80\":1}}],[\"页面\",{\"0\":{\"82\":1}}],[\"页面上创建\",{\"1\":{\"78\":1}}],[\"接受的值可以是\",{\"1\":{\"81\":1}}],[\"接下来我们使用的数据集为\",{\"1\":{\"94\":1}}],[\"接下来我们将介绍如何在自己的数据集上进行微调\",{\"1\":{\"58\":1}}],[\"接下来使用常规的\",{\"1\":{\"86\":1}}],[\"接下来\",{\"1\":{\"86\":1}}],[\"接下来可以使用传统的\",{\"1\":{\"85\":1}}],[\"接下来在定义\",{\"1\":{\"67\":1}}],[\"接下来还有\",{\"1\":{\"26\":1}}],[\"覆盖缓存中的\",{\"1\":{\"81\":1}}],[\"除了可以指定\",{\"1\":{\"81\":1}}],[\"除此之外\",{\"1\":{\"40\":1}}],[\"删除等有关的方法\",{\"1\":{\"81\":1}}],[\"同样\",{\"1\":{\"80\":1}}],[\"同样地\",{\"1\":{\"44\":1}}],[\"文件路径\",{\"1\":{\"91\":1}}],[\"文件夹\",{\"1\":{\"86\":1}}],[\"文件\",{\"1\":{\"80\":1,\"85\":1,\"91\":1}}],[\"文件直接推到仓库中\",{\"1\":{\"80\":1}}],[\"推到仓库中\",{\"1\":{\"80\":1}}],[\"推荐阅读\",{\"1\":{\"25\":1}}],[\"推荐\",{\"1\":{\"22\":1}}],[\"做完你想做的事情时候\",{\"1\":{\"80\":1}}],[\"向\",{\"1\":{\"80\":1}}],[\"又能将\",{\"1\":{\"80\":1}}],[\"又提出了第三种方式\",{\"1\":{\"41\":1}}],[\"知道你的权限\",{\"1\":{\"80\":1}}],[\"允许版本控制和重现\",{\"1\":{\"76\":1}}],[\"共享预训练模型\",{\"0\":{\"78\":1}}],[\"共享\",{\"0\":{\"76\":1}}],[\"集群上进行训练\",{\"1\":{\"75\":1}}],[\"集群或\",{\"1\":{\"75\":1}}],[\"较底层实现\",{\"1\":{\"75\":1}}],[\"学会了如何加载\",{\"1\":{\"75\":1}}],[\"学习率调度器\",{\"0\":{\"71\":1}}],[\"回顾一下之前获得\",{\"1\":{\"86\":1}}],[\"回顾本章\",{\"1\":{\"75\":1}}],[\"回答弹出的问题\",{\"1\":{\"74\":1}}],[\"更多示例\",{\"1\":{\"74\":1}}],[\"更好的做法是当我们构建一个\",{\"1\":{\"61\":1}}],[\"请查看\",{\"1\":{\"88\":1}}],[\"请确保安装了\",{\"1\":{\"86\":1}}],[\"请求将文件直接传到\",{\"1\":{\"84\":1}}],[\"请看这里\",{\"1\":{\"74\":1}}],[\"请直接从\",{\"1\":{\"59\":1}}],[\"累积\",{\"1\":{\"73\":1}}],[\"轮\",{\"1\":{\"71\":1}}],[\"乘\",{\"1\":{\"71\":1}}],[\"默认情况下为\",{\"1\":{\"96\":1}}],[\"默认训练\",{\"1\":{\"71\":1}}],[\"默认的\",{\"1\":{\"71\":1}}],[\"默认是一个函数\",{\"1\":{\"62\":1}}],[\"优化器\",{\"0\":{\"70\":1}}],[\"快速检验下是否有错\",{\"1\":{\"68\":1,\"69\":1}}],[\"快速体验\",{\"0\":{\"16\":1}}],[\"改为\",{\"1\":{\"67\":1}}],[\"让它自己处理\",{\"1\":{\"67\":1}}],[\"让它在训练时进行评估\",{\"1\":{\"64\":1}}],[\"准备\",{\"0\":{\"67\":1}}],[\"准确性\",{\"0\":{\"8\":1}}],[\"章介绍这些配置项\",{\"1\":{\"65\":1}}],[\"构建\",{\"1\":{\"65\":1}}],[\"构建的\",{\"1\":{\"41\":1}}],[\"平均时长\",{\"1\":{\"65\":1}}],[\"评估结果\",{\"1\":{\"87\":1}}],[\"评估指标\",{\"1\":{\"87\":1}}],[\"评估\",{\"0\":{\"65\":1}}],[\"评估时会直接输出\",{\"1\":{\"64\":1}}],[\"想要进行评估可以设置\",{\"1\":{\"64\":1}}],[\"调用\",{\"1\":{\"64\":1}}],[\"定义的是一样的\",{\"1\":{\"64\":1}}],[\"定义模型\",{\"1\":{\"64\":1}}],[\"被替换成了做\",{\"1\":{\"64\":1}}],[\"被称为状态字典\",{\"1\":{\"36\":1}}],[\"`dataset\",{\"1\":{\"95\":1}}],[\"`\",{\"1\":{\"64\":1,\"95\":1}}],[\"`pip\",{\"1\":{\"64\":2}}],[\"`pytorch`\",{\"1\":{\"64\":1}}],[\"`accelerate>=0\",{\"1\":{\"64\":1}}],[\"`trainer`\",{\"1\":{\"64\":1}}],[\"官方示例时\",{\"1\":{\"64\":1}}],[\"官方示例\",{\"1\":{\"64\":1,\"94\":1}}],[\"其余的都是\",{\"1\":{\"86\":1}}],[\"其余的参数都可以设置为默认值\",{\"1\":{\"64\":1}}],[\"其中会填上你的\",{\"1\":{\"80\":1}}],[\"其中的模型不局限于\",{\"1\":{\"76\":1}}],[\"其中\",{\"1\":{\"37\":1}}],[\"其中记录了模型的权重\",{\"1\":{\"36\":1}}],[\"必须提供的唯一参数是训练模型的存储路径\",{\"1\":{\"64\":1}}],[\"配置\",{\"1\":{\"74\":1}}],[\"配置运行环境\",{\"1\":{\"63\":1}}],[\"配置中包含很多建立模型要用到的属性\",{\"1\":{\"34\":1}}],[\"现在我的工作路径中有一个\",{\"1\":{\"86\":1}}],[\"现在我们有想要推到\",{\"1\":{\"85\":1}}],[\"现在我们需要手动处理\",{\"1\":{\"67\":1}}],[\"现在我们不使用\",{\"1\":{\"66\":1}}],[\"现在我们可以定义\",{\"1\":{\"64\":1,\"65\":1}}],[\"现在你的仓库中有了全部所需的文件\",{\"1\":{\"80\":1}}],[\"现在你学会了如何微调\",{\"1\":{\"75\":1}}],[\"现在可以开始训练啦\",{\"1\":{\"72\":1}}],[\"现在可以将\",{\"1\":{\"37\":1}}],[\"现在\",{\"1\":{\"62\":1}}],[\"之前\",{\"1\":{\"67\":1}}],[\"之前我们用过\",{\"1\":{\"73\":1}}],[\"之前我们直接将\",{\"1\":{\"67\":1}}],[\"之前我们要先定义\",{\"1\":{\"64\":1}}],[\"之后输出这些\",{\"1\":{\"65\":1}}],[\"之后进行评估\",{\"1\":{\"64\":1}}],[\"之后将会被填充到\",{\"1\":{\"62\":1}}],[\"之间\",{\"1\":{\"53\":1}}],[\"则这个\",{\"1\":{\"62\":1}}],[\"则会使用默认\",{\"1\":{\"16\":1}}],[\"8lbs\",{\"1\":{\"94\":1}}],[\"851ff3e\",{\"1\":{\"86\":1}}],[\"8578431372549019\",{\"1\":{\"65\":1}}],[\"8907849829351535\",{\"1\":{\"73\":1}}],[\"8996539792387542\",{\"1\":{\"65\":1}}],[\"8431372549019608\",{\"1\":{\"73\":1}}],[\"8\",{\"1\":{\"62\":6,\"68\":4,\"69\":1}}],[\"8789\",{\"1\":{\"50\":1}}],[\"动态填充\",{\"0\":{\"62\":1}}],[\"动词\",{\"1\":{\"13\":1}}],[\"新的字段即预处理函数返回的字典中的每个键\",{\"1\":{\"61\":1}}],[\"加速了预处理\",{\"1\":{\"61\":1}}],[\"加载的方式并指定\",{\"1\":{\"94\":1}}],[\"加载远程数据集\",{\"0\":{\"92\":1}}],[\"加载本地文件会创建一个带有\",{\"1\":{\"91\":1}}],[\"加载本地数据集\",{\"0\":{\"91\":1}}],[\"加载并计算\",{\"1\":{\"89\":1}}],[\"加载和保存\",{\"1\":{\"43\":1}}],[\"加载\",{\"0\":{\"43\":1},\"1\":{\"43\":1}}],[\"设置为\",{\"1\":{\"92\":1}}],[\"设置\",{\"1\":{\"61\":1}}],[\"设备成本\",{\"1\":{\"21\":1}}],[\"下载并解压\",{\"1\":{\"91\":1}}],[\"下的文件目录\",{\"1\":{\"86\":1}}],[\"下次执行时不需要重复计算\",{\"1\":{\"61\":1}}],[\"下面我将使用\",{\"1\":{\"95\":1}}],[\"下面我们验证一下\",{\"1\":{\"94\":1}}],[\"下面我们将介绍三种上传文件到\",{\"1\":{\"83\":1}}],[\"下面我们添加一些输出\",{\"1\":{\"72\":1}}],[\"下面我们要进行微调了\",{\"1\":{\"62\":1}}],[\"下面我们先介绍下\",{\"1\":{\"43\":1}}],[\"下面我们来使用模型进行预测\",{\"1\":{\"37\":1}}],[\"下面是和仓库创建\",{\"1\":{\"81\":1}}],[\"下面的分步调用只是为了让大家更清楚\",{\"1\":{\"44\":1}}],[\"下面分别介绍这两步\",{\"1\":{\"44\":1}}],[\"下面介绍\",{\"1\":{\"40\":1}}],[\"下面介绍几种\",{\"1\":{\"38\":1}}],[\"下面将使用\",{\"1\":{\"33\":1}}],[\"下面列举了一部分\",{\"1\":{\"29\":1}}],[\"且提供了很多配置项\",{\"1\":{\"65\":1}}],[\"且结果会被缓存\",{\"1\":{\"61\":1}}],[\"且你在这个过程中丢失了信息\",{\"1\":{\"40\":1}}],[\"空间不足将会有问题\",{\"1\":{\"61\":1}}],[\"过程中需要在\",{\"1\":{\"61\":1}}],[\"过程可以使用\",{\"1\":{\"45\":1}}],[\"哪一个是第二个句子\",{\"1\":{\"61\":1}}],[\"哪怕牺牲一部分recall\",{\"1\":{\"11\":1}}],[\"哪怕牺牲一部分precision\",{\"1\":{\"11\":1}}],[\"数据\",{\"0\":{\"94\":1}}],[\"数据加载器\",{\"0\":{\"68\":1}}],[\"数据预处理完成了\",{\"1\":{\"68\":1}}],[\"数据预处理\",{\"1\":{\"66\":1}}],[\"数据预处理的方式和之前介绍的一样\",{\"1\":{\"66\":1}}],[\"数据集不在\",{\"1\":{\"89\":1}}],[\"数据集预处理\",{\"0\":{\"61\":1}}],[\"数据集有关的\",{\"1\":{\"65\":1}}],[\"数据集有\",{\"1\":{\"59\":1}}],[\"数据集\",{\"1\":{\"59\":1}}],[\"数决定\",{\"1\":{\"40\":1}}],[\"开始阅读\",{\"1\":{\"59\":1}}],[\"注\",{\"1\":{\"59\":1}}],[\"注意力层使得模型对不同位置的字词有着不同的关注程度\",{\"1\":{\"23\":1}}],[\"注意力层\",{\"0\":{\"23\":1}}],[\"注意\",{\"1\":{\"17\":1,\"24\":1,\"29\":1,\"44\":1,\"47\":1,\"64\":1}}],[\"处理不在\",{\"0\":{\"90\":1}}],[\"处理数据\",{\"0\":{\"59\":1}}],[\"处理多个序列\",{\"0\":{\"49\":1}}],[\"尝试了多种可配置的\",{\"1\":{\"57\":1}}],[\"总计时长\",{\"1\":{\"65\":1}}],[\"总结\",{\"0\":{\"57\":1,\"75\":1}}],[\"总结文本\",{\"1\":{\"13\":1}}],[\"总是知道应该是怎样的\",{\"1\":{\"55\":1}}],[\"添加了两个特殊词\",{\"1\":{\"55\":1}}],[\"添加可能对模型有用的其他输入\",{\"1\":{\"28\":1}}],[\"特殊\",{\"0\":{\"55\":1}}],[\"特征\",{\"1\":{\"22\":1}}],[\"截断序列\",{\"1\":{\"53\":1}}],[\"就能处理比较长的序列\",{\"1\":{\"53\":1}}],[\"就可以通过\",{\"1\":{\"78\":1}}],[\"就可以\",{\"1\":{\"44\":1}}],[\"像\",{\"1\":{\"53\":1}}],[\"换用支持更长序列的模型\",{\"1\":{\"53\":1}}],[\"有一个参数是\",{\"1\":{\"96\":1}}],[\"有大写有小写\",{\"1\":{\"94\":1}}],[\"有的\",{\"1\":{\"88\":1}}],[\"有三个字段\",{\"1\":{\"65\":1}}],[\"有些模型只在句尾加特殊词\",{\"1\":{\"55\":1}}],[\"有些模型只在句首加特殊词\",{\"1\":{\"55\":1}}],[\"有些模型不用特殊词\",{\"1\":{\"55\":1}}],[\"有两种方式来解决这个问题\",{\"1\":{\"53\":1}}],[\"有明显的性别相关性\",{\"1\":{\"17\":1}}],[\"至此\",{\"1\":{\"68\":1}}],[\"至\",{\"1\":{\"53\":1}}],[\"长短不一\",{\"1\":{\"94\":1}}],[\"长序列\",{\"0\":{\"53\":1}}],[\"长度非\",{\"1\":{\"5\":1}}],[\"长度为\",{\"1\":{\"5\":1}}],[\"应该怎么办\",{\"1\":{\"89\":1}}],[\"应该忽略它\",{\"1\":{\"52\":1}}],[\"应该着重考虑提高precision指标\",{\"1\":{\"11\":1}}],[\"应该着重考虑提高recall指标\",{\"1\":{\"11\":1}}],[\"元素只有\",{\"1\":{\"52\":1}}],[\"忽略\",{\"1\":{\"51\":1}}],[\"需要传入一个可迭代的索引\",{\"1\":{\"94\":1}}],[\"需要使用\",{\"1\":{\"86\":1}}],[\"需要删掉\",{\"1\":{\"67\":1}}],[\"需要的输入\",{\"1\":{\"67\":1}}],[\"需要用到\",{\"1\":{\"65\":1}}],[\"需要告诉\",{\"1\":{\"51\":1}}],[\"需要一个词表\",{\"1\":{\"44\":1}}],[\"都加载到内存中\",{\"1\":{\"61\":1}}],[\"都作为上下文考虑进去了\",{\"1\":{\"51\":1}}],[\"都做了什么\",{\"0\":{\"27\":1}}],[\"造成不一致是因为\",{\"1\":{\"51\":1}}],[\"观察上面的结果\",{\"1\":{\"51\":1}}],[\"观察结果\",{\"1\":{\"17\":1}}],[\"输入\",{\"1\":{\"51\":1}}],[\"输出\",{\"0\":{\"30\":1},\"1\":{\"52\":1}}],[\"输出的部分一起作为\",{\"1\":{\"24\":1}}],[\"填充\",{\"0\":{\"51\":1}}],[\"填充的内容无意义\",{\"1\":{\"24\":1}}],[\"批处理\",{\"0\":{\"50\":1}}],[\"词汇表索引是唯一能够使模型正常工作的输入吗\",{\"1\":{\"49\":1}}],[\"词表将有效地减小\",{\"1\":{\"41\":1}}],[\"通过\",{\"1\":{\"50\":1}}],[\"通过本章的学习\",{\"1\":{\"48\":1}}],[\"通常开篇为简短的概述说明其用途\",{\"1\":{\"87\":1}}],[\"通常有一层或多层线性层组成\",{\"1\":{\"31\":1}}],[\"通常很大\",{\"1\":{\"30\":1}}],[\"组合在一起了\",{\"1\":{\"47\":1}}],[\"组织\",{\"1\":{\"13\":1}}],[\"方不仅将索引转化为了\",{\"1\":{\"47\":1}}],[\"方法了\",{\"1\":{\"73\":1}}],[\"方法进行预测\",{\"1\":{\"65\":1}}],[\"方法来访问\",{\"1\":{\"80\":1}}],[\"方法来构建数据集\",{\"1\":{\"61\":1}}],[\"方法来实现\",{\"1\":{\"47\":1}}],[\"方法来保存模型\",{\"1\":{\"36\":1}}],[\"方法实现\",{\"1\":{\"45\":1}}],[\"方法\",{\"0\":{\"84\":1,\"86\":1,\"96\":1},\"1\":{\"29\":1,\"35\":1,\"57\":1,\"64\":1,\"65\":1,\"85\":1}}],[\"解码\",{\"0\":{\"47\":1},\"1\":{\"47\":1}}],[\"解码器使用编码器的表示\",{\"1\":{\"22\":1}}],[\"解码器\",{\"1\":{\"22\":1}}],[\"转换为\",{\"0\":{\"46\":1}}],[\"转化为字符串\",{\"1\":{\"48\":1}}],[\"转化为数字表示\",{\"1\":{\"44\":1}}],[\"转化成\",{\"1\":{\"37\":1,\"48\":1,\"51\":1}}],[\"编码\",{\"0\":{\"44\":1},\"1\":{\"95\":1}}],[\"编码器接收输入并构建其表示\",{\"1\":{\"22\":1}}],[\"编码器\",{\"1\":{\"22\":1}}],[\"然后是以下几部分\",{\"1\":{\"87\":1}}],[\"然后我们就可以保存\",{\"1\":{\"85\":1}}],[\"然后我们就可以使用\",{\"1\":{\"43\":1}}],[\"然后在\",{\"1\":{\"74\":1}}],[\"然后你可以使用下面的命令使用该配置文件启动分布式训练\",{\"1\":{\"74\":1}}],[\"然后它会将你的答案写入配置文件中\",{\"1\":{\"74\":1}}],[\"然后\",{\"1\":{\"64\":1}}],[\"然后转化为\",{\"1\":{\"44\":1}}],[\"然后会介绍\",{\"1\":{\"26\":1}}],[\"比较少用的词可以分解为有意义的子词\",{\"1\":{\"42\":1}}],[\"比如通过配置\",{\"1\":{\"65\":1}}],[\"比如中文字符会比拉丁系语言的字符携带更多信息\",{\"1\":{\"41\":1}}],[\"比如\",{\"1\":{\"13\":1,\"23\":1,\"24\":2,\"36\":1,\"42\":1,\"74\":1}}],[\"考虑到上面两种技术\",{\"1\":{\"41\":1}}],[\"要处理\",{\"1\":{\"41\":1}}],[\"要进行微调\",{\"1\":{\"21\":1}}],[\"但我们只传递来一个序列\",{\"1\":{\"50\":1}}],[\"但也因语言而异\",{\"1\":{\"41\":1}}],[\"但是它没有办法处理\",{\"1\":{\"84\":1}}],[\"但是它不会告诉你这个模型表现得怎么样\",{\"1\":{\"64\":1}}],[\"但是也有一些不足\",{\"1\":{\"61\":1}}],[\"但是我们需要传入句子对\",{\"1\":{\"61\":1}}],[\"但是模型看到了目标中的第4个单词\",{\"1\":{\"24\":1}}],[\"但是不允许它使用没有预测的词汇\",{\"1\":{\"24\":1}}],[\"但是对于具体问题\",{\"1\":{\"19\":1}}],[\"但是会给患者带来麻烦\",{\"1\":{\"11\":1}}],[\"相对于\",{\"1\":{\"41\":1}}],[\"相关的前五名答案之一\",{\"1\":{\"17\":1}}],[\"首位各添加了一个\",{\"1\":{\"55\":1}}],[\"首先我们克隆远端仓库\",{\"1\":{\"85\":1}}],[\"首先创建\",{\"1\":{\"80\":1}}],[\"首先创建函数对输入进行\",{\"1\":{\"61\":1}}],[\"首先更新\",{\"1\":{\"64\":1}}],[\"首先\",{\"1\":{\"41\":1,\"85\":1,\"86\":1}}],[\"首页\",{\"0\":{\"0\":1}}],[\"因为\",{\"1\":{\"64\":1}}],[\"因为将所有的数据\",{\"1\":{\"61\":1}}],[\"因为每个字词都是通过\",{\"1\":{\"41\":1}}],[\"因此它能够获得完整的输入句子来对当前词语进行最佳预测\",{\"1\":{\"24\":1}}],[\"产生了大量\",{\"1\":{\"40\":1}}],[\"表示输入的哪部分是第一个句子\",{\"1\":{\"61\":1}}],[\"表示该位置是\",{\"1\":{\"52\":1}}],[\"表示\",{\"1\":{\"40\":1}}],[\"一般用\",{\"1\":{\"40\":1}}],[\"一些标记\",{\"1\":{\"5\":1}}],[\"另外\",{\"1\":{\"40\":1,\"41\":1,\"76\":1}}],[\"没有提供该方法的话\",{\"1\":{\"64\":1}}],[\"没有对句子对进行过预训练\",{\"1\":{\"64\":1}}],[\"没有介绍\",{\"1\":{\"61\":1}}],[\"没有体现出它们之间的相似与联系\",{\"1\":{\"40\":1}}],[\"没有指定\",{\"1\":{\"16\":1}}],[\"个词的\",{\"1\":{\"95\":1}}],[\"个样本\",{\"1\":{\"94\":1}}],[\"个作为一个\",{\"1\":{\"62\":1}}],[\"个句子对\",{\"1\":{\"59\":1}}],[\"个不同文本分类任务中的性能\",{\"1\":{\"59\":1}}],[\"个\",{\"1\":{\"40\":1,\"41\":1,\"53\":1}}],[\"个单词\",{\"1\":{\"40\":1}}],[\"英语中有\",{\"1\":{\"40\":1}}],[\"那段代码\",{\"1\":{\"74\":1}}],[\"那么该字段还会包括\",{\"1\":{\"65\":1}}],[\"那么\",{\"1\":{\"51\":1}}],[\"那么它在之前已经被缓存了\",{\"1\":{\"29\":1}}],[\"那将会生成大量\",{\"1\":{\"40\":1}}],[\"从随机打乱的数据集中选取前\",{\"1\":{\"94\":1}}],[\"从\",{\"0\":{\"60\":1},\"1\":{\"40\":1,\"89\":1}}],[\"从文本中提取答案\",{\"1\":{\"13\":1}}],[\"此表的大小由语料中的独立\",{\"1\":{\"40\":1}}],[\"算法\",{\"0\":{\"39\":1},\"1\":{\"38\":1}}],[\"了仓库\",{\"1\":{\"85\":1}}],[\"了解\",{\"1\":{\"57\":1,\"89\":1}}],[\"了解如何加载模型\",{\"1\":{\"26\":1}}],[\"了\",{\"1\":{\"37\":1,\"64\":1,\"85\":1}}],[\"传给\",{\"1\":{\"67\":1}}],[\"传递给\",{\"1\":{\"37\":1,\"69\":1}}],[\"传入多个句子\",{\"1\":{\"16\":1}}],[\"传入一个句子\",{\"1\":{\"16\":1}}],[\"只需要\",{\"1\":{\"41\":1}}],[\"只必传的\",{\"1\":{\"37\":1}}],[\"只接收\",{\"1\":{\"37\":1}}],[\"只能看到已经翻译好的字词\",{\"1\":{\"24\":1}}],[\"76\",{\"1\":{\"68\":3}}],[\"768\",{\"1\":{\"30\":1,\"34\":1}}],[\"7276\",{\"1\":{\"50\":1}}],[\"7993\",{\"1\":{\"43\":1,\"46\":1,\"47\":1}}],[\"7592\",{\"1\":{\"37\":1}}],[\"经过\",{\"1\":{\"62\":1}}],[\"经常使用的词不应该再被切分为更小的子词\",{\"1\":{\"42\":1}}],[\"经\",{\"1\":{\"37\":1}}],[\"给定文本\",{\"1\":{\"37\":1}}],[\"给定问题和上下文\",{\"1\":{\"13\":1}}],[\"提供评估的方法\",{\"1\":{\"64\":1}}],[\"提供了加载本地和远程数据集的方法\",{\"1\":{\"90\":1}}],[\"提供了\",{\"1\":{\"62\":1,\"63\":1}}],[\"提供了模型权重\",{\"1\":{\"36\":1}}],[\"提供了模型的架构信息\",{\"1\":{\"36\":1}}],[\"提供句子对列表\",{\"1\":{\"61\":1}}],[\"提示\",{\"1\":{\"35\":1,\"60\":1,\"64\":1,\"74\":1,\"77\":1,\"91\":1}}],[\"版本错误解决方案\",{\"1\":{\"64\":1}}],[\"版本\",{\"1\":{\"36\":1}}],[\"该文件夹中包含\",{\"1\":{\"85\":1}}],[\"该文件中也有一些\",{\"1\":{\"36\":1}}],[\"该脚本可以在任何分布式设备上运行\",{\"1\":{\"74\":1}}],[\"该\",{\"1\":{\"59\":1,\"61\":1}}],[\"该过程通过\",{\"1\":{\"46\":1}}],[\"该句子中的全部字词\",{\"1\":{\"24\":1}}],[\"保存\",{\"0\":{\"43\":1},\"1\":{\"43\":1}}],[\"保存方法\",{\"0\":{\"36\":1}}],[\"保存所有的\",{\"1\":{\"26\":1}}],[\"环境变量来自定义缓存路径\",{\"1\":{\"35\":1,\"60\":1}}],[\"~\",{\"1\":{\"35\":1,\"60\":1}}],[\"缓存路径为\",{\"1\":{\"35\":1,\"60\":1}}],[\"替换为\",{\"1\":{\"35\":1}}],[\"不需要\",{\"1\":{\"84\":1}}],[\"不同\",{\"1\":{\"70\":1}}],[\"不同的模型会使用不同的特殊词\",{\"1\":{\"55\":1}}],[\"不同的加载方法\",{\"0\":{\"35\":1}}],[\"不是\",{\"1\":{\"67\":1}}],[\"不论如何\",{\"1\":{\"55\":1}}],[\"不过使用效果肯定很差\",{\"1\":{\"35\":1}}],[\"打印\",{\"1\":{\"34\":1}}],[\"初始化\",{\"1\":{\"34\":1,\"86\":1}}],[\"得到最后的结果\",{\"1\":{\"73\":1}}],[\"得到最终的结论\",{\"1\":{\"32\":1}}],[\"得到的输入传递给\",{\"1\":{\"30\":1}}],[\"到\",{\"1\":{\"71\":1}}],[\"到最大长度效率不高\",{\"1\":{\"61\":1}}],[\"到整个词表大小\",{\"1\":{\"40\":1}}],[\"到此为止\",{\"1\":{\"32\":1}}],[\"到底有多少数据被成功预测为positive\",{\"1\":{\"10\":1}}],[\"层中\",{\"1\":{\"32\":1}}],[\"所有的\",{\"1\":{\"32\":1}}],[\"所以你可以删除\",{\"1\":{\"74\":1}}],[\"所以我们可以使用\",{\"1\":{\"94\":1}}],[\"所以我们可以跳过自己解压的过程\",{\"1\":{\"91\":1}}],[\"所以我们可以不传递\",{\"1\":{\"64\":1}}],[\"所以我们可以用它们构建张量并把张量提供给模型\",{\"1\":{\"44\":1}}],[\"所以我们在使用该模型做推理的时候也应该在首尾加上它们\",{\"1\":{\"55\":1}}],[\"所以我们需要用我们所选的模型的名称来实例化\",{\"1\":{\"44\":1}}],[\"所以输出维度是\",{\"1\":{\"31\":1}}],[\"所以\",{\"1\":{\"11\":1,\"61\":1}}],[\"判断每个句子是\",{\"1\":{\"31\":1}}],[\"序列\",{\"1\":{\"54\":1}}],[\"序列的数字表示的长度\",{\"1\":{\"30\":1}}],[\"序列到序列\",{\"1\":{\"18\":1}}],[\"每eval\",{\"1\":{\"64\":1}}],[\"每\",{\"1\":{\"64\":1}}],[\"每一个集合中包含这样几列\",{\"1\":{\"60\":1}}],[\"每一层的定义\",{\"1\":{\"14\":1}}],[\"每个句子对有一个标签来指明两个句子是否同义\",{\"1\":{\"59\":1}}],[\"每个字词都被分配了一个\",{\"1\":{\"40\":1}}],[\"每个模型输入的张量维度\",{\"1\":{\"30\":1}}],[\"每次处理的序列长度\",{\"1\":{\"30\":1}}],[\"高维张量\",{\"0\":{\"30\":1}}],[\"还可以使用\",{\"1\":{\"91\":1}}],[\"还要先定义几个对象\",{\"1\":{\"67\":1}}],[\"还将同一个词中的\",{\"1\":{\"47\":1}}],[\"还会加载或保存对应的词表\",{\"1\":{\"43\":1}}],[\"还是\",{\"1\":{\"31\":1}}],[\"还提供了不同的架构\",{\"1\":{\"29\":1}}],[\"还有一些其他的参数\",{\"1\":{\"84\":1}}],[\"还有一些参数\",{\"1\":{\"81\":1}}],[\"还有很多\",{\"1\":{\"59\":1}}],[\"还有\",{\"1\":{\"14\":1}}],[\"+\",{\"1\":{\"29\":1,\"74\":6,\"92\":2}}],[\"架构\",{\"1\":{\"29\":1}}],[\"架构于\",{\"1\":{\"18\":1}}],[\"部分\",{\"1\":{\"29\":1}}],[\"模块输出的张量通常很大\",{\"1\":{\"30\":1}}],[\"模块\",{\"1\":{\"29\":1}}],[\"模型都用\",{\"1\":{\"76\":1}}],[\"模型都是语言模型\",{\"1\":{\"19\":1}}],[\"模型在\",{\"1\":{\"59\":1}}],[\"模型能处理的序列长度是有限的\",{\"1\":{\"53\":1}}],[\"模型默认接收多个句子作为输入\",{\"1\":{\"50\":1}}],[\"模型需要处理大量\",{\"1\":{\"41\":1}}],[\"模型需要关注\",{\"1\":{\"23\":1}}],[\"模型使用这些\",{\"1\":{\"40\":1}}],[\"模型只能处理数字\",{\"1\":{\"38\":1}}],[\"模型只接收\",{\"1\":{\"28\":1}}],[\"模型的更多细节\",{\"1\":{\"35\":1}}],[\"模型的第一步是加载配置对象\",{\"1\":{\"34\":1}}],[\"模型输出的都是\",{\"1\":{\"32\":1}}],[\"模型不能直接处理原始文本\",{\"1\":{\"28\":1}}],[\"模型一般都很大\",{\"1\":{\"26\":1}}],[\"模型\",{\"1\":{\"18\":3,\"22\":3,\"26\":1}}],[\"模型分为三类\",{\"1\":{\"18\":1}}],[\"模型中发生的每个操作的定义\",{\"1\":{\"14\":1}}],[\"模型框架\",{\"1\":{\"14\":1}}],[\"与\",{\"1\":{\"29\":1}}],[\"为什么会出错呢\",{\"1\":{\"50\":1}}],[\"为此我们要查看\",{\"1\":{\"43\":1}}],[\"为\",{\"1\":{\"28\":1,\"61\":1,\"64\":1,\"72\":1,\"94\":1}}],[\"为了理解\",{\"1\":{\"87\":1}}],[\"为了充分利用集群\",{\"1\":{\"74\":1}}],[\"为了让我们知道训练的进度\",{\"1\":{\"72\":1}}],[\"为了定义学习率调度器\",{\"1\":{\"71\":1}}],[\"为了获得预测结果\",{\"1\":{\"65\":1}}],[\"为了处理长序列\",{\"1\":{\"56\":1}}],[\"为了批处理\",{\"1\":{\"56\":1}}],[\"为了解决这个问题\",{\"1\":{\"51\":1}}],[\"为了实现这一步骤\",{\"1\":{\"44\":1}}],[\"为了加快训练\",{\"1\":{\"24\":1}}],[\"为了在大规模数据上进行预训练\",{\"1\":{\"17\":1}}],[\"创建新列\",{\"0\":{\"95\":1}}],[\"创建仓库\",{\"1\":{\"81\":1}}],[\"创建模型仓库\",{\"0\":{\"79\":1}}],[\"创建模型仓库的三种方法\",{\"1\":{\"78\":1}}],[\"创建好仓库后\",{\"1\":{\"78\":1}}],[\"创建\",{\"0\":{\"34\":1},\"1\":{\"28\":1}}],[\"创建分类器对象\",{\"1\":{\"16\":1}}],[\"上应该怎么做\",{\"1\":{\"89\":1}}],[\"上加载数据集\",{\"1\":{\"89\":1}}],[\"上的数据集\",{\"0\":{\"90\":1}}],[\"上的\",{\"1\":{\"85\":1}}],[\"上传\",{\"0\":{\"83\":1}}],[\"上传最后一版\",{\"1\":{\"80\":1}}],[\"上传到你所在的组织下\",{\"1\":{\"80\":1}}],[\"上传到\",{\"1\":{\"80\":1}}],[\"上传文件了\",{\"1\":{\"78\":1}}],[\"上共享模型会自动为该模型部署托管的推理\",{\"1\":{\"76\":1}}],[\"上进行分布式训练\",{\"1\":{\"74\":1}}],[\"上进行正确的\",{\"1\":{\"62\":1}}],[\"上开箱即用\",{\"1\":{\"65\":1}}],[\"上运行\",{\"1\":{\"64\":1,\"72\":1}}],[\"上\",{\"1\":{\"64\":1}}],[\"上一篇文章中介绍了如何使用\",{\"1\":{\"58\":1}}],[\"上一节中提到的\",{\"1\":{\"27\":1}}],[\"上述例子中为16\",{\"1\":{\"30\":1}}],[\"上述例子中为2\",{\"1\":{\"30\":1}}],[\"上面的代码没抛出\",{\"1\":{\"94\":1}}],[\"上面的代码确实将文本转化成了数字表示\",{\"1\":{\"61\":1}}],[\"上面的代码下载了\",{\"1\":{\"29\":1}}],[\"上面模型的默认\",{\"1\":{\"28\":1}}],[\"并\",{\"1\":{\"86\":1}}],[\"并成功\",{\"1\":{\"85\":1}}],[\"并不直观\",{\"1\":{\"64\":1}}],[\"并将它们拼接起来\",{\"1\":{\"62\":1}}],[\"并会为你处理好它\",{\"1\":{\"55\":1}}],[\"并在\",{\"1\":{\"43\":1}}],[\"并尽可能小\",{\"1\":{\"38\":1}}],[\"并实例化模型\",{\"1\":{\"33\":1}}],[\"并实例化了对应的模型\",{\"1\":{\"29\":1}}],[\"并缓存下来\",{\"1\":{\"28\":1}}],[\"并负责将数字表示转化成文本\",{\"1\":{\"26\":1}}],[\"名称\",{\"1\":{\"28\":1}}],[\"名词\",{\"1\":{\"13\":1}}],[\"显然\",{\"1\":{\"28\":1}}],[\"显然这样的模型在实际中不会获得好的效果\",{\"1\":{\"24\":1}}],[\"作为参数传递\",{\"1\":{\"64\":1}}],[\"作为参数传递给\",{\"1\":{\"52\":1}}],[\"作为输入\",{\"1\":{\"28\":1,\"31\":1}}],[\"作者使用动图清晰地描述了\",{\"1\":{\"22\":1}}],[\"实现微调以及评估\",{\"1\":{\"75\":1}}],[\"实现的是简单的从\",{\"1\":{\"71\":1}}],[\"实现\",{\"1\":{\"46\":1}}],[\"实现这一步有不同的规则\",{\"1\":{\"44\":1}}],[\"实现更好性能的一般策略是增加模型的大小以及预训练的数据量\",{\"1\":{\"20\":1}}],[\"实际上我们更希望得到类似于概率的结果\",{\"1\":{\"32\":1}}],[\"实际上\",{\"1\":{\"28\":1}}],[\"预期用途和限制\",{\"1\":{\"87\":1}}],[\"预测用了多长时间\",{\"1\":{\"65\":1}}],[\"预处理数据\",{\"1\":{\"89\":1}}],[\"预处理数据集\",{\"1\":{\"75\":1}}],[\"预处理\",{\"1\":{\"27\":1}}],[\"预训练这类模型可以使用\",{\"1\":{\"22\":1}}],[\"预训练\",{\"1\":{\"21\":1}}],[\"负责将文本转成数字表示\",{\"1\":{\"26\":1}}],[\"类以于\",{\"1\":{\"85\":1}}],[\"类以及它的\",{\"1\":{\"28\":1}}],[\"类进行微调\",{\"1\":{\"66\":1}}],[\"类来微调各种预训练模型\",{\"1\":{\"63\":1}}],[\"类可以根据\",{\"1\":{\"33\":1}}],[\"类似于\",{\"1\":{\"43\":2}}],[\"类似\",{\"1\":{\"29\":1,\"70\":1,\"94\":1}}],[\"类\",{\"0\":{\"85\":1},\"1\":{\"26\":1,\"31\":1,\"33\":1,\"64\":1}}],[\"类和\",{\"1\":{\"26\":1}}],[\"看来我们还需要把\",{\"1\":{\"94\":1}}],[\"看来是患者\",{\"1\":{\"94\":1}}],[\"看看它的输出\",{\"1\":{\"30\":1}}],[\"看一看\",{\"1\":{\"26\":1}}],[\"看到\",{\"1\":{\"24\":1}}],[\"来解码\",{\"1\":{\"95\":1}}],[\"来随机抽取样本\",{\"1\":{\"94\":1}}],[\"来完成的\",{\"1\":{\"87\":1}}],[\"来添加小文件\",{\"1\":{\"86\":1}}],[\"来上传文件\",{\"1\":{\"86\":1}}],[\"来设置不同的名称\",{\"1\":{\"80\":1}}],[\"来代替\",{\"1\":{\"74\":1}}],[\"来达到同样的目的\",{\"1\":{\"66\":1}}],[\"来启动\",{\"1\":{\"65\":1}}],[\"来查看\",{\"1\":{\"60\":1}}],[\"来进行推理\",{\"1\":{\"57\":1}}],[\"来进行\",{\"1\":{\"56\":1}}],[\"来\",{\"1\":{\"54\":1}}],[\"来填充\",{\"1\":{\"51\":1}}],[\"来说的一个\",{\"1\":{\"41\":1}}],[\"来说\",{\"1\":{\"41\":1,\"51\":1}}],[\"来表示不在词表中的字词\",{\"1\":{\"40\":1}}],[\"来表示每个字词\",{\"1\":{\"40\":1}}],[\"来覆盖某门语言\",{\"1\":{\"40\":1}}],[\"来确定模型结构\",{\"1\":{\"33\":1}}],[\"来实现\",{\"1\":{\"51\":1}}],[\"来实现在上一节中\",{\"1\":{\"26\":1}}],[\"来实例化\",{\"1\":{\"44\":1}}],[\"来实例化任何模型\",{\"1\":{\"33\":1}}],[\"来获得正确的翻译\",{\"1\":{\"23\":1}}],[\"训练过程\",{\"1\":{\"87\":1}}],[\"训练数据\",{\"1\":{\"87\":1}}],[\"训练结束后\",{\"1\":{\"80\":1}}],[\"训练模型\",{\"1\":{\"80\":1}}],[\"训练和评估时所用的全部超参\",{\"1\":{\"64\":1}}],[\"训练\",{\"0\":{\"64\":1,\"66\":1},\"1\":{\"26\":1,\"65\":1}}],[\"训练或者部署是一项复杂的任务\",{\"1\":{\"26\":1}}],[\"训练的初始权重\",{\"1\":{\"14\":1}}],[\"进行托管\",{\"1\":{\"76\":1}}],[\"进行加速\",{\"0\":{\"74\":1}}],[\"进行评估\",{\"1\":{\"64\":1}}],[\"进行微调\",{\"0\":{\"63\":1}}],[\"进行微调可以有效降低时间\",{\"1\":{\"21\":1}}],[\"进行\",{\"1\":{\"61\":1}}],[\"进行截断\",{\"1\":{\"53\":1}}],[\"进行推理\",{\"0\":{\"37\":1}}],[\"进行预处理\",{\"0\":{\"28\":1},\"1\":{\"32\":1}}],[\"进行使用\",{\"1\":{\"25\":1}}],[\"按照页面提示进行即可\",{\"1\":{\"82\":1}}],[\"按照上面的配置\",{\"1\":{\"80\":1}}],[\"按照\",{\"1\":{\"25\":1}}],[\"任务\",{\"1\":{\"25\":1}}],[\"任务以及如何使用\",{\"1\":{\"25\":1}}],[\"任务有很多\",{\"1\":{\"13\":1}}],[\"小模型可能是768\",{\"1\":{\"30\":1}}],[\"小结\",{\"0\":{\"25\":1,\"48\":1,\"56\":1}}],[\"小写\",{\"1\":{\"5\":2}}],[\"小写加粗\",{\"1\":{\"5\":1}}],[\"5gb\",{\"1\":{\"84\":1}}],[\"5e\",{\"1\":{\"71\":1}}],[\"59\",{\"1\":{\"62\":2}}],[\"5980e\",{\"1\":{\"32\":1}}],[\"50\",{\"1\":{\"62\":2}}],[\"500\",{\"1\":{\"40\":2,\"64\":1}}],[\"5801\",{\"1\":{\"59\":1}}],[\"5803\",{\"1\":{\"51\":1,\"52\":1}}],[\"5694\",{\"1\":{\"51\":2,\"52\":1}}],[\"5607\",{\"1\":{\"32\":2}}],[\"512\",{\"1\":{\"34\":1,\"52\":1,\"53\":2}}],[\"5223\",{\"1\":{\"28\":1}}],[\"5\",{\"0\":{\"25\":1,\"48\":1,\"54\":1,\"55\":1,\"56\":1,\"89\":1},\"1\":{\"32\":1,\"70\":1,\"71\":1,\"74\":2,\"94\":1}}],[\"会撑爆你的\",{\"1\":{\"89\":1}}],[\"会在每次保存\",{\"1\":{\"80\":1}}],[\"会在之后介绍\",{\"1\":{\"28\":1}}],[\"会帮你处理设备的问题\",{\"1\":{\"74\":1}}],[\"会帮助模型获得正确的翻译\",{\"1\":{\"23\":1}}],[\"会跟着\",{\"1\":{\"73\":1}}],[\"会输出一次\",{\"1\":{\"64\":1}}],[\"会将尽量少的字词标记为\",{\"1\":{\"40\":1}}],[\"会被分别构建不同的\",{\"1\":{\"40\":1}}],[\"会被随机初始化\",{\"1\":{\"35\":1}}],[\"会使用填充的方式使句子长度保持一致\",{\"1\":{\"24\":1}}],[\"第二步\",{\"1\":{\"64\":1}}],[\"第二步是将\",{\"1\":{\"44\":1}}],[\"第二个句子\",{\"1\":{\"32\":1}}],[\"第二个注意力层\",{\"1\":{\"24\":1}}],[\"第一步是克隆\",{\"1\":{\"86\":1}}],[\"第一步是将文本切分为\",{\"1\":{\"44\":1}}],[\"第一步\",{\"1\":{\"64\":1}}],[\"第一个句子\",{\"1\":{\"32\":1}}],[\"第一个注意力层关注所有\",{\"1\":{\"24\":1}}],[\"完整的目标\",{\"1\":{\"24\":1}}],[\"连同\",{\"1\":{\"24\":1}}],[\"已经生成了3个单词\",{\"1\":{\"24\":1}}],[\"分别使用方法\",{\"1\":{\"43\":1}}],[\"分别接收两种语言的同一个句子\",{\"1\":{\"24\":1}}],[\"分数\",{\"0\":{\"11\":1}}],[\"最好是要统计一下字数\",{\"1\":{\"95\":1}}],[\"最简单的方式就是在定义\",{\"1\":{\"80\":1}}],[\"最难的步骤大概是为\",{\"1\":{\"63\":1}}],[\"最后\",{\"1\":{\"86\":1}}],[\"最后一项任务就是在每个\",{\"1\":{\"61\":1}}],[\"最后让我们看一看如何使用\",{\"1\":{\"56\":1}}],[\"最初的\",{\"1\":{\"24\":1}}],[\"最开始是为了翻译任务而设计的\",{\"1\":{\"24\":1}}],[\"最小二乘法\",{\"1\":{\"4\":1}}],[\"原始模型\",{\"0\":{\"24\":1}}],[\"原始模型中很容易掺杂性别歧视\",{\"1\":{\"17\":1}}],[\"关注\",{\"1\":{\"23\":1}}],[\"翻译\",{\"1\":{\"23\":1}}],[\"翻译成中文\",{\"1\":{\"23\":1}}],[\"将其设为\",{\"1\":{\"96\":1}}],[\"将少于\",{\"1\":{\"95\":1}}],[\"将你的\",{\"1\":{\"80\":1}}],[\"将模型上传至\",{\"1\":{\"80\":1}}],[\"将代码存到\",{\"1\":{\"74\":1}}],[\"将列\",{\"1\":{\"67\":1}}],[\"将我们之前构造的对象\",{\"1\":{\"64\":1}}],[\"将每个\",{\"1\":{\"51\":1}}],[\"将每一个\",{\"1\":{\"28\":1}}],[\"将词表索引转化成字符串\",{\"1\":{\"47\":1}}],[\"将上面的\",{\"1\":{\"37\":1}}],[\"将输入传递给\",{\"1\":{\"32\":1}}],[\"将输入传递给模型\",{\"1\":{\"27\":1}}],[\"将这些高维张量映射到不同的维度上\",{\"1\":{\"31\":1}}],[\"将文本转换成数字的过程叫做编码\",{\"1\":{\"44\":1}}],[\"将文本转化为\",{\"1\":{\"57\":1}}],[\"将文本转化为张量\",{\"1\":{\"37\":1}}],[\"将文本转化成\",{\"1\":{\"43\":1}}],[\"将文本传递给\",{\"1\":{\"28\":1}}],[\"将文本切分成\",{\"1\":{\"28\":1,\"41\":1}}],[\"将文本翻译成另一种语言\",{\"1\":{\"13\":1}}],[\"将\",{\"0\":{\"46\":1},\"1\":{\"23\":1,\"48\":2,\"67\":1,\"80\":1,\"92\":1}}],[\"或者想先运行数据处理的整体代码\",{\"1\":{\"59\":1}}],[\"或者\",{\"1\":{\"28\":1,\"76\":1,\"78\":1}}],[\"或\",{\"0\":{\"30\":1},\"1\":{\"22\":1,\"40\":1,\"64\":1,\"74\":1,\"81\":2,\"84\":1,\"95\":1}}],[\"或命名实体\",{\"1\":{\"13\":1}}],[\"适用于需要根据输入进行生成的任务\",{\"1\":{\"22\":1}}],[\"适用于需要理解输入的任务\",{\"1\":{\"22\":1}}],[\"适用于生成任务\",{\"1\":{\"22\":1}}],[\"以该列为基准做排序\",{\"1\":{\"95\":1}}],[\"以对数据有一个直观的认识\",{\"1\":{\"94\":1}}],[\"以上的文件\",{\"1\":{\"84\":1}}],[\"以在\",{\"1\":{\"75\":1}}],[\"以在训练过程中查看训练效果\",{\"1\":{\"72\":1}}],[\"以让\",{\"1\":{\"72\":1}}],[\"以获取\",{\"1\":{\"62\":1}}],[\"以启动多进程\",{\"1\":{\"61\":1}}],[\"以及\",{\"1\":{\"64\":1}}],[\"以及行数\",{\"1\":{\"60\":1}}],[\"以及其他输入来生成目标序列\",{\"1\":{\"22\":1}}],[\"以确保使用和预训练时相同的规则\",{\"1\":{\"44\":1}}],[\"以下\",{\"1\":{\"42\":1}}],[\"以\",{\"1\":{\"31\":1}}],[\"以作为神经网络的输入\",{\"1\":{\"26\":1}}],[\"以使得模型关注某些表示\",{\"1\":{\"24\":1}}],[\"以从输入中获得理解\",{\"1\":{\"22\":1}}],[\"以衡量其准确性方面的性能\",{\"1\":{\"11\":1}}],[\"即可\",{\"1\":{\"92\":1}}],[\"即数据数量\",{\"1\":{\"60\":1}}],[\"即每个序列的表示应该是一样长的\",{\"1\":{\"51\":1}}],[\"即我们在上一节中谈到的\",{\"1\":{\"29\":1}}],[\"即下图中\",{\"1\":{\"29\":1}}],[\"即\",{\"1\":{\"28\":1,\"61\":1,\"71\":1}}],[\"即在正在被翻译的字词之前已经生成的部分\",{\"1\":{\"24\":1}}],[\"即特征\",{\"1\":{\"22\":1}}],[\"即他们已经以自监督学习\",{\"1\":{\"19\":1}}],[\"主要差异在于他们的权重衰减正则化\",{\"1\":{\"70\":1}}],[\"主要由两部分组成\",{\"1\":{\"22\":1}}],[\"主要围绕以下四部分展开\",{\"1\":{\"1\":1}}],[\"扩展阅读\",{\"1\":{\"22\":1}}],[\"结束时输出\",{\"1\":{\"65\":1}}],[\"结构相同\",{\"1\":{\"52\":1}}],[\"结构如下\",{\"1\":{\"24\":1}}],[\"结构\",{\"0\":{\"22\":1}}],[\"结果\",{\"1\":{\"16\":2}}],[\"微调模型\",{\"1\":{\"58\":1}}],[\"微调预训练模型\",{\"0\":{\"58\":1}}],[\"微调\",{\"1\":{\"21\":1}}],[\"花费长达数周的时间\",{\"1\":{\"21\":1}}],[\"指从头开始训练模型\",{\"1\":{\"21\":1}}],[\"迁移学习\",{\"0\":{\"21\":1}}],[\"等等来筛选\",{\"1\":{\"88\":1}}],[\"等\",{\"1\":{\"19\":1}}],[\"等指标\",{\"1\":{\"7\":1}}],[\"包括动态填充和\",{\"1\":{\"75\":1}}],[\"包括\",{\"1\":{\"19\":1}}],[\"自编码\",{\"1\":{\"18\":1}}],[\"自回归\",{\"1\":{\"18\":1}}],[\"大都在\",{\"1\":{\"53\":1}}],[\"大体上可以将\",{\"1\":{\"18\":1}}],[\"大写\",{\"1\":{\"5\":1}}],[\"大写粗体\",{\"1\":{\"5\":1}}],[\"月推出\",{\"1\":{\"18\":1}}],[\"60mg\",{\"1\":{\"94\":1}}],[\"6617\",{\"1\":{\"69\":1}}],[\"62\",{\"1\":{\"62\":1}}],[\"6251\",{\"1\":{\"61\":1}}],[\"67\",{\"1\":{\"62\":6}}],[\"6123\",{\"1\":{\"32\":2}}],[\"6\",{\"1\":{\"18\":1,\"94\":1}}],[\"年\",{\"1\":{\"18\":1}}],[\"背景知识\",{\"0\":{\"18\":1}}],[\"种族歧视等问题\",{\"1\":{\"17\":1}}],[\"女性工作\",{\"1\":{\"17\":1}}],[\"妓女成为了\",{\"1\":{\"17\":1}}],[\"研究员们会收集尽可能多的数据\",{\"1\":{\"17\":1}}],[\"偏见\",{\"0\":{\"17\":1}}],[\"局限性和偏见\",{\"1\":{\"87\":1}}],[\"局限性和\",{\"1\":{\"77\":1}}],[\"局限性\",{\"0\":{\"17\":1}}],[\"参数很灵活\",{\"1\":{\"91\":1}}],[\"参数给\",{\"1\":{\"61\":1}}],[\"参数去掉了\",{\"1\":{\"61\":1}}],[\"参数设置\",{\"1\":{\"50\":1}}],[\"参数传递对应的模型名称\",{\"1\":{\"16\":1}}],[\"参考文档\",{\"1\":{\"85\":1}}],[\"参考\",{\"1\":{\"11\":1,\"85\":1}}],[\"中还有一些\",{\"1\":{\"95\":1}}],[\"中有如下信息\",{\"1\":{\"88\":1}}],[\"中包含了全部的\",{\"1\":{\"85\":1}}],[\"中增加\",{\"1\":{\"80\":1}}],[\"中对应的仓库上\",{\"1\":{\"80\":1}}],[\"中执行\",{\"1\":{\"74\":1}}],[\"中尝试\",{\"1\":{\"74\":1}}],[\"中使用\",{\"1\":{\"73\":1}}],[\"中我们介绍了如何使用\",{\"1\":{\"66\":1}}],[\"中详细介绍\",{\"1\":{\"64\":1}}],[\"中传递\",{\"1\":{\"64\":2,\"65\":1}}],[\"中前\",{\"1\":{\"62\":1}}],[\"中保存整个数据集\",{\"1\":{\"61\":1}}],[\"中加载数据集\",{\"0\":{\"60\":1}}],[\"中不仅有\",{\"1\":{\"59\":1}}],[\"中准备大型数据集\",{\"1\":{\"58\":1}}],[\"中已有对应示例\",{\"1\":{\"54\":1}}],[\"中获取\",{\"1\":{\"51\":1}}],[\"中讨论\",{\"1\":{\"43\":1}}],[\"中来讨论\",{\"1\":{\"43\":1}}],[\"中你已经看到了如何加载以及保存使用模型\",{\"1\":{\"37\":1}}],[\"中看到建立模型所需的属性\",{\"1\":{\"36\":1}}],[\"中查看它是如何训练的\",{\"1\":{\"77\":1}}],[\"中查看并下载\",{\"1\":{\"75\":1}}],[\"中查看更多示例\",{\"1\":{\"74\":1}}],[\"中查看\",{\"1\":{\"35\":1}}],[\"中的最大长度\",{\"1\":{\"61\":1}}],[\"中的数据\",{\"1\":{\"61\":1}}],[\"中的数据集\",{\"1\":{\"60\":1}}],[\"中的说明或者使用页面上的\",{\"1\":{\"25\":1}}],[\"中的\",{\"1\":{\"25\":1,\"51\":1,\"67\":1}}],[\"中\",{\"1\":{\"24\":1,\"37\":1,\"74\":1,\"88\":1}}],[\"中心\",{\"1\":{\"16\":1}}],[\"中上传你自己的模型\",{\"1\":{\"15\":1}}],[\"见\",{\"1\":{\"16\":1,\"44\":1,\"94\":1}}],[\"988bc5a\",{\"1\":{\"86\":1}}],[\"9598\",{\"1\":{\"32\":1}}],[\"9598048329353333\",{\"1\":{\"16\":2}}],[\"9946e\",{\"1\":{\"32\":1}}],[\"9995\",{\"1\":{\"32\":1}}],[\"999\",{\"1\":{\"28\":1,\"37\":2,\"52\":4,\"53\":2}}],[\"9994558691978455\",{\"1\":{\"16\":1}}],[\"9\",{\"1\":{\"32\":2,\"94\":1}}],[\"039\",{\"1\":{\"94\":1,\"95\":1}}],[\"000\",{\"1\":{\"40\":2}}],[\"0005\",{\"1\":{\"32\":1}}],[\"0402\",{\"1\":{\"32\":1}}],[\"04\",{\"1\":{\"32\":1}}],[\"01\",{\"1\":{\"32\":2}}],[\"0195e\",{\"1\":{\"32\":1}}],[\"02\",{\"1\":{\"32\":1,\"34\":1}}],[\"0\",{\"1\":{\"16\":3,\"28\":16,\"32\":5,\"34\":4,\"40\":1,\"43\":9,\"50\":1,\"51\":2,\"52\":2033,\"60\":3,\"61\":16,\"65\":2,\"69\":1,\"71\":1,\"73\":2,\"91\":1,\"94\":5}}],[\"wget\",{\"1\":{\"91\":2}}],[\"when\",{\"1\":{\"94\":1}}],[\"which\",{\"1\":{\"94\":1}}],[\"whatever\",{\"1\":{\"86\":1}}],[\"whoami\",{\"1\":{\"81\":1}}],[\"whom\",{\"1\":{\"60\":1}}],[\"whole\",{\"1\":{\"16\":2,\"28\":2,\"50\":3,\"52\":1,\"53\":1,\"55\":3,\"56\":1}}],[\"with\",{\"1\":{\"64\":1,\"73\":1,\"86\":2,\"94\":4}}],[\"witness\",{\"1\":{\"60\":2}}],[\"will\",{\"1\":{\"16\":1,\"50\":1,\"52\":3,\"53\":2,\"94\":1}}],[\"way\",{\"1\":{\"94\":1}}],[\"waking\",{\"1\":{\"94\":1}}],[\"walk\",{\"1\":{\"94\":1}}],[\"warmup\",{\"1\":{\"71\":1,\"74\":2}}],[\"warning\",{\"1\":{\"64\":1}}],[\"wash\",{\"1\":{\"94\":1}}],[\"wasnt\",{\"1\":{\"94\":1}}],[\"was\",{\"1\":{\"40\":2,\"94\":7}}],[\"waitress\",{\"1\":{\"17\":1}}],[\"waiter\",{\"1\":{\"17\":1}}],[\"waiting\",{\"1\":{\"16\":2,\"28\":2,\"50\":3,\"52\":1,\"53\":1,\"55\":3,\"56\":1}}],[\"would\",{\"1\":{\"94\":1}}],[\"wordpiece\",{\"1\":{\"42\":1}}],[\"word\",{\"0\":{\"40\":1},\"1\":{\"40\":1,\"41\":3}}],[\"words\",{\"1\":{\"28\":1,\"41\":1}}],[\"works\",{\"1\":{\"17\":2}}],[\"woman\",{\"1\":{\"17\":1,\"94\":1}}],[\"wear\",{\"1\":{\"94\":1}}],[\"weeks\",{\"1\":{\"94\":3}}],[\"web\",{\"0\":{\"82\":1},\"1\":{\"78\":1}}],[\"weight\",{\"1\":{\"43\":1,\"70\":1,\"94\":2}}],[\"we\",{\"1\":{\"16\":1}}],[\"==\",{\"1\":{\"94\":1}}],[\"=\",{\"1\":{\"16\":2,\"17\":3,\"28\":5,\"29\":2,\"30\":1,\"31\":3,\"32\":1,\"34\":2,\"35\":2,\"37\":4,\"40\":1,\"43\":2,\"45\":3,\"46\":1,\"47\":1,\"50\":21,\"51\":7,\"52\":7,\"53\":4,\"55\":4,\"56\":6,\"60\":2,\"61\":7,\"62\":4,\"63\":5,\"64\":3,\"65\":9,\"66\":5,\"67\":2,\"68\":2,\"69\":2,\"70\":1,\"71\":3,\"72\":5,\"73\":5,\"74\":22,\"80\":4,\"85\":1,\"86\":3,\"91\":5,\"92\":3,\"94\":5,\"95\":4}}],[\"函数将一次处理多个数据\",{\"1\":{\"96\":1}}],[\"函数了\",{\"1\":{\"65\":1}}],[\"函数加载与\",{\"1\":{\"65\":1}}],[\"函数返回的\",{\"1\":{\"65\":1}}],[\"函数并传递给了\",{\"1\":{\"65\":1}}],[\"函数并在训练时使用它\",{\"1\":{\"65\":1}}],[\"函数提供计算模型性能的方法\",{\"1\":{\"64\":1}}],[\"函数的处理方式是想数据集中添加新的字段\",{\"1\":{\"61\":1}}],[\"函数来实现这一点\",{\"1\":{\"28\":1}}],[\"函数来执行不同的\",{\"1\":{\"25\":1}}],[\"函数实际上经过了以下几个步骤\",{\"1\":{\"27\":1}}],[\"函数完成的任务\",{\"1\":{\"26\":1}}],[\"函数\",{\"1\":{\"16\":1,\"65\":1}}],[\"库提供的\",{\"1\":{\"73\":1}}],[\"库提供了很多方法和类\",{\"1\":{\"81\":1}}],[\"库提供了简单易用的命令来下载并缓存\",{\"1\":{\"60\":1}}],[\"库提供了简单的api\",{\"1\":{\"26\":1}}],[\"库提供了\",{\"1\":{\"16\":1}}],[\"库通过\",{\"1\":{\"62\":1}}],[\"库已经使用了多线程\",{\"1\":{\"61\":1}}],[\"库用\",{\"1\":{\"61\":1}}],[\"库在任何分布式设备上轻松运行自定义训练过程\",{\"1\":{\"58\":1}}],[\"库还提供了\",{\"1\":{\"29\":1}}],[\"库\",{\"0\":{\"16\":1,\"81\":1,\"89\":1},\"1\":{\"65\":1,\"72\":1,\"78\":1,\"89\":2}}],[\"库来创建并使用公开的模型\",{\"1\":{\"15\":1}}],[\"你能看到在\",{\"1\":{\"88\":1}}],[\"你需要使用\",{\"1\":{\"81\":1}}],[\"你需要使用预训练模型以及针对特定任务的数据集再次进行训练\",{\"1\":{\"21\":1}}],[\"你需要身份令牌一遍\",{\"1\":{\"80\":1}}],[\"你把代码贴到函数下面\",{\"1\":{\"74\":1}}],[\"你也可以用\",{\"1\":{\"80\":1}}],[\"你也可以使用\",{\"1\":{\"74\":1}}],[\"你也可以在\",{\"1\":{\"15\":1}}],[\"你将能够回答以下问题\",{\"1\":{\"89\":1}}],[\"你将了解到\",{\"1\":{\"58\":1}}],[\"你将会看到\",{\"1\":{\"14\":1}}],[\"你应该了解\",{\"1\":{\"48\":1}}],[\"你最后一次保存\",{\"1\":{\"36\":1}}],[\"你可以通过\",{\"1\":{\"88\":1}}],[\"你可以通过设置\",{\"1\":{\"35\":1,\"60\":1}}],[\"你可以阅读\",{\"1\":{\"87\":1}}],[\"你可以训练模型\",{\"1\":{\"80\":1}}],[\"你可以使用\",{\"1\":{\"80\":2}}],[\"你可以使用🤗\",{\"1\":{\"15\":1}}],[\"你可以自己去探索\",{\"1\":{\"76\":1}}],[\"你可以先看看数据\",{\"1\":{\"60\":1}}],[\"你可以在\",{\"1\":{\"36\":1,\"74\":1}}],[\"你可以在这个\",{\"1\":{\"35\":1}}],[\"你可以在模型中心中查找模型\",{\"1\":{\"25\":1}}],[\"你可以在模型中心中查找预训练模型\",{\"1\":{\"15\":1}}],[\"你可以直接使用随机初始化的\",{\"1\":{\"35\":1}}],[\"你可以直接使用对应的\",{\"1\":{\"33\":1}}],[\"能做什么\",{\"0\":{\"15\":1}}],[\"它使用\",{\"1\":{\"84\":1}}],[\"它会生成\",{\"1\":{\"80\":1}}],[\"它会下载对应模型的\",{\"1\":{\"28\":1}}],[\"它返回的对象有\",{\"1\":{\"65\":1}}],[\"它包含\",{\"1\":{\"64\":1}}],[\"它接收一个\",{\"1\":{\"62\":1}}],[\"它把你的数据集转化为\",{\"1\":{\"62\":1}}],[\"它不会将整个\",{\"1\":{\"61\":1}}],[\"它有\",{\"1\":{\"60\":1}}],[\"它有以下三个维度\",{\"1\":{\"30\":1}}],[\"它和\",{\"1\":{\"52\":1,\"70\":1,\"94\":1}}],[\"它必须是矩阵\",{\"1\":{\"51\":1}}],[\"它可以减少\",{\"1\":{\"40\":1}}],[\"它的目标是找到最有意义的表示\",{\"1\":{\"38\":1}}],[\"它将输入的文本转化为数字\",{\"1\":{\"37\":1}}],[\"它根据\",{\"1\":{\"33\":1}}],[\"它也有\",{\"1\":{\"29\":1}}],[\"它就没那么有针对性了\",{\"1\":{\"19\":1}}],[\"它聚合了预训练模型和对应的文本预处理\",{\"1\":{\"16\":1}}],[\"它是大规模意大利语问答数据集\",{\"1\":{\"91\":1}}],[\"它是构建\",{\"1\":{\"62\":1}}],[\"它是怎么处理数字输入并输出预测的\",{\"1\":{\"26\":1}}],[\"它是\",{\"1\":{\"14\":1,\"59\":1}}],[\"它赋予precision\",{\"1\":{\"11\":1}}],[\"团队为\",{\"1\":{\"14\":1}}],[\"gained\",{\"1\":{\"94\":1}}],[\"gain\",{\"1\":{\"94\":1}}],[\"great\",{\"1\":{\"94\":1}}],[\"grad\",{\"1\":{\"32\":2,\"50\":1,\"51\":3,\"52\":1,\"69\":1,\"72\":1,\"73\":1,\"74\":2}}],[\"gluten\",{\"1\":{\"94\":2}}],[\"glue\",{\"1\":{\"59\":1,\"60\":1,\"63\":1,\"65\":2,\"66\":1,\"73\":1}}],[\"gzip\",{\"1\":{\"91\":1}}],[\"gz\",{\"1\":{\"91\":5,\"92\":2}}],[\"github\",{\"1\":{\"91\":2,\"92\":1}}],[\"gitattributes\",{\"1\":{\"85\":1,\"86\":1}}],[\"git\",{\"0\":{\"86\":1},\"1\":{\"76\":1,\"78\":2,\"83\":2,\"84\":2,\"85\":16,\"86\":23}}],[\"gpu\",{\"1\":{\"65\":1,\"72\":2,\"74\":1,\"75\":1}}],[\"gpt\",{\"1\":{\"18\":1,\"19\":1,\"22\":2,\"42\":1}}],[\"going\",{\"1\":{\"94\":1}}],[\"gotten\",{\"1\":{\"94\":1}}],[\"got\",{\"1\":{\"50\":1}}],[\"google\",{\"1\":{\"14\":1}}],[\"get\",{\"1\":{\"71\":2,\"74\":4}}],[\"gelu\",{\"1\":{\"34\":1}}],[\"generation\",{\"1\":{\"16\":1}}],[\"generator\",{\"1\":{\"16\":2}}],[\"generalized\",{\"1\":{\"4\":2}}],[\"如上面的\",{\"1\":{\"80\":1}}],[\"如我们要使用\",{\"1\":{\"77\":1}}],[\"如何获得同时有\",{\"1\":{\"91\":1}}],[\"如何创建自己的数据集并将其推至\",{\"1\":{\"89\":1}}],[\"如何对数据集进行切片\",{\"1\":{\"89\":1}}],[\"如何从\",{\"1\":{\"58\":1}}],[\"如何利用\",{\"1\":{\"57\":1,\"58\":1}}],[\"如何使用自定义训练过程\",{\"1\":{\"58\":1}}],[\"如何使用\",{\"1\":{\"57\":1,\"58\":1,\"87\":1}}],[\"如何处理不同长度的多个序列\",{\"1\":{\"49\":1}}],[\"如何处理多个序列\",{\"1\":{\"49\":1}}],[\"如何生成\",{\"1\":{\"43\":1}}],[\"如果使用\",{\"1\":{\"86\":1}}],[\"如果要将\",{\"1\":{\"80\":1}}],[\"如果想要在每个\",{\"1\":{\"65\":1}}],[\"如果想让两次的结果相同\",{\"1\":{\"51\":1}}],[\"如果我们自定义了\",{\"1\":{\"65\":1}}],[\"如果我们想使用这种\",{\"1\":{\"40\":1}}],[\"如果遇到下面的错误\",{\"1\":{\"64\":1}}],[\"如果传入了大于最大限度的序列会崩溃\",{\"1\":{\"53\":1}}],[\"如果两个序列的长度不一样怎么办\",{\"1\":{\"51\":1}}],[\"如果某个\",{\"1\":{\"40\":1}}],[\"如果你确实需要使用\",{\"1\":{\"89\":1}}],[\"如果你确切知道你想使用什么类型的模型\",{\"1\":{\"33\":1}}],[\"如果你使用\",{\"1\":{\"80\":1}}],[\"如果你要复制粘贴分布式训练的代码\",{\"1\":{\"74\":1}}],[\"如果你想使用某个特定的\",{\"1\":{\"80\":1}}],[\"如果你想将仓库放到组织下\",{\"1\":{\"80\":1}}],[\"如果你想在\",{\"1\":{\"74\":1}}],[\"如果你想在训练过程中自动上传你的模型到\",{\"1\":{\"64\":1}}],[\"如果你想做进一步了解\",{\"1\":{\"25\":1}}],[\"如果你的数据集很大\",{\"1\":{\"89\":1}}],[\"如果你的\",{\"1\":{\"61\":1}}],[\"如果你不想了解这些细节\",{\"1\":{\"59\":1}}],[\"如果还在同一环境中\",{\"1\":{\"29\":1}}],[\"如果不想使用默认模型\",{\"1\":{\"16\":1}}],[\"如翻译或摘要\",{\"1\":{\"22\":1}}],[\"如文本生成\",{\"1\":{\"22\":1}}],[\"如句子分类和命名实体识别\",{\"1\":{\"22\":1}}],[\"如\",{\"1\":{\"14\":1,\"91\":1,\"94\":1}}],[\"也提供了一些函数处理\",{\"1\":{\"94\":1}}],[\"也是\",{\"1\":{\"52\":1,\"64\":1}}],[\"也会被考虑进去\",{\"1\":{\"51\":1}}],[\"也会减少\",{\"1\":{\"41\":1}}],[\"也就是\",{\"1\":{\"40\":1}}],[\"也有针对标点符号增加了额外规则的\",{\"1\":{\"40\":1}}],[\"也可做同样的操作\",{\"1\":{\"80\":1}}],[\"也可以上传自己的模型和数据集\",{\"1\":{\"76\":1}}],[\"也可以指定\",{\"1\":{\"54\":1}}],[\"也可以使用\",{\"1\":{\"43\":1}}],[\"也可以说\",{\"1\":{\"14\":1}}],[\"也可能是指\",{\"1\":{\"14\":1}}],[\"也不能让正常邮件直接进垃圾箱\",{\"1\":{\"11\":1}}],[\"也不能让真正患病的人检测不出癌症\",{\"1\":{\"11\":1}}],[\"可能不会有\",{\"1\":{\"61\":1}}],[\"可能被切分成\",{\"1\":{\"42\":1}}],[\"可能有十几个\",{\"1\":{\"41\":1}}],[\"可能本身没有含义\",{\"1\":{\"41\":1}}],[\"可能是患者的\",{\"1\":{\"94\":1}}],[\"可能是\",{\"1\":{\"28\":1}}],[\"可能是指\",{\"1\":{\"14\":1}}],[\"可以配置\",{\"1\":{\"96\":1}}],[\"可以传入\",{\"1\":{\"95\":1}}],[\"可以是单个文件路径\",{\"1\":{\"91\":1}}],[\"可以观察到\",{\"1\":{\"86\":1}}],[\"可以发现\",{\"1\":{\"86\":1}}],[\"可以指定\",{\"1\":{\"81\":1}}],[\"可以用来进行\",{\"1\":{\"65\":1}}],[\"可以将\",{\"1\":{\"65\":1}}],[\"可以将序列转化成特定结构的\",{\"1\":{\"50\":1}}],[\"可以尝试下面方法\",{\"1\":{\"64\":1}}],[\"可以通过传递\",{\"1\":{\"61\":1}}],[\"可以通过查看\",{\"1\":{\"60\":1}}],[\"可以在\",{\"1\":{\"51\":1,\"64\":1,\"77\":1}}],[\"可以使用进度条\",{\"1\":{\"72\":1}}],[\"可以使用\",{\"1\":{\"47\":1,\"53\":1,\"65\":1,\"86\":1,\"94\":1,\"95\":1}}],[\"可以使用空格来将句子切分为字词\",{\"1\":{\"40\":1}}],[\"可以接收很多参数\",{\"1\":{\"37\":1}}],[\"可以\",{\"1\":{\"24\":1}}],[\"可以绘出混淆矩阵\",{\"1\":{\"7\":1}}],[\"可通过\",{\"1\":{\"16\":1}}],[\"范语\",{\"1\":{\"14\":1}}],[\"这将在本地创建文件夹<path\",{\"1\":{\"85\":1}}],[\"这将会创建名为\",{\"1\":{\"80\":1}}],[\"这里会用到\",{\"1\":{\"94\":2}}],[\"这里我们传入了\",{\"1\":{\"94\":1}}],[\"这里不展开介绍\",{\"1\":{\"82\":1}}],[\"这里只是画成\",{\"1\":{\"29\":1}}],[\"这种方式既可以创建仓库\",{\"1\":{\"80\":1}}],[\"这种情况是最应该避免的\",{\"1\":{\"11\":2}}],[\"这次我们再执行\",{\"1\":{\"65\":1}}],[\"这样\",{\"1\":{\"95\":1}}],[\"这样我们只需要将长度填充为该\",{\"1\":{\"61\":1}}],[\"这样模型才能进行处理\",{\"1\":{\"61\":1}}],[\"这样做有两点好处\",{\"1\":{\"41\":1}}],[\"这是用来衡量\",{\"1\":{\"65\":1}}],[\"这是因为\",{\"1\":{\"64\":1}}],[\"这是有用的\",{\"1\":{\"61\":1}}],[\"这是在预训练时使用的\",{\"1\":{\"55\":1}}],[\"这是logits\",{\"1\":{\"32\":1}}],[\"这要通过\",{\"1\":{\"51\":1}}],[\"这要用到\",{\"1\":{\"35\":1}}],[\"这显然是不应该的\",{\"1\":{\"51\":1}}],[\"这个词表和预训练时的词表是相同的\",{\"1\":{\"44\":1}}],[\"这个架构只包括最基本的\",{\"1\":{\"29\":1}}],[\"这也会产生一些问题\",{\"1\":{\"41\":1}}],[\"这两个文件相辅相成\",{\"1\":{\"36\":1}}],[\"这两部分可以单独使用\",{\"1\":{\"22\":1}}],[\"这会保存两个文件\",{\"1\":{\"36\":1}}],[\"这类模型有\",{\"1\":{\"22\":3}}],[\"这类模型在其进行训练的语料上进行了理解\",{\"1\":{\"19\":1}}],[\"这取决于你要做什么任务\",{\"1\":{\"22\":1}}],[\"这意味着它无法检索到一个词的合理表示\",{\"1\":{\"40\":1}}],[\"这意味着该模型已针对生成输出进行了优化\",{\"1\":{\"22\":1}}],[\"这意味着对模型进行了优化\",{\"1\":{\"22\":1}}],[\"这篇文章\",{\"1\":{\"22\":1}}],[\"这往往需要使用大规模语料\",{\"1\":{\"21\":1}}],[\"这其中可能会夹杂一些意识形态或者价值观的刻板印象\",{\"1\":{\"17\":1}}],[\"这些术语\",{\"1\":{\"14\":1}}],[\"术语\",{\"0\":{\"14\":1}}],[\"根据输入文本生成新的句子\",{\"1\":{\"13\":1}}],[\"根据上下文中提供的信息提取问题的答案\",{\"1\":{\"13\":1}}],[\"用逗号作为分割符\",{\"1\":{\"94\":1}}],[\"用于迭代批次\",{\"0\":{\"68\":1}}],[\"用来衡量\",{\"1\":{\"59\":1}}],[\"用屏蔽词填充文本中的空白\",{\"1\":{\"13\":1}}],[\"用自动生成的文本完成提示\",{\"1\":{\"13\":1}}],[\"生成上下文\",{\"1\":{\"13\":1}}],[\"地点\",{\"1\":{\"13\":1}}],[\"人\",{\"1\":{\"13\":1}}],[\"形容词\",{\"1\":{\"13\":1}}],[\"识别句子的语法成分\",{\"1\":{\"13\":1}}],[\"对\",{\"1\":{\"80\":1}}],[\"对模型进行微调\",{\"1\":{\"80\":1}}],[\"对应的代码\",{\"1\":{\"67\":1}}],[\"对象呢\",{\"1\":{\"91\":1}}],[\"对象的\",{\"1\":{\"80\":1}}],[\"对象\",{\"1\":{\"28\":1,\"60\":1,\"91\":1,\"94\":1}}],[\"对于预测没有提供相对有用的信息\",{\"1\":{\"95\":1}}],[\"对于大文件\",{\"1\":{\"83\":1,\"86\":1}}],[\"对于基础的微调来说表现得也很不错\",{\"1\":{\"64\":1}}],[\"对于短的序列\",{\"1\":{\"51\":1}}],[\"对于\",{\"1\":{\"41\":1,\"51\":1,\"83\":1}}],[\"对于第一个句子\",{\"1\":{\"32\":1}}],[\"对于具体问题\",{\"1\":{\"19\":1}}],[\"对于一个给定\",{\"1\":{\"14\":1}}],[\"对句子中的每个词语进行分类\",{\"1\":{\"13\":1}}],[\"对整个句子进行分类\",{\"1\":{\"13\":1}}],[\"检测电子邮件是否为垃圾邮件\",{\"1\":{\"13\":1}}],[\"获取评论的情绪\",{\"1\":{\"13\":1}}],[\"获得\",{\"1\":{\"2\":1,\"37\":2}}],[\"而非创建\",{\"1\":{\"81\":1}}],[\"而使用\",{\"1\":{\"41\":1}}],[\"而不是\",{\"1\":{\"41\":1}}],[\"而重新训练又需要大量的时间和数据\",{\"1\":{\"35\":1}}],[\"而\",{\"1\":{\"24\":1,\"94\":1}}],[\"而对\",{\"1\":{\"23\":1}}],[\"而是要理解上下文的含义\",{\"1\":{\"13\":1}}],[\"而耽误治疗离世\",{\"1\":{\"11\":1}}],[\"多分类模型accuracy\",{\"1\":{\"11\":1}}],[\"多重假设检验\",{\"1\":{\"4\":1}}],[\"虽然这不致命\",{\"1\":{\"11\":1}}],[\"继续以癌症诊断场景为\",{\"1\":{\"11\":1}}],[\"尽可能提高precision值\",{\"1\":{\"11\":1}}],[\"尽可能提高recall值\",{\"1\":{\"11\":1}}],[\"于是预训练模型的\",{\"1\":{\"64\":1}}],[\"于是这里我们没有启用多进程\",{\"1\":{\"61\":1}}],[\"于是构建每个单词到\",{\"1\":{\"40\":1}}],[\"于是我们将它输入至\",{\"1\":{\"32\":1}}],[\"于是我们不用\",{\"1\":{\"31\":1}}],[\"于是我们需要进行迁移学习\",{\"1\":{\"19\":1}}],[\"于是要先将文本输入转换成数字表示\",{\"1\":{\"28\":1}}],[\"于是\",{\"1\":{\"11\":1}}],[\"正常邮件为\",{\"1\":{\"11\":1}}],[\"垃圾邮件分类的目标是\",{\"1\":{\"11\":1}}],[\"垃圾邮件分类时\",{\"1\":{\"11\":1}}],[\"垃圾邮件为positive\",{\"1\":{\"11\":1}}],[\"癌症诊断系统的目标是\",{\"1\":{\"11\":1}}],[\"癌症诊断场景中\",{\"1\":{\"11\":1}}],[\"例如\",{\"1\":{\"11\":2,\"24\":1,\"40\":1,\"86\":1}}],[\"时配置\",{\"1\":{\"80\":1}}],[\"时就会在每个\",{\"1\":{\"65\":1}}],[\"时间\",{\"1\":{\"65\":1}}],[\"时你会看到\",{\"1\":{\"64\":1}}],[\"时的一个参数\",{\"1\":{\"62\":1}}],[\"时该结构会不一样\",{\"1\":{\"61\":1}}],[\"时使用的\",{\"1\":{\"36\":1}}],[\"时\",{\"1\":{\"11\":2,\"23\":2,\"35\":1,\"50\":1,\"61\":2,\"64\":1,\"80\":2}}],[\"希望尽量避免产生\",{\"1\":{\"11\":2}}],[\"当你调用\",{\"1\":{\"80\":1}}],[\"当我们将全部\",{\"1\":{\"73\":1}}],[\"当在\",{\"1\":{\"64\":1}}],[\"当进行批处理时\",{\"1\":{\"51\":1}}],[\"当然你也可以将\",{\"1\":{\"35\":1}}],[\"当然\",{\"1\":{\"29\":1,\"41\":1}}],[\"当\",{\"1\":{\"11\":2}}],[\"^first\",{\"1\":{\"11\":1}}],[\"使得同时对数据集中的多个元素同时做处理\",{\"1\":{\"61\":1}}],[\"使得用户可以通过它来加载\",{\"1\":{\"26\":1}}],[\"使其和最长的序列一样长\",{\"1\":{\"51\":1}}],[\"使其成为准确性指标的替代方案\",{\"1\":{\"11\":1}}],[\"使用制表符作为分隔符\",{\"1\":{\"94\":1}}],[\"使用预训练模型时\",{\"1\":{\"77\":1}}],[\"使用预训练模型\",{\"0\":{\"77\":1}}],[\"使用的\",{\"1\":{\"86\":2}}],[\"使用的默认\",{\"1\":{\"64\":1}}],[\"使用的算法\",{\"1\":{\"43\":1}}],[\"使用其他的\",{\"1\":{\"61\":1}}],[\"使用不同的\",{\"1\":{\"61\":1}}],[\"使用了\",{\"1\":{\"56\":1}}],[\"使用了来自\",{\"1\":{\"24\":1}}],[\"使用该类需要安装\",{\"1\":{\"85\":1}}],[\"使用该方法会加载或保存\",{\"1\":{\"43\":1}}],[\"使用该函数可以直接根据输入返回目标输出\",{\"1\":{\"16\":1}}],[\"使用这种方式\",{\"1\":{\"42\":2}}],[\"使用默认的配置来创建\",{\"1\":{\"35\":1}}],[\"使用模型的\",{\"1\":{\"28\":1}}],[\"使用模型进行预测时使用的上述操作应该和预训练时的操作一致\",{\"1\":{\"28\":1}}],[\"使用\",{\"0\":{\"26\":1,\"28\":1,\"37\":1,\"63\":1,\"66\":1,\"74\":1,\"80\":1,\"81\":1,\"82\":1},\"1\":{\"28\":1,\"32\":1,\"36\":1,\"40\":1,\"51\":1,\"57\":1,\"74\":1,\"75\":1,\"78\":2,\"80\":1,\"84\":1,\"89\":1,\"94\":1}}],[\"使用注意力层\",{\"1\":{\"24\":1}}],[\"使用更小的数据集完成\",{\"1\":{\"21\":1}}],[\"使用分类器做预测后\",{\"1\":{\"7\":1}}],[\"和多\",{\"1\":{\"65\":1}}],[\"和我们之前使用\",{\"1\":{\"64\":1}}],[\"和预训练模型来进行推理\",{\"1\":{\"58\":1}}],[\"和编码\",{\"1\":{\"47\":1}}],[\"和\",{\"0\":{\"76\":1},\"1\":{\"11\":1,\"23\":1,\"24\":1,\"26\":1,\"40\":2,\"43\":1,\"52\":2,\"53\":1,\"55\":1,\"56\":1,\"57\":1,\"60\":1,\"61\":1,\"64\":1,\"65\":3,\"75\":1,\"80\":2,\"83\":1,\"85\":4,\"86\":4,\"91\":1,\"94\":3}}],[\"是患者\",{\"1\":{\"94\":1}}],[\"是什么\",{\"1\":{\"89\":1}}],[\"是通过\",{\"1\":{\"87\":1}}],[\"是都要创建\",{\"1\":{\"81\":1}}],[\"是每个\",{\"1\":{\"80\":1}}],[\"是主网站\",{\"1\":{\"76\":1}}],[\"是否想用给定的\",{\"1\":{\"81\":1}}],[\"是否对其他人可见\",{\"1\":{\"81\":1}}],[\"是否出错时使用的\",{\"1\":{\"69\":1}}],[\"是否能在\",{\"1\":{\"62\":1}}],[\"是否存在序列过长的问题\",{\"1\":{\"49\":1}}],[\"是指一次向模型传递多个句子\",{\"1\":{\"50\":1}}],[\"是在预训练好的模型上进行进一步的训练\",{\"1\":{\"21\":1}}],[\"是大模型\",{\"0\":{\"20\":1}}],[\"是语言模型\",{\"0\":{\"19\":1}}],[\"是由\",{\"1\":{\"14\":1}}],[\"是一个\",{\"1\":{\"14\":1}}],[\"是将没有患癌症的人诊断为癌症\",{\"1\":{\"11\":1}}],[\"是把正常邮件识别为垃圾邮件\",{\"1\":{\"11\":1}}],[\"是得了癌症的病人没有被诊断出癌症\",{\"1\":{\"11\":1}}],[\"是\",{\"1\":{\"11\":1,\"28\":1,\"42\":1,\"60\":2,\"86\":2,\"94\":1}}],[\"48\",{\"1\":{\"91\":1}}],[\"442\",{\"1\":{\"91\":2}}],[\"4418e\",{\"1\":{\"32\":1}}],[\"400mb\",{\"1\":{\"86\":1}}],[\"408\",{\"1\":{\"60\":1,\"61\":1,\"65\":2}}],[\"473\",{\"1\":{\"95\":1}}],[\"47\",{\"1\":{\"62\":1}}],[\"4125\",{\"1\":{\"51\":1,\"52\":1}}],[\"4658\",{\"1\":{\"37\":1}}],[\"4\",{\"0\":{\"11\":1,\"22\":1,\"23\":1,\"24\":1,\"47\":1,\"49\":1,\"50\":1,\"51\":1,\"52\":1,\"53\":2,\"71\":1,\"74\":1,\"76\":1},\"1\":{\"32\":2,\"34\":1,\"43\":1,\"54\":1,\"64\":1,\"94\":1}}],[\"召回率\",{\"0\":{\"10\":1}}],[\"37\",{\"1\":{\"94\":1}}],[\"35686c2\",{\"1\":{\"86\":1}}],[\"32\",{\"1\":{\"62\":1}}],[\"3668\",{\"1\":{\"60\":1,\"61\":1}}],[\"3374\",{\"1\":{\"51\":1}}],[\"3895\",{\"1\":{\"51\":2,\"52\":1}}],[\"3835\",{\"1\":{\"37\":1}}],[\"30\",{\"1\":{\"95\":2}}],[\"30mg\",{\"1\":{\"94\":1}}],[\"3014\",{\"1\":{\"43\":1,\"46\":1,\"47\":1}}],[\"30522\",{\"1\":{\"34\":1}}],[\"3072\",{\"1\":{\"34\":1}}],[\"3422\",{\"1\":{\"94\":1}}],[\"3464\",{\"1\":{\"32\":1}}],[\"3403\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":2,\"55\":2}}],[\"3\",{\"0\":{\"10\":1,\"18\":1,\"19\":1,\"20\":1,\"21\":2,\"32\":1,\"38\":1,\"39\":1,\"40\":1,\"41\":1,\"42\":2,\"43\":1,\"44\":2,\"45\":2,\"46\":2,\"47\":1,\"48\":1,\"52\":1,\"58\":1,\"62\":1,\"66\":1,\"67\":1,\"68\":1,\"69\":1,\"70\":2,\"71\":1,\"72\":1,\"73\":2,\"74\":1,\"82\":1,\"84\":1,\"85\":1,\"86\":2,\"87\":1,\"96\":1},\"1\":{\"32\":1,\"43\":2,\"44\":1,\"71\":2,\"74\":2,\"94\":1}}],[\"3f\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"11\":1}}],[\"真实positve的数据到底占多少\",{\"1\":{\"9\":1}}],[\"在处理\",{\"1\":{\"95\":1}}],[\"在第三章中我们介绍了\",{\"1\":{\"94\":1}}],[\"在第三章中我们初步体验了\",{\"1\":{\"89\":1}}],[\"在本章内容中\",{\"1\":{\"89\":1}}],[\"在本篇文章中\",{\"1\":{\"58\":1}}],[\"在较低层的实现中\",{\"1\":{\"80\":1}}],[\"在哪些数据集上训练的\",{\"1\":{\"77\":1}}],[\"在前两章中你了解了\",{\"1\":{\"75\":1}}],[\"在多\",{\"1\":{\"65\":1}}],[\"在每个\",{\"1\":{\"64\":1}}],[\"在实例化\",{\"1\":{\"64\":1}}],[\"在我们定义\",{\"1\":{\"64\":1}}],[\"在批处理中这将数据整理到一个\",{\"1\":{\"62\":1}}],[\"在批处理句子时\",{\"1\":{\"24\":1}}],[\"在这个例子中\",{\"1\":{\"61\":1}}],[\"在顶部加了一个维度\",{\"1\":{\"50\":1}}],[\"在此之前我们直接调用\",{\"1\":{\"50\":1}}],[\"在之前的例子中\",{\"1\":{\"49\":1}}],[\"在使用过程中直接调用\",{\"1\":{\"44\":1}}],[\"在一些多语言模型中使用\",{\"1\":{\"42\":1}}],[\"在大一些的模型中可能是3072甚至更大\",{\"1\":{\"30\":1}}],[\"在上一节中\",{\"1\":{\"28\":1}}],[\"在\",{\"1\":{\"24\":1,\"37\":2,\"64\":1,\"66\":1,\"73\":1,\"75\":1,\"76\":1,\"78\":1,\"88\":1}}],[\"在生成第4个单词时\",{\"1\":{\"24\":1}}],[\"在训练过程中\",{\"1\":{\"24\":1}}],[\"在翻译\",{\"1\":{\"23\":1}}],[\"在做文本翻译任务时\",{\"1\":{\"23\":1}}],[\"在该文章中\",{\"1\":{\"22\":1}}],[\"在迁移学习时\",{\"1\":{\"19\":1}}],[\"在模型上进一步微调并不会消除这种偏差\",{\"1\":{\"17\":1}}],[\"在接下来的学习中\",{\"1\":{\"14\":1}}],[\"在所有的positive数据中\",{\"1\":{\"10\":1}}],[\"在预测为positive的所有数据中\",{\"1\":{\"9\":1}}],[\"在评价一个二分类的机器学习模型的性能时\",{\"1\":{\"7\":1}}],[\"精确度\",{\"0\":{\"9\":1}}],[\"把正样本错误的预测为负\",{\"1\":{\"7\":1}}],[\"把正样本成功预测为正\",{\"1\":{\"7\":1}}],[\"把负样本错误地预测为正\",{\"1\":{\"7\":1}}],[\"把负样本成功预测为负\",{\"1\":{\"7\":1}}],[\"机器学习的评价指标\",{\"0\":{\"7\":1}}],[\"joints\",{\"1\":{\"94\":1}}],[\"just\",{\"1\":{\"94\":1}}],[\"jim\",{\"1\":{\"40\":2}}],[\"jsonl\",{\"1\":{\"90\":1}}],[\"json\",{\"1\":{\"36\":3,\"84\":2,\"86\":12,\"90\":4,\"91\":13,\"92\":3}}],[\"jjj\",{\"0\":{\"6\":1}}],[\"jth\",{\"1\":{\"5\":1}}],[\"随机变量\",{\"1\":{\"5\":1}}],[\"矩阵\",{\"1\":{\"5\":2}}],[\"的词数较少时\",{\"1\":{\"95\":1}}],[\"的数据过滤掉\",{\"1\":{\"94\":1}}],[\"的这个想法是正确的\",{\"1\":{\"94\":1}}],[\"的猜想\",{\"1\":{\"94\":1}}],[\"的区别在于\",{\"1\":{\"94\":1}}],[\"的变体\",{\"1\":{\"94\":1}}],[\"的匹配规则选择多有满足规则的文件\",{\"1\":{\"91\":1}}],[\"的基本步骤\",{\"1\":{\"89\":1}}],[\"的基本结构\",{\"1\":{\"57\":1}}],[\"的重要作用\",{\"1\":{\"87\":1}}],[\"的大小超过了\",{\"1\":{\"86\":1}}],[\"的版本是最新的\",{\"1\":{\"85\":1}}],[\"的方式\",{\"1\":{\"86\":1}}],[\"的方式管理本地仓库\",{\"1\":{\"85\":1}}],[\"的方式在大量文本上进行了训练\",{\"1\":{\"19\":1}}],[\"的方法\",{\"1\":{\"83\":1}}],[\"的文件管理系统基于\",{\"1\":{\"83\":1}}],[\"的登录命令\",{\"1\":{\"81\":1}}],[\"的仓库\",{\"1\":{\"80\":1}}],[\"的加速\",{\"1\":{\"74\":1}}],[\"的长度\",{\"1\":{\"71\":1}}],[\"的线性衰减\",{\"1\":{\"71\":1}}],[\"的默认\",{\"1\":{\"70\":1}}],[\"的格式设为\",{\"1\":{\"67\":1}}],[\"的最大值的取出\",{\"1\":{\"65\":1}}],[\"的最大长度为\",{\"1\":{\"62\":1}}],[\"的路径\",{\"1\":{\"64\":1}}],[\"的函数称为\",{\"1\":{\"62\":1}}],[\"的值是\",{\"1\":{\"61\":1}}],[\"的含义\",{\"1\":{\"60\":1}}],[\"的十个数据集之一\",{\"1\":{\"59\":1}}],[\"的限制\",{\"1\":{\"57\":1}}],[\"的组成\",{\"1\":{\"57\":1}}],[\"的类型\",{\"1\":{\"54\":1}}],[\"的结果累积后就可以使用\",{\"1\":{\"73\":1}}],[\"的结果一致了\",{\"1\":{\"52\":1}}],[\"的结果不一样\",{\"1\":{\"51\":1}}],[\"的结果的第二行与\",{\"1\":{\"51\":1}}],[\"的结构\",{\"1\":{\"25\":1}}],[\"的结构和原理\",{\"1\":{\"22\":1}}],[\"的原子操作\",{\"1\":{\"48\":1}}],[\"的原则是\",{\"1\":{\"42\":1}}],[\"的该过程相反\",{\"1\":{\"47\":1}}],[\"的两个步骤分别做了什么\",{\"1\":{\"44\":1}}],[\"的时候已经下载好了\",{\"1\":{\"44\":1}}],[\"的分为两步\",{\"1\":{\"44\":1}}],[\"的中间方法\",{\"1\":{\"43\":1}}],[\"的产生\",{\"1\":{\"40\":1}}],[\"的映射要有\",{\"1\":{\"40\":1}}],[\"的作用是将文本转化为模型可以处理的数字\",{\"1\":{\"38\":1}}],[\"的来源或路径\",{\"1\":{\"36\":1}}],[\"的内容\",{\"1\":{\"34\":1}}],[\"的模型\",{\"1\":{\"31\":1}}],[\"的\",{\"1\":{\"29\":1,\"32\":1,\"43\":3,\"45\":1,\"60\":1,\"64\":2,\"65\":1,\"74\":1,\"91\":3,\"95\":1}}],[\"的处理步骤\",{\"1\":{\"28\":1}}],[\"的输出是一个命名元祖\",{\"1\":{\"65\":1}}],[\"的输出是\",{\"1\":{\"65\":1}}],[\"的输出\",{\"1\":{\"24\":1}}],[\"的输入来生成第4个单词\",{\"1\":{\"24\":1}}],[\"的过去的输入\",{\"1\":{\"24\":1}}],[\"的关注度可能小一些\",{\"1\":{\"23\":1}}],[\"的目标\",{\"1\":{\"22\":1}}],[\"的权重\",{\"1\":{\"14\":1}}],[\"的任务不仅仅是理解单个字词的含义\",{\"1\":{\"13\":1}}],[\"的成本代价很高\",{\"1\":{\"11\":2}}],[\"的综合考量\",{\"1\":{\"11\":1}}],[\"的向量\",{\"1\":{\"5\":2}}],[\"的学习过程\",{\"1\":{\"1\":1}}],[\"year\",{\"1\":{\"94\":1}}],[\"your\",{\"1\":{\"86\":1,\"94\":2}}],[\"you\",{\"1\":{\"16\":1,\"94\":1}}],[\"y\",{\"1\":{\"8\":2,\"9\":2,\"10\":2,\"11\":2}}],[\"y=​y1​y2​⋮yn​​​\",{\"1\":{\"5\":1}}],[\"yi​\",{\"1\":{\"5\":1}}],[\"xanax\",{\"1\":{\"94\":1}}],[\"xl\",{\"1\":{\"22\":1}}],[\"x1​​x1​​⋯​xp​​\",{\"1\":{\"5\":1}}],[\"x=\",{\"1\":{\"5\":1}}],[\"x=​x1t​x2t​⋮xnt​​​\",{\"1\":{\"5\":1}}],[\"x=​x11​x21​⋮xn1​​x12​x22​⋮xn2​​⋯⋯⋱⋯​x1p​x1p​⋮xnp​​​\",{\"1\":{\"5\":1}}],[\"xj​=​x1​jx2​j⋮xn​j​​\",{\"1\":{\"5\":1}}],[\"xj​\",{\"1\":{\"5\":1}}],[\"xi​=​xi​1xi​2⋮xi​p​​\",{\"1\":{\"5\":1}}],[\"xi​\",{\"1\":{\"5\":1}}],[\"xi​j\",{\"1\":{\"5\":1}}],[\"x\",{\"1\":{\"5\":3,\"62\":2,\"94\":2,\"95\":4}}],[\"im\",{\"1\":{\"94\":2}}],[\"importerror\",{\"1\":{\"64\":1}}],[\"import\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"11\":1,\"16\":2,\"17\":1,\"28\":2,\"29\":1,\"31\":1,\"32\":1,\"34\":1,\"35\":2,\"37\":1,\"43\":2,\"45\":1,\"50\":4,\"56\":2,\"60\":1,\"61\":1,\"62\":1,\"63\":2,\"64\":3,\"65\":2,\"66\":2,\"68\":1,\"69\":1,\"70\":1,\"71\":1,\"72\":2,\"73\":1,\"74\":5,\"80\":2,\"81\":2,\"84\":1,\"85\":1,\"86\":1,\"91\":1,\"94\":1,\"95\":1}}],[\"irritated\",{\"1\":{\"94\":1}}],[\"if\",{\"1\":{\"62\":1,\"72\":1,\"74\":1,\"94\":1}}],[\"id>\",{\"1\":{\"86\":1}}],[\"id=\",{\"1\":{\"80\":2,\"84\":1}}],[\"id=none\",{\"1\":{\"60\":4}}],[\"idx\",{\"1\":{\"60\":6,\"61\":3,\"62\":2,\"67\":2}}],[\"id\",{\"1\":{\"34\":1,\"40\":4,\"51\":6,\"52\":1,\"55\":1,\"94\":5}}],[\"id2label\",{\"1\":{\"32\":2}}],[\"ids\",{\"0\":{\"46\":1},\"1\":{\"28\":1,\"37\":3,\"43\":5,\"44\":1,\"46\":4,\"48\":2,\"50\":15,\"51\":9,\"52\":8,\"53\":2,\"55\":6,\"57\":1,\"61\":15,\"62\":4,\"65\":3,\"67\":2,\"68\":2}}],[\"is\",{\"1\":{\"28\":1,\"43\":1,\"45\":2,\"47\":1,\"61\":6,\"72\":1,\"74\":1,\"86\":1,\"94\":3}}],[\"its\",{\"1\":{\"94\":1}}],[\"items\",{\"1\":{\"62\":2,\"68\":1,\"72\":1,\"73\":1,\"74\":1}}],[\"it\",{\"1\":{\"23\":1,\"86\":2,\"91\":17,\"92\":4,\"94\":5}}],[\"ith\",{\"1\":{\"5\":1}}],[\"illustrated\",{\"1\":{\"22\":1,\"25\":1}}],[\"i\",{\"1\":{\"16\":3,\"23\":2,\"28\":4,\"50\":4,\"52\":2,\"53\":2,\"55\":3,\"56\":2,\"94\":15,\"95\":2}}],[\"inactive\",{\"1\":{\"94\":1}}],[\"ineffective\",{\"1\":{\"94\":1}}],[\"information\",{\"1\":{\"81\":1}}],[\"inference\",{\"0\":{\"37\":1},\"1\":{\"25\":1,\"56\":1}}],[\"install\",{\"1\":{\"64\":4,\"86\":1}}],[\"indexerror\",{\"1\":{\"50\":1}}],[\"independent\",{\"1\":{\"5\":1}}],[\"ingredients\",{\"1\":{\"94\":1}}],[\"ing\",{\"1\":{\"42\":1}}],[\"intended\",{\"1\":{\"87\":1}}],[\"intermediate\",{\"1\":{\"34\":1}}],[\"int32\",{\"1\":{\"60\":1}}],[\"introduction\",{\"1\":{\"2\":1}}],[\"initializer\",{\"1\":{\"34\":1}}],[\"inputs\",{\"1\":{\"28\":4,\"30\":1,\"31\":1,\"37\":2,\"50\":4,\"52\":8,\"53\":4,\"55\":3,\"61\":3}}],[\"input\",{\"0\":{\"46\":1},\"1\":{\"5\":1,\"28\":1,\"37\":5,\"40\":2,\"43\":3,\"44\":1,\"48\":1,\"50\":7,\"52\":5,\"53\":2,\"55\":2,\"57\":1,\"61\":6,\"62\":3,\"67\":1,\"68\":1}}],[\"in\",{\"1\":{\"5\":1,\"16\":1,\"17\":2,\"50\":1,\"62\":4,\"68\":2,\"72\":3,\"73\":2,\"74\":5,\"84\":1,\"94\":5}}],[\"include\",{\"1\":{\"4\":1}}],[\"urldrugname\",{\"1\":{\"94\":1}}],[\"url\",{\"1\":{\"92\":4}}],[\"ubuntu\",{\"1\":{\"91\":1}}],[\"utils\",{\"1\":{\"68\":1}}],[\"u\",{\"1\":{\"64\":2}}],[\"u`\",{\"1\":{\"64\":1}}],[\"upon\",{\"1\":{\"94\":1}}],[\"upload\",{\"0\":{\"84\":1},\"1\":{\"81\":1,\"84\":3}}],[\"update\",{\"1\":{\"72\":1,\"74\":2,\"81\":1}}],[\"up\",{\"1\":{\"52\":3,\"86\":1}}],[\"using\",{\"1\":{\"43\":1,\"45\":2,\"47\":1,\"64\":1}}],[\"uses\",{\"1\":{\"87\":1}}],[\"username\",{\"1\":{\"86\":1}}],[\"user\",{\"1\":{\"81\":1}}],[\"use\",{\"1\":{\"5\":1,\"34\":1,\"80\":1,\"86\":1,\"87\":1,\"94\":2}}],[\"unescape\",{\"1\":{\"95\":2}}],[\"unable\",{\"1\":{\"94\":1}}],[\"unnamed\",{\"1\":{\"94\":5}}],[\"unique\",{\"1\":{\"94\":2}}],[\"unix\",{\"1\":{\"91\":1}}],[\"unigram\",{\"1\":{\"42\":1}}],[\"unstage\",{\"1\":{\"86\":1}}],[\"unsupervised\",{\"1\":{\"4\":1}}],[\"unk\",{\"1\":{\"40\":1}}],[\"unknown\",{\"1\":{\"40\":4,\"41\":1}}],[\"uncased\",{\"1\":{\"17\":1,\"28\":2,\"29\":2,\"31\":1,\"50\":2,\"56\":1,\"61\":1,\"63\":1,\"66\":1}}],[\"unmasker\",{\"1\":{\"17\":3}}],[\"2443\",{\"1\":{\"43\":1,\"46\":1,\"47\":1}}],[\"29\",{\"1\":{\"34\":1}}],[\"2x2\",{\"1\":{\"31\":1}}],[\"2117\",{\"1\":{\"61\":1}}],[\"2163\",{\"1\":{\"51\":1}}],[\"2166\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":1,\"55\":2}}],[\"2172\",{\"1\":{\"28\":1}}],[\"2878\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":1,\"55\":2}}],[\"2607\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":1,\"55\":2}}],[\"20\",{\"1\":{\"64\":1}}],[\"2034\",{\"1\":{\"61\":1}}],[\"2031\",{\"1\":{\"52\":4,\"53\":2}}],[\"2008\",{\"1\":{\"94\":1}}],[\"2007\",{\"1\":{\"94\":1}}],[\"2006\",{\"1\":{\"94\":1}}],[\"2000\",{\"1\":{\"94\":1}}],[\"2003\",{\"1\":{\"61\":2}}],[\"200\",{\"1\":{\"51\":20,\"52\":5}}],[\"2005\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":2,\"55\":2}}],[\"2061\",{\"1\":{\"28\":1,\"52\":4,\"53\":2}}],[\"2028\",{\"1\":{\"61\":1}}],[\"2023\",{\"1\":{\"28\":1,\"61\":2}}],[\"2026\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":1,\"55\":2}}],[\"2042\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":2,\"55\":2}}],[\"2017\",{\"1\":{\"18\":1}}],[\"2310\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":2,\"55\":2}}],[\"2\",{\"0\":{\"5\":1,\"9\":1,\"15\":1,\"16\":1,\"17\":2,\"20\":1,\"24\":1,\"26\":1,\"29\":1,\"30\":1,\"31\":2,\"33\":1,\"34\":1,\"35\":1,\"36\":2,\"37\":2,\"41\":1,\"43\":1,\"46\":1,\"51\":1,\"56\":1,\"61\":1,\"63\":1,\"64\":1,\"65\":2,\"69\":1,\"72\":1,\"78\":1,\"79\":1,\"80\":1,\"81\":2,\"82\":1,\"83\":2,\"84\":1,\"85\":2,\"86\":1,\"92\":1,\"93\":1,\"94\":1,\"95\":2,\"96\":1},\"1\":{\"16\":1,\"22\":1,\"28\":2,\"29\":2,\"30\":1,\"31\":3,\"34\":1,\"37\":1,\"42\":1,\"50\":4,\"56\":1,\"59\":1,\"61\":1,\"65\":1,\"66\":1,\"69\":1,\"94\":1}}],[\"pea\",{\"1\":{\"94\":1}}],[\"peel\",{\"1\":{\"94\":1}}],[\"persistent\",{\"1\":{\"94\":1}}],[\"person\",{\"1\":{\"94\":1}}],[\"per\",{\"1\":{\"94\":2}}],[\"performing\",{\"1\":{\"4\":1}}],[\"psychiatrist\",{\"1\":{\"94\":1}}],[\"pkl\",{\"1\":{\"90\":1}}],[\"pickled\",{\"1\":{\"90\":1}}],[\"pip\",{\"1\":{\"64\":2}}],[\"pipeline\",{\"0\":{\"27\":1},\"1\":{\"16\":6,\"17\":2,\"25\":1,\"26\":1,\"27\":1,\"28\":3,\"57\":1}}],[\"py\",{\"1\":{\"74\":2}}],[\"python\",{\"0\":{\"81\":1},\"1\":{\"36\":2,\"78\":1,\"94\":1,\"95\":2}}],[\"pytorch\",{\"0\":{\"66\":1},\"1\":{\"28\":1,\"36\":1,\"50\":1,\"62\":1,\"67\":1,\"86\":5}}],[\"please\",{\"1\":{\"64\":1}}],[\"pull\",{\"1\":{\"85\":2}}],[\"pushed\",{\"1\":{\"86\":1}}],[\"push\",{\"0\":{\"80\":1},\"1\":{\"64\":1,\"78\":1,\"80\":9,\"85\":2,\"86\":2}}],[\"puppeteer\",{\"1\":{\"40\":2}}],[\"patient\",{\"1\":{\"94\":2}}],[\"path\",{\"1\":{\"84\":1}}],[\"pain\",{\"1\":{\"94\":2}}],[\"panic\",{\"1\":{\"94\":2}}],[\"pandas\",{\"1\":{\"89\":1,\"90\":2,\"94\":1}}],[\"paragraphs\",{\"1\":{\"91\":3}}],[\"parameters\",{\"1\":{\"70\":1,\"74\":2}}],[\"paraphrase\",{\"1\":{\"59\":1}}],[\"pad\",{\"1\":{\"34\":1,\"51\":3,\"52\":4,\"61\":1}}],[\"padding=\",{\"1\":{\"52\":3,\"74\":1}}],[\"padding=true\",{\"1\":{\"28\":1,\"50\":3,\"56\":1,\"61\":1}}],[\"padding\",{\"0\":{\"51\":1,\"62\":1},\"1\":{\"28\":1,\"51\":7,\"52\":1,\"54\":1,\"56\":1,\"61\":4,\"62\":4}}],[\"pt\",{\"1\":{\"28\":2,\"50\":2,\"56\":1}}],[\"post\",{\"1\":{\"84\":1}}],[\"position\",{\"1\":{\"34\":2}}],[\"positive\",{\"1\":{\"7\":2,\"11\":1,\"16\":2,\"31\":1,\"32\":3}}],[\"points\",{\"1\":{\"5\":1}}],[\"primary\",{\"1\":{\"94\":1}}],[\"private\",{\"1\":{\"81\":1}}],[\"print\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"11\":1,\"17\":2,\"28\":1,\"30\":1,\"31\":1,\"32\":2,\"40\":1,\"45\":1,\"46\":1,\"47\":1,\"50\":3,\"51\":3,\"52\":5,\"53\":2,\"55\":4,\"61\":2,\"65\":1,\"69\":1,\"71\":1,\"95\":1}}],[\"principal\",{\"1\":{\"4\":2}}],[\"promise\",{\"1\":{\"94\":1}}],[\"properly\",{\"1\":{\"94\":1}}],[\"progress\",{\"1\":{\"72\":2,\"74\":4}}],[\"procedure\",{\"1\":{\"87\":1}}],[\"proc\",{\"1\":{\"61\":1}}],[\"prob\",{\"1\":{\"34\":2}}],[\"probs\",{\"1\":{\"34\":1}}],[\"prostitute\",{\"1\":{\"17\":1}}],[\"previously\",{\"1\":{\"94\":1}}],[\"prepare\",{\"1\":{\"74\":2}}],[\"pretrained\",{\"1\":{\"28\":2,\"29\":2,\"31\":1,\"35\":1,\"36\":2,\"43\":5,\"44\":1,\"45\":1,\"50\":4,\"51\":1,\"56\":2,\"61\":1,\"63\":1,\"64\":1,\"65\":1,\"66\":1,\"69\":1,\"74\":2,\"80\":2,\"85\":2,\"86\":4}}],[\"pretraining\",{\"1\":{\"21\":1}}],[\"preds\",{\"1\":{\"65\":3}}],[\"pred\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"11\":1}}],[\"prediction\",{\"1\":{\"73\":1}}],[\"predictions=predictions\",{\"1\":{\"65\":1,\"73\":1}}],[\"predictions=preds\",{\"1\":{\"65\":1}}],[\"predictions\",{\"1\":{\"5\":1,\"32\":2,\"65\":10,\"73\":1}}],[\"predict\",{\"1\":{\"65\":3}}],[\"predictors\",{\"1\":{\"5\":1}}],[\"precision=tp+fptp​\",{\"1\":{\"9\":1}}],[\"precision\",{\"0\":{\"9\":1},\"1\":{\"7\":1,\"9\":3,\"11\":5,\"65\":1}}],[\"p\",{\"1\":{\"5\":2}}],[\"full\",{\"1\":{\"88\":1}}],[\"function\",{\"1\":{\"61\":2,\"62\":2,\"63\":2,\"66\":2,\"74\":2,\"94\":1}}],[\"functional\",{\"1\":{\"32\":1}}],[\"fr\",{\"1\":{\"88\":1}}],[\"from=\",{\"1\":{\"85\":1}}],[\"from\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"11\":1,\"16\":2,\"17\":1,\"28\":4,\"29\":3,\"31\":2,\"34\":1,\"35\":5,\"43\":5,\"44\":1,\"45\":2,\"50\":6,\"51\":1,\"56\":3,\"60\":1,\"61\":2,\"62\":1,\"63\":3,\"64\":4,\"65\":1,\"66\":3,\"68\":1,\"69\":2,\"70\":1,\"71\":1,\"72\":1,\"74\":7,\"80\":4,\"81\":2,\"84\":1,\"85\":1,\"86\":3,\"91\":1,\"94\":5}}],[\"f0f7783\",{\"1\":{\"86\":1}}],[\"folder>\",{\"1\":{\"85\":5,\"86\":2}}],[\"fortokenclassification\",{\"1\":{\"29\":1}}],[\"forsequenceclassification\",{\"1\":{\"29\":1}}],[\"forquestionanswering\",{\"1\":{\"29\":1}}],[\"format\",{\"1\":{\"67\":1,\"90\":1}}],[\"formaskedlm\",{\"1\":{\"29\":1}}],[\"formultiplechoice\",{\"1\":{\"29\":1}}],[\"forcausallm\",{\"1\":{\"29\":1}}],[\"for\",{\"1\":{\"4\":1,\"5\":2,\"16\":2,\"17\":2,\"28\":2,\"50\":3,\"52\":2,\"53\":2,\"55\":3,\"56\":1,\"62\":3,\"68\":2,\"72\":3,\"73\":2,\"74\":5,\"86\":1,\"87\":1,\"94\":6}}],[\"forests\",{\"1\":{\"4\":1}}],[\"feet\",{\"1\":{\"94\":2}}],[\"feel\",{\"1\":{\"94\":2}}],[\"feels\",{\"1\":{\"23\":2}}],[\"felt\",{\"1\":{\"94\":1}}],[\"features\",{\"0\":{\"30\":1},\"1\":{\"5\":1,\"60\":5,\"61\":3,\"91\":3}}],[\"finger\",{\"1\":{\"94\":1}}],[\"finetuned\",{\"1\":{\"28\":2,\"29\":2,\"31\":1,\"50\":2,\"56\":1,\"80\":2}}],[\"fine\",{\"1\":{\"19\":1,\"21\":1,\"58\":1,\"86\":1,\"89\":1}}],[\"field=\",{\"1\":{\"91\":3,\"92\":1}}],[\"filter\",{\"1\":{\"94\":2,\"95\":2}}],[\"file>\",{\"1\":{\"84\":1}}],[\"file\",{\"0\":{\"84\":1},\"1\":{\"81\":2,\"83\":1,\"84\":3,\"85\":1,\"86\":6,\"90\":3}}],[\"files=data\",{\"1\":{\"91\":2,\"92\":1,\"94\":1}}],[\"files=\",{\"1\":{\"90\":4,\"91\":2}}],[\"files\",{\"0\":{\"83\":1},\"1\":{\"81\":1,\"83\":1,\"85\":3,\"90\":1,\"91\":5,\"92\":3,\"94\":2}}],[\"fill\",{\"1\":{\"17\":1}}],[\"first\",{\"1\":{\"61\":3,\"86\":1}}],[\"fn=data\",{\"1\":{\"68\":2}}],[\"fn=<nlllossbackward0>\",{\"1\":{\"69\":1}}],[\"fn=<addmmbackward0>\",{\"1\":{\"50\":1,\"51\":3,\"52\":1}}],[\"fn=<addmmbackward>\",{\"1\":{\"32\":1}}],[\"fn=<softmaxbackward0>\",{\"1\":{\"32\":1}}],[\"fn\",{\"1\":{\"7\":1,\"11\":4}}],[\"fp16=true\",{\"1\":{\"65\":1}}],[\"fp\",{\"1\":{\"7\":1,\"11\":5}}],[\"fail\",{\"1\":{\"50\":1}}],[\"false\",{\"1\":{\"7\":2,\"11\":2}}],[\"face\",{\"0\":{\"90\":1},\"1\":{\"1\":1,\"76\":1,\"80\":1,\"83\":1,\"88\":1,\"89\":1,\"94\":2}}],[\"f1−score=precision+recall2×precision×recall​\",{\"1\":{\"11\":1}}],[\"f1\",{\"0\":{\"11\":2},\"1\":{\"7\":1,\"11\":3,\"65\":2,\"73\":1}}],[\"blood\",{\"1\":{\"94\":1}}],[\"blanket\",{\"1\":{\"23\":1}}],[\"bc20ff2\",{\"1\":{\"86\":1}}],[\"bias\",{\"1\":{\"77\":1,\"87\":1}}],[\"bin\",{\"1\":{\"36\":3,\"86\":5}}],[\"branch\",{\"1\":{\"86\":3}}],[\"break\",{\"1\":{\"68\":1}}],[\"brother\",{\"1\":{\"60\":2}}],[\"but\",{\"1\":{\"50\":1,\"94\":1}}],[\"bpe\",{\"1\":{\"42\":1,\"86\":4}}],[\"byte\",{\"1\":{\"42\":1}}],[\"became\",{\"1\":{\"94\":1}}],[\"better\",{\"1\":{\"94\":1}}],[\"benefit\",{\"1\":{\"95\":4}}],[\"benefitsreview\",{\"1\":{\"94\":1,\"95\":3}}],[\"benchmark\",{\"1\":{\"59\":2}}],[\"be\",{\"1\":{\"50\":1,\"86\":3,\"94\":3}}],[\"been\",{\"1\":{\"16\":2,\"28\":2,\"50\":3,\"52\":1,\"53\":1,\"55\":3,\"56\":1}}],[\"berttokenizer\",{\"1\":{\"43\":2}}],[\"bertmodel\",{\"1\":{\"34\":2,\"35\":3}}],[\"bertconfig\",{\"1\":{\"34\":3}}],[\"bert\",{\"1\":{\"14\":5,\"17\":1,\"18\":1,\"19\":1,\"22\":1,\"33\":1,\"34\":2,\"35\":3,\"42\":1,\"43\":3,\"45\":1,\"52\":1,\"53\":1,\"61\":1,\"63\":1,\"64\":1,\"66\":1,\"80\":2,\"95\":2}}],[\"bad\",{\"1\":{\"94\":2}}],[\"back\",{\"1\":{\"94\":1}}],[\"backward\",{\"1\":{\"72\":1,\"74\":3}}],[\"bached\",{\"1\":{\"51\":1}}],[\"bar\",{\"1\":{\"72\":2,\"74\":4}}],[\"bart\",{\"1\":{\"18\":1,\"19\":1,\"22\":1}}],[\"batches\",{\"1\":{\"62\":1,\"71\":1,\"73\":1}}],[\"batched=true\",{\"1\":{\"61\":1,\"63\":1,\"66\":1}}],[\"batched\",{\"1\":{\"50\":1,\"51\":4,\"52\":2,\"61\":1,\"96\":1}}],[\"batching\",{\"1\":{\"50\":1}}],[\"batch\",{\"1\":{\"30\":1,\"51\":1,\"61\":4,\"62\":6,\"68\":4,\"69\":2,\"72\":4,\"73\":7,\"74\":6,\"96\":2}}],[\"base\",{\"1\":{\"14\":2,\"17\":1,\"28\":2,\"29\":2,\"31\":1,\"35\":2,\"43\":2,\"50\":2,\"56\":1,\"61\":1,\"63\":1,\"66\":1,\"77\":1,\"80\":1,\"86\":1,\"88\":1}}],[\"based\",{\"0\":{\"40\":1,\"41\":1,\"86\":1},\"1\":{\"4\":1,\"40\":2,\"41\":3,\"45\":1}}],[\"bagging\",{\"1\":{\"4\":1}}],[\"body\",{\"1\":{\"94\":1}}],[\"boys\",{\"1\":{\"94\":1}}],[\"boosting\",{\"1\":{\"4\":1}}],[\"bootstrap\",{\"1\":{\"4\":1}}],[\"both\",{\"1\":{\"4\":2}}],[\"runtime\",{\"1\":{\"64\":1}}],[\"running\",{\"1\":{\"40\":1}}],[\"run\",{\"1\":{\"40\":1,\"64\":1}}],[\"rating\",{\"1\":{\"94\":1}}],[\"rate\",{\"0\":{\"71\":1},\"1\":{\"71\":1}}],[\"ram\",{\"1\":{\"61\":2,\"89\":1}}],[\"range\",{\"1\":{\"34\":1,\"50\":2,\"72\":2,\"74\":4,\"94\":2}}],[\"random\",{\"1\":{\"4\":1}}],[\"raw\",{\"1\":{\"28\":2,\"60\":7,\"61\":5,\"63\":2,\"66\":2,\"91\":2,\"92\":1,\"94\":2}}],[\"roberta\",{\"1\":{\"22\":1}}],[\"rows\",{\"1\":{\"5\":1,\"60\":3,\"61\":3,\"91\":3,\"95\":1}}],[\"r\",{\"1\":{\"17\":4,\"94\":13}}],[\"recently\",{\"1\":{\"94\":1}}],[\"recall和f1\",{\"1\":{\"11\":1}}],[\"recall=tp+fntp​\",{\"1\":{\"10\":1}}],[\"recall\",{\"0\":{\"10\":1},\"1\":{\"7\":1,\"10\":3,\"11\":5}}],[\"red\",{\"1\":{\"94\":1}}],[\"reasons\",{\"1\":{\"94\":2}}],[\"really\",{\"1\":{\"94\":2}}],[\"real\",{\"1\":{\"94\":1}}],[\"readme\",{\"1\":{\"86\":1,\"87\":1}}],[\"review\",{\"1\":{\"94\":2,\"95\":10}}],[\"reporting\",{\"1\":{\"87\":1}}],[\"repo=\",{\"1\":{\"84\":1}}],[\"repository\",{\"0\":{\"85\":1},\"1\":{\"81\":1,\"85\":3}}],[\"repo\",{\"1\":{\"74\":1,\"80\":1,\"81\":8,\"84\":2,\"85\":10}}],[\"regular\",{\"1\":{\"83\":1}}],[\"regularization\",{\"1\":{\"70\":1}}],[\"regressive\",{\"1\":{\"18\":1,\"22\":1}}],[\"regression\",{\"1\":{\"4\":6}}],[\"rename\",{\"1\":{\"67\":1,\"94\":2}}],[\"remove\",{\"1\":{\"67\":1}}],[\"references=batch\",{\"1\":{\"73\":1}}],[\"references=labels\",{\"1\":{\"65\":1}}],[\"references=predictions\",{\"1\":{\"65\":1}}],[\"referring\",{\"1\":{\"60\":1}}],[\"requires\",{\"1\":{\"64\":1}}],[\"retrieve\",{\"1\":{\"29\":1,\"81\":1}}],[\"returning\",{\"1\":{\"94\":1}}],[\"returns\",{\"1\":{\"50\":3}}],[\"return\",{\"1\":{\"16\":1,\"28\":2,\"50\":5,\"54\":1,\"56\":1,\"61\":1,\"63\":1,\"65\":1,\"66\":1,\"94\":1,\"95\":1}}],[\"restore\",{\"1\":{\"86\":1}}],[\"restart\",{\"1\":{\"64\":1}}],[\"research\",{\"1\":{\"59\":1}}],[\"res\",{\"1\":{\"40\":1}}],[\"results\",{\"1\":{\"87\":1}}],[\"result\",{\"1\":{\"17\":4,\"43\":1,\"45\":1,\"46\":1,\"47\":1,\"50\":1}}],[\"response\",{\"1\":{\"5\":1}}],[\"r×s\",{\"1\":{\"5\":1}}],[\"ridge\",{\"1\":{\"4\":1}}],[\"keys\",{\"1\":{\"94\":1}}],[\"k\",{\"1\":{\"4\":2,\"5\":1,\"62\":5,\"68\":2,\"72\":2,\"73\":2,\"74\":2}}],[\"visibility\",{\"1\":{\"81\":1}}],[\"vocabulary\",{\"1\":{\"44\":1}}],[\"vocab\",{\"1\":{\"34\":2}}],[\"very\",{\"1\":{\"94\":1}}],[\"verify\",{\"1\":{\"94\":1}}],[\"version\",{\"1\":{\"34\":1,\"86\":1}}],[\"ve\",{\"1\":{\"16\":2,\"28\":2,\"50\":3,\"52\":1,\"53\":1,\"55\":3,\"56\":1}}],[\"vector\",{\"1\":{\"4\":2,\"5\":2}}],[\"vs\",{\"0\":{\"14\":1}}],[\"v\",{\"1\":{\"11\":1,\"62\":4,\"68\":2,\"72\":2,\"73\":2,\"74\":2}}],[\"variable\",{\"1\":{\"5\":2,\"87\":1}}],[\"variables\",{\"1\":{\"5\":3}}],[\"validation\",{\"1\":{\"4\":1,\"60\":2,\"61\":1,\"64\":2,\"65\":3,\"68\":1}}],[\"value\",{\"1\":{\"4\":1,\"5\":1,\"60\":3}}],[\"values\",{\"1\":{\"4\":2}}],[\"广义加性模型\",{\"1\":{\"4\":1}}],[\"over\",{\"1\":{\"94\":1}}],[\"old\",{\"1\":{\"94\":1}}],[\"ocd\",{\"1\":{\"94\":2}}],[\"oscar\",{\"1\":{\"88\":1}}],[\"object\",{\"1\":{\"94\":1}}],[\"objects\",{\"1\":{\"86\":3}}],[\"observation\",{\"1\":{\"5\":1}}],[\"observations\",{\"1\":{\"5\":1}}],[\"optimizer=optimizer\",{\"1\":{\"71\":1,\"74\":2}}],[\"optimizer\",{\"0\":{\"70\":1},\"1\":{\"70\":2,\"72\":2,\"74\":10}}],[\"out\",{\"1\":{\"50\":1}}],[\"outputs\",{\"1\":{\"30\":2,\"31\":2,\"32\":2,\"69\":3,\"72\":2,\"73\":2,\"74\":4}}],[\"output\",{\"1\":{\"5\":1,\"37\":1,\"50\":2,\"56\":1}}],[\"once\",{\"1\":{\"94\":1}}],[\"one\",{\"1\":{\"61\":3}}],[\"on\",{\"1\":{\"36\":2,\"43\":1,\"86\":2,\"94\":4}}],[\"only\",{\"1\":{\"22\":2,\"60\":1}}],[\"others\",{\"1\":{\"29\":1}}],[\"of\",{\"1\":{\"4\":2,\"5\":7,\"50\":2,\"60\":2,\"94\":3}}],[\"organization=\",{\"1\":{\"80\":2,\"81\":1}}],[\"organization\",{\"1\":{\"80\":1,\"81\":2}}],[\"original\",{\"1\":{\"94\":1}}],[\"origin\",{\"1\":{\"51\":1,\"86\":2}}],[\"or\",{\"1\":{\"4\":2,\"42\":1,\"52\":1,\"53\":1,\"64\":1,\"94\":1}}],[\"eye\",{\"1\":{\"94\":1}}],[\"eat\",{\"1\":{\"94\":1}}],[\"eating\",{\"1\":{\"23\":2}}],[\"effects\",{\"1\":{\"94\":3}}],[\"effective\",{\"1\":{\"94\":3}}],[\"effectiveness\",{\"1\":{\"94\":2}}],[\"elevate\",{\"1\":{\"94\":1}}],[\"electra\",{\"1\":{\"22\":1}}],[\"else\",{\"1\":{\"72\":1,\"74\":1}}],[\"epochs\",{\"1\":{\"71\":2,\"72\":1,\"74\":6}}],[\"epoch\",{\"1\":{\"64\":2,\"65\":3,\"71\":1,\"72\":1,\"74\":2,\"80\":2}}],[\"eps\",{\"1\":{\"34\":1}}],[\"evaluate\",{\"1\":{\"65\":5,\"73\":3}}],[\"evaluation\",{\"0\":{\"65\":1,\"73\":1},\"1\":{\"64\":1,\"65\":1,\"87\":1}}],[\"eval\",{\"1\":{\"64\":1,\"65\":3,\"68\":1,\"73\":2,\"74\":4}}],[\"evidence\",{\"1\":{\"60\":2}}],[\"excluding\",{\"1\":{\"94\":1}}],[\"extremley\",{\"1\":{\"94\":1}}],[\"extremely\",{\"1\":{\"94\":1}}],[\"exfoliated\",{\"1\":{\"94\":1}}],[\"example\",{\"1\":{\"61\":3,\"63\":3,\"66\":3,\"90\":1,\"94\":2,\"95\":2}}],[\"expected\",{\"1\":{\"50\":1,\"94\":1}}],[\"equivalent\",{\"1\":{\"60\":4}}],[\"er\",{\"1\":{\"45\":1}}],[\"error\",{\"1\":{\"4\":1}}],[\"embeddings\",{\"1\":{\"34\":1}}],[\"embedding\",{\"1\":{\"29\":1,\"34\":1}}],[\"endo\",{\"1\":{\"94\":1}}],[\"english\",{\"1\":{\"28\":2,\"29\":2,\"31\":1,\"50\":2,\"56\":1}}],[\"encoded\",{\"1\":{\"37\":2}}],[\"encode\",{\"1\":{\"22\":1}}],[\"encoder\",{\"1\":{\"22\":2,\"24\":4}}],[\"encoders\",{\"1\":{\"22\":1}}],[\"encoding\",{\"0\":{\"44\":1},\"1\":{\"18\":1,\"22\":1,\"44\":3,\"47\":1}}],[\"entire\",{\"1\":{\"4\":1}}],[\"mcg\",{\"1\":{\"94\":1}}],[\"moisturizer\",{\"1\":{\"94\":1}}],[\"morning\",{\"1\":{\"94\":1}}],[\"more\",{\"1\":{\"94\":2}}],[\"module\",{\"1\":{\"95\":1}}],[\"modules\",{\"1\":{\"35\":1}}],[\"moderate\",{\"1\":{\"94\":1}}],[\"moderately\",{\"1\":{\"94\":2}}],[\"model在\",{\"1\":{\"72\":1}}],[\"model=\",{\"1\":{\"16\":1,\"17\":1}}],[\"models\",{\"0\":{\"12\":1,\"33\":1,\"76\":1},\"1\":{\"4\":2,\"14\":1,\"22\":3,\"59\":1,\"80\":1,\"81\":1}}],[\"model\",{\"0\":{\"19\":1,\"29\":1,\"30\":1,\"31\":1,\"69\":1,\"83\":1,\"87\":1,\"88\":1},\"1\":{\"4\":1,\"14\":3,\"16\":4,\"25\":1,\"26\":3,\"29\":2,\"30\":2,\"31\":3,\"32\":3,\"33\":3,\"34\":2,\"35\":6,\"36\":4,\"37\":7,\"43\":2,\"50\":7,\"51\":4,\"52\":11,\"53\":5,\"55\":3,\"56\":3,\"57\":2,\"64\":4,\"65\":2,\"67\":1,\"69\":3,\"70\":1,\"72\":3,\"73\":2,\"74\":13,\"75\":1,\"77\":1,\"80\":19,\"81\":3,\"84\":1,\"85\":6,\"86\":22,\"87\":6,\"88\":4}}],[\"modified\",{\"1\":{\"86\":1}}],[\"m\",{\"1\":{\"86\":1,\"94\":1,\"95\":2}}],[\"md\",{\"1\":{\"86\":1,\"87\":1}}],[\"minutes\",{\"1\":{\"94\":2}}],[\"mit\",{\"1\":{\"88\":1}}],[\"mixed\",{\"1\":{\"65\":1}}],[\"microsoft\",{\"1\":{\"59\":1}}],[\"ml\",{\"1\":{\"59\":1}}],[\"mrpc\",{\"1\":{\"59\":2,\"60\":1,\"63\":1,\"65\":4,\"66\":1,\"73\":1,\"80\":2}}],[\"mbart\",{\"1\":{\"22\":1}}],[\"must\",{\"1\":{\"94\":1}}],[\"mu\",{\"1\":{\"80\":1}}],[\"much\",{\"1\":{\"16\":1,\"28\":2,\"94\":3}}],[\"multiple\",{\"1\":{\"4\":1}}],[\"my\",{\"1\":{\"16\":2,\"28\":2,\"36\":2,\"43\":1,\"50\":3,\"52\":1,\"53\":1,\"55\":3,\"56\":1,\"80\":1,\"90\":4,\"94\":15}}],[\"made\",{\"1\":{\"94\":1}}],[\"master\",{\"1\":{\"91\":2,\"92\":1}}],[\"mask=torch\",{\"1\":{\"52\":1}}],[\"masks\",{\"0\":{\"52\":1},\"1\":{\"57\":1}}],[\"mask\",{\"1\":{\"17\":3,\"24\":1,\"28\":1,\"43\":2,\"51\":1,\"52\":8,\"53\":2,\"61\":5,\"62\":1,\"67\":1,\"68\":1}}],[\"main\",{\"1\":{\"86\":4}}],[\"maid\",{\"1\":{\"17\":1}}],[\"mapping\",{\"1\":{\"89\":1}}],[\"map\",{\"0\":{\"96\":1},\"1\":{\"61\":4,\"63\":1,\"66\":1,\"86\":3,\"89\":1,\"94\":3,\"95\":2,\"96\":2}}],[\"marian\",{\"1\":{\"22\":1}}],[\"management\",{\"1\":{\"81\":2}}],[\"man\",{\"1\":{\"17\":1}}],[\"maximum\",{\"1\":{\"52\":1}}],[\"max\",{\"1\":{\"16\":1,\"34\":1,\"52\":5,\"53\":4,\"74\":1}}],[\"matrix\",{\"1\":{\"5\":1,\"7\":1}}],[\"making\",{\"1\":{\"5\":1}}],[\"machines\",{\"1\":{\"4\":1}}],[\"machine\",{\"1\":{\"4\":1}}],[\"me\",{\"1\":{\"94\":2}}],[\"medication\",{\"1\":{\"94\":1}}],[\"memory\",{\"1\":{\"89\":1}}],[\"mechanic\",{\"1\":{\"17\":1}}],[\"metadata\",{\"0\":{\"88\":1},\"1\":{\"36\":1}}],[\"metrics=compute\",{\"1\":{\"65\":1}}],[\"metrics\",{\"1\":{\"64\":1,\"65\":16,\"73\":1,\"81\":1,\"87\":1,\"89\":1}}],[\"metric\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"11\":1,\"65\":5,\"73\":6}}],[\"methods\",{\"1\":{\"4\":2,\"81\":1}}],[\"means\",{\"1\":{\"4\":1}}],[\"mean\",{\"1\":{\"4\":1}}],[\"qualitative\",{\"1\":{\"4\":1}}],[\"quantitative\",{\"1\":{\"4\":1}}],[\"celiacs\",{\"1\":{\"94\":1}}],[\"celexa\",{\"1\":{\"94\":3}}],[\"cell\",{\"1\":{\"74\":1}}],[\"cymbalta\",{\"1\":{\"94\":2}}],[\"csv\",{\"1\":{\"90\":4,\"94\":5}}],[\"cb23931\",{\"1\":{\"86\":1}}],[\"cd\",{\"1\":{\"86\":1}}],[\"cream\",{\"1\":{\"94\":1}}],[\"create\",{\"1\":{\"81\":4}}],[\"creation\",{\"1\":{\"81\":1}}],[\"crux82\",{\"1\":{\"91\":2,\"92\":1}}],[\"cross\",{\"1\":{\"4\":1}}],[\"cpu\",{\"1\":{\"72\":1,\"74\":1}}],[\"cuda\",{\"1\":{\"72\":2,\"74\":2}}],[\"checked\",{\"1\":{\"94\":1}}],[\"checkpoint\",{\"1\":{\"28\":2,\"29\":2,\"31\":2,\"33\":2,\"36\":2,\"50\":6,\"51\":1,\"56\":3,\"61\":2,\"63\":2,\"64\":1,\"65\":1,\"66\":2,\"69\":1,\"74\":2,\"80\":3,\"86\":3}}],[\"checkpoints\",{\"0\":{\"14\":1},\"1\":{\"14\":4,\"28\":2,\"29\":1,\"61\":2,\"64\":1,\"77\":1}}],[\"changes\",{\"1\":{\"86\":1}}],[\"change\",{\"1\":{\"81\":1}}],[\"chapter\",{\"1\":{\"43\":1,\"64\":1}}],[\"characters\",{\"1\":{\"41\":1}}],[\"character\",{\"0\":{\"41\":1},\"1\":{\"40\":1,\"41\":4}}],[\"ctrl\",{\"1\":{\"22\":1}}],[\"cannot\",{\"1\":{\"94\":1}}],[\"cancer\",{\"1\":{\"94\":1}}],[\"caseine\",{\"1\":{\"94\":3}}],[\"cased\",{\"1\":{\"14\":2,\"35\":2,\"43\":2,\"45\":1}}],[\"camembert\",{\"1\":{\"77\":1,\"80\":1,\"86\":1,\"88\":1}}],[\"calculation\",{\"1\":{\"65\":1}}],[\"called\",{\"1\":{\"60\":1,\"95\":2}}],[\"cache\",{\"1\":{\"34\":1,\"35\":1,\"60\":1}}],[\"cards\",{\"1\":{\"87\":1}}],[\"card\",{\"0\":{\"87\":1,\"88\":1},\"1\":{\"25\":1,\"35\":1,\"77\":1,\"80\":1,\"87\":3,\"88\":3}}],[\"carpenter\",{\"1\":{\"17\":1}}],[\"categorical\",{\"1\":{\"4\":1}}],[\"clone\",{\"1\":{\"85\":3,\"86\":2}}],[\"cli\",{\"1\":{\"81\":2}}],[\"cls\",{\"1\":{\"55\":2,\"61\":3}}],[\"clustering\",{\"1\":{\"4\":2}}],[\"classlabel\",{\"1\":{\"60\":1}}],[\"classifier\",{\"1\":{\"4\":1,\"16\":3,\"28\":2,\"34\":1}}],[\"classification\",{\"1\":{\"4\":2,\"31\":1,\"64\":1}}],[\"class\",{\"1\":{\"4\":1,\"29\":1}}],[\"couldnt\",{\"1\":{\"94\":1}}],[\"course\",{\"1\":{\"1\":1,\"16\":3,\"28\":2,\"50\":3,\"52\":1,\"53\":1,\"55\":3,\"56\":1}}],[\"costed\",{\"1\":{\"94\":1}}],[\"cost\",{\"1\":{\"94\":2}}],[\"co\",{\"1\":{\"86\":2}}],[\"commentsreview\",{\"1\":{\"94\":1}}],[\"committed\",{\"1\":{\"86\":2}}],[\"commit\",{\"1\":{\"85\":2,\"86\":3}}],[\"com\",{\"1\":{\"91\":2,\"92\":1}}],[\"company\",{\"1\":{\"94\":1}}],[\"compute\",{\"1\":{\"64\":1,\"65\":11,\"73\":3,\"95\":2}}],[\"computer\",{\"1\":{\"36\":2,\"43\":1}}],[\"components\",{\"1\":{\"4\":2}}],[\"colab\",{\"1\":{\"64\":1}}],[\"collator=data\",{\"1\":{\"64\":1,\"65\":1}}],[\"collator\",{\"1\":{\"62\":3,\"63\":1,\"64\":4,\"65\":1,\"66\":1,\"68\":2,\"75\":1}}],[\"collate\",{\"1\":{\"62\":2,\"68\":2}}],[\"columns\",{\"1\":{\"67\":1}}],[\"column\",{\"1\":{\"5\":1,\"67\":2,\"94\":4,\"95\":2}}],[\"corpus\",{\"1\":{\"59\":1}}],[\"cool\",{\"1\":{\"37\":1}}],[\"condition\",{\"1\":{\"94\":9}}],[\"content\",{\"1\":{\"81\":1}}],[\"continuous\",{\"1\":{\"4\":1}}],[\"convert\",{\"1\":{\"46\":2,\"50\":2,\"55\":1,\"61\":1}}],[\"config\",{\"1\":{\"32\":2,\"34\":3,\"36\":3,\"74\":1,\"84\":2,\"86\":6}}],[\"configuration\",{\"1\":{\"26\":1,\"80\":1}}],[\"confusion\",{\"1\":{\"7\":1}}],[\"线性判别分析\",{\"1\":{\"4\":1}}],[\"dperessed\",{\"1\":{\"94\":1}}],[\"daily\",{\"1\":{\"94\":2}}],[\"day\",{\"1\":{\"94\":2}}],[\"date\",{\"1\":{\"86\":1}}],[\"dataframe\",{\"1\":{\"90\":1}}],[\"dataframes\",{\"1\":{\"90\":1}}],[\"datacollatorwithpadding\",{\"1\":{\"62\":3,\"63\":2,\"64\":1,\"66\":2}}],[\"dataloader\",{\"0\":{\"68\":1},\"1\":{\"62\":1,\"68\":6,\"69\":1,\"71\":2,\"72\":1,\"73\":1,\"74\":8}}],[\"dataset=tokenized\",{\"1\":{\"64\":2,\"65\":2}}],[\"datasetdict\",{\"1\":{\"60\":2,\"61\":1,\"91\":4,\"94\":2}}],[\"dataset\",{\"1\":{\"60\":9,\"61\":6,\"63\":4,\"66\":2,\"67\":1,\"81\":2,\"89\":1,\"90\":4,\"91\":14,\"92\":2,\"94\":21,\"95\":10,\"96\":1}}],[\"datasets\",{\"0\":{\"89\":1},\"1\":{\"1\":1,\"59\":1,\"60\":6,\"61\":8,\"62\":1,\"63\":2,\"64\":3,\"65\":3,\"66\":4,\"67\":8,\"68\":2,\"75\":1,\"81\":1,\"88\":1,\"89\":2,\"90\":1,\"91\":2,\"94\":2,\"95\":1}}],[\"data\",{\"1\":{\"5\":1,\"62\":3,\"63\":1,\"64\":4,\"65\":1,\"66\":1,\"68\":1,\"87\":1,\"90\":5,\"91\":10,\"92\":4,\"94\":2}}],[\"due\",{\"1\":{\"94\":5}}],[\"dummy\",{\"1\":{\"80\":5,\"81\":2,\"84\":1,\"85\":6,\"86\":7}}],[\"dry\",{\"1\":{\"94\":2}}],[\"druglibtest\",{\"1\":{\"94\":1}}],[\"druglibtrain\",{\"1\":{\"94\":1}}],[\"drug\",{\"1\":{\"94\":13,\"95\":8}}],[\"dropout\",{\"1\":{\"34\":3}}],[\"dkv\",{\"1\":{\"91\":1}}],[\"dl\",{\"1\":{\"74\":4}}],[\"dynamic\",{\"0\":{\"62\":1},\"1\":{\"61\":1}}],[\"dtype=\",{\"1\":{\"60\":3}}],[\"doubled\",{\"1\":{\"94\":1}}],[\"done\",{\"1\":{\"94\":1}}],[\"do\",{\"1\":{\"86\":1}}],[\"dogs\",{\"1\":{\"40\":1}}],[\"dog\",{\"1\":{\"40\":1}}],[\"doctors\",{\"1\":{\"94\":2}}],[\"doctor\",{\"1\":{\"17\":1,\"94\":1}}],[\"diet\",{\"1\":{\"94\":1}}],[\"dicing\",{\"0\":{\"94\":1}}],[\"dictionary\",{\"1\":{\"36\":1}}],[\"different\",{\"1\":{\"80\":1}}],[\"dimension\",{\"1\":{\"50\":1}}],[\"dim=\",{\"1\":{\"32\":1,\"73\":1}}],[\"directory\",{\"1\":{\"36\":2,\"43\":1}}],[\"distorting\",{\"1\":{\"60\":2}}],[\"distilbert\",{\"1\":{\"28\":2,\"29\":2,\"31\":1,\"50\":2,\"52\":1,\"53\":1,\"56\":1}}],[\"distillbert\",{\"1\":{\"22\":1}}],[\"distilgpt2\",{\"1\":{\"16\":1}}],[\"distinct\",{\"1\":{\"5\":1}}],[\"discriminant\",{\"1\":{\"4\":1}}],[\"dfsjh\",{\"1\":{\"6\":1}}],[\"depression\",{\"1\":{\"94\":4}}],[\"dependent\",{\"1\":{\"5\":1}}],[\"description\",{\"1\":{\"87\":1}}],[\"delimiter=\",{\"1\":{\"94\":1}}],[\"delimiter\",{\"1\":{\"94\":1}}],[\"deliberately\",{\"1\":{\"60\":2}}],[\"delete\",{\"1\":{\"81\":2}}],[\"device\",{\"1\":{\"72\":6,\"73\":1,\"74\":8}}],[\"decay\",{\"1\":{\"70\":1}}],[\"decoded\",{\"1\":{\"47\":2}}],[\"decode\",{\"1\":{\"47\":3,\"55\":3}}],[\"decoder\",{\"1\":{\"22\":3,\"24\":7}}],[\"decoders\",{\"1\":{\"22\":1}}],[\"decoding\",{\"0\":{\"47\":1},\"1\":{\"47\":1}}],[\"def\",{\"1\":{\"61\":1,\"63\":1,\"65\":1,\"66\":1,\"94\":1,\"95\":1}}],[\"deep\",{\"1\":{\"4\":1}}],[\"lh\",{\"1\":{\"86\":1}}],[\"lfs\",{\"1\":{\"78\":1,\"83\":1,\"84\":1,\"85\":2,\"86\":10}}],[\"lr=3e\",{\"1\":{\"74\":2}}],[\"lr=5e\",{\"1\":{\"70\":1}}],[\"lr\",{\"1\":{\"71\":1,\"72\":1,\"74\":4}}],[\"lowercase\",{\"1\":{\"94\":2}}],[\"lower\",{\"1\":{\"94\":3}}],[\"logout\",{\"1\":{\"81\":1}}],[\"login\",{\"1\":{\"81\":2}}],[\"logits\",{\"1\":{\"31\":1,\"32\":3,\"50\":1,\"51\":3,\"52\":1,\"65\":4,\"69\":1,\"73\":3}}],[\"logistic\",{\"1\":{\"4\":1}}],[\"loop\",{\"0\":{\"72\":1,\"73\":1},\"1\":{\"67\":1,\"73\":1,\"75\":1}}],[\"loss\",{\"1\":{\"64\":2,\"65\":2,\"69\":1,\"72\":3,\"74\":7}}],[\"loading\",{\"1\":{\"90\":1,\"91\":1}}],[\"load\",{\"1\":{\"60\":2,\"63\":2,\"65\":3,\"66\":2,\"73\":1,\"90\":4,\"91\":4,\"92\":1,\"94\":2}}],[\"longer\",{\"1\":{\"53\":2}}],[\"longest\",{\"1\":{\"52\":1}}],[\"longformer\",{\"1\":{\"53\":1}}],[\"ly\",{\"1\":{\"42\":1}}],[\"ls\",{\"1\":{\"36\":1,\"86\":2}}],[\"lambda\",{\"1\":{\"94\":1,\"95\":2}}],[\"large\",{\"1\":{\"83\":1,\"85\":1}}],[\"launcher\",{\"1\":{\"74\":2}}],[\"launch\",{\"1\":{\"74\":1}}],[\"layer\",{\"1\":{\"34\":1,\"51\":1,\"94\":1}}],[\"layers\",{\"0\":{\"23\":1},\"1\":{\"29\":1,\"34\":1,\"51\":1,\"52\":1}}],[\"last\",{\"1\":{\"30\":1}}],[\"lasso\",{\"1\":{\"4\":1}}],[\"languages\",{\"1\":{\"88\":1}}],[\"language\",{\"0\":{\"19\":1},\"1\":{\"88\":1}}],[\"lawyer\",{\"1\":{\"17\":1}}],[\"labels=2\",{\"1\":{\"64\":1,\"65\":1,\"69\":1,\"74\":2}}],[\"labels\",{\"1\":{\"62\":1,\"65\":1,\"67\":3,\"68\":1,\"73\":1}}],[\"label\",{\"1\":{\"16\":3,\"32\":1,\"60\":7,\"61\":3,\"65\":3,\"67\":2}}],[\"license\",{\"1\":{\"88\":1}}],[\"libraries\",{\"1\":{\"88\":1}}],[\"limitations\",{\"1\":{\"87\":2}}],[\"list\",{\"1\":{\"37\":1,\"81\":4,\"91\":1,\"95\":1}}],[\"like\",{\"1\":{\"18\":3,\"23\":3,\"94\":1}}],[\"life\",{\"1\":{\"16\":2,\"28\":2,\"50\":3,\"52\":1,\"53\":1,\"55\":3,\"56\":1,\"94\":2}}],[\"lines\",{\"1\":{\"90\":1}}],[\"line\",{\"1\":{\"50\":1}}],[\"liner\",{\"1\":{\"4\":1}}],[\"linear\",{\"1\":{\"4\":7,\"71\":1,\"74\":2}}],[\"let\",{\"1\":{\"94\":1}}],[\"legs\",{\"1\":{\"94\":3}}],[\"lexapro\",{\"1\":{\"94\":2}}],[\"len\",{\"1\":{\"62\":1,\"71\":1,\"74\":2,\"94\":2,\"95\":1}}],[\"length=8\",{\"1\":{\"52\":1,\"53\":1}}],[\"length=30\",{\"1\":{\"16\":1}}],[\"length\",{\"1\":{\"5\":2,\"30\":1,\"52\":5,\"53\":3,\"74\":1,\"95\":6}}],[\"led\",{\"1\":{\"53\":1}}],[\"levels\",{\"1\":{\"94\":1}}],[\"level\",{\"1\":{\"42\":1,\"58\":1}}],[\"leave\",{\"1\":{\"94\":1}}],[\"lease\",{\"1\":{\"4\":1}}],[\"learning\",{\"0\":{\"21\":1,\"71\":1},\"1\":{\"2\":1,\"4\":3,\"19\":1,\"71\":1}}],[\">\",{\"1\":{\"4\":3,\"11\":2,\"95\":1}}],[\"small\",{\"1\":{\"94\":1}}],[\"switching\",{\"1\":{\"94\":1}}],[\"switched\",{\"1\":{\"94\":2}}],[\"swelling\",{\"1\":{\"94\":2}}],[\"swell\",{\"1\":{\"94\":1}}],[\"symptoms\",{\"1\":{\"94\":3}}],[\"symptms\",{\"1\":{\"94\":1}}],[\"symbols\",{\"1\":{\"28\":1}}],[\"synthroid\",{\"1\":{\"94\":2}}],[\"skin\",{\"1\":{\"94\":2}}],[\"sklearn\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"11\":1}}],[\"slicing\",{\"0\":{\"94\":1}}],[\"squad\",{\"1\":{\"91\":17,\"92\":4}}],[\"squares\",{\"1\":{\"4\":1}}],[\"shocked\",{\"1\":{\"94\":1}}],[\"should\",{\"1\":{\"94\":1}}],[\"shows\",{\"1\":{\"94\":1}}],[\"shuffle\",{\"1\":{\"94\":2}}],[\"shuffle=true\",{\"1\":{\"68\":1}}],[\"shell\",{\"1\":{\"91\":1}}],[\"shape\",{\"1\":{\"30\":1,\"31\":1,\"62\":1,\"65\":2,\"68\":1,\"69\":1}}],[\"sat\",{\"1\":{\"94\":1}}],[\"said\",{\"1\":{\"94\":1}}],[\"sample\",{\"1\":{\"94\":2}}],[\"samples\",{\"1\":{\"62\":5}}],[\"saved\",{\"1\":{\"94\":1}}],[\"save\",{\"1\":{\"36\":2,\"43\":2,\"80\":1,\"85\":2,\"86\":2}}],[\"specification\",{\"1\":{\"88\":1}}],[\"specified\",{\"1\":{\"52\":1,\"53\":1}}],[\"special\",{\"1\":{\"86\":3}}],[\"space\",{\"1\":{\"81\":2}}],[\"split\",{\"1\":{\"40\":1,\"94\":3,\"95\":1}}],[\"side\",{\"1\":{\"94\":3}}],[\"sideeffectsreview\",{\"1\":{\"94\":1}}],[\"sideeffects\",{\"1\":{\"94\":1}}],[\"simple\",{\"1\":{\"43\":1,\"45\":2,\"47\":1}}],[\"size=8\",{\"1\":{\"68\":2}}],[\"size\",{\"1\":{\"30\":3,\"31\":1,\"34\":4,\"62\":4,\"68\":4,\"69\":1,\"94\":1,\"96\":1}}],[\"sst\",{\"1\":{\"28\":2,\"29\":2,\"31\":1,\"50\":2,\"56\":1}}],[\"sort\",{\"1\":{\"95\":2}}],[\"sore\",{\"1\":{\"94\":1}}],[\"soak\",{\"1\":{\"94\":1}}],[\"some\",{\"1\":{\"81\":1,\"94\":1}}],[\"softmax\",{\"1\":{\"32\":2}}],[\"soft\",{\"1\":{\"23\":1}}],[\"so\",{\"1\":{\"16\":1,\"28\":2,\"50\":1,\"52\":1,\"53\":1,\"56\":1,\"94\":4}}],[\"s\",{\"1\":{\"11\":1}}],[\"script\",{\"1\":{\"90\":1,\"91\":1}}],[\"scheduler\",{\"0\":{\"71\":1},\"1\":{\"71\":4,\"72\":1,\"74\":8}}],[\"score的超级无敌深入探讨\",{\"1\":{\"11\":1}}],[\"score相同的权重\",{\"1\":{\"11\":1}}],[\"score和recall\",{\"1\":{\"11\":1}}],[\"score\",{\"0\":{\"11\":1},\"1\":{\"7\":1,\"8\":2,\"9\":2,\"10\":2,\"11\":3,\"16\":3}}],[\"scalar\",{\"1\":{\"4\":1}}],[\"sun\",{\"1\":{\"94\":1}}],[\"sunscreen\",{\"1\":{\"94\":1}}],[\"surprise\",{\"1\":{\"94\":1}}],[\"survival\",{\"1\":{\"4\":1}}],[\"subsided\",{\"1\":{\"94\":1}}],[\"subword\",{\"0\":{\"42\":1},\"1\":{\"41\":1,\"42\":2}}],[\"subwords\",{\"1\":{\"28\":1}}],[\"supervised\",{\"1\":{\"19\":1}}],[\"support\",{\"1\":{\"4\":2}}],[\"section2\",{\"0\":{\"99\":1}}],[\"section1\",{\"0\":{\"98\":1}}],[\"second\",{\"1\":{\"61\":3}}],[\"see\",{\"1\":{\"94\":1}}],[\"sever\",{\"1\":{\"94\":2}}],[\"severe\",{\"1\":{\"94\":1}}],[\"sep\",{\"1\":{\"55\":2,\"61\":6}}],[\"sentence\",{\"1\":{\"61\":3}}],[\"sentences\",{\"1\":{\"61\":2}}],[\"sentence2\",{\"1\":{\"60\":6,\"61\":7,\"62\":2,\"63\":1,\"66\":1,\"67\":2}}],[\"sentence1\",{\"1\":{\"60\":6,\"61\":7,\"62\":2,\"63\":1,\"66\":1,\"67\":2}}],[\"sentencepiece\",{\"1\":{\"42\":1,\"86\":4}}],[\"sentiment\",{\"1\":{\"16\":2,\"28\":3}}],[\"select\",{\"1\":{\"94\":3}}],[\"selection\",{\"1\":{\"4\":1}}],[\"self\",{\"1\":{\"19\":1}}],[\"sequence2\",{\"1\":{\"51\":3,\"52\":1}}],[\"sequence1\",{\"1\":{\"51\":2}}],[\"sequences\",{\"1\":{\"37\":3,\"50\":4,\"52\":8,\"53\":5,\"56\":2}}],[\"sequences=2\",{\"1\":{\"16\":1}}],[\"sequence\",{\"1\":{\"18\":2,\"22\":2,\"30\":1,\"31\":1,\"45\":2,\"50\":5,\"52\":1,\"53\":3,\"55\":3,\"64\":1}}],[\"set\",{\"1\":{\"4\":1,\"60\":3,\"62\":1,\"67\":1}}],[\"started\",{\"1\":{\"94\":3}}],[\"stay\",{\"1\":{\"94\":1}}],[\"staged\",{\"1\":{\"86\":2}}],[\"status\",{\"1\":{\"86\":2}}],[\"state\",{\"1\":{\"30\":1,\"36\":1}}],[\"states\",{\"0\":{\"30\":1},\"1\":{\"29\":1,\"31\":1}}],[\"statistical\",{\"1\":{\"2\":1,\"4\":2}}],[\"storage\",{\"1\":{\"83\":1,\"85\":1}}],[\"step\",{\"1\":{\"72\":2,\"74\":4}}],[\"steps=num\",{\"1\":{\"71\":1,\"74\":2}}],[\"steps=0\",{\"1\":{\"71\":1,\"74\":2}}],[\"steps\",{\"1\":{\"64\":3,\"71\":4,\"72\":1,\"74\":6}}],[\"stepwise\",{\"1\":{\"4\":1}}],[\"strategy=\",{\"1\":{\"65\":1,\"80\":1}}],[\"strategy\",{\"1\":{\"64\":1}}],[\"string\",{\"1\":{\"47\":2,\"60\":2}}],[\"str\",{\"1\":{\"17\":2}}],[\"zero\",{\"1\":{\"4\":1,\"72\":1,\"74\":2}}],[\"html\",{\"1\":{\"94\":1,\"95\":5}}],[\"https\",{\"1\":{\"86\":2,\"91\":2,\"92\":1}}],[\"http\",{\"1\":{\"84\":1}}],[\"hypo\",{\"1\":{\"94\":1}}],[\"hypothesis\",{\"1\":{\"4\":1}}],[\"healthy\",{\"1\":{\"94\":2}}],[\"header\",{\"1\":{\"88\":1}}],[\"head\",{\"1\":{\"31\":1,\"64\":2}}],[\"heads\",{\"0\":{\"31\":1},\"1\":{\"31\":1,\"34\":1}}],[\"helps\",{\"1\":{\"94\":1}}],[\"hello\",{\"1\":{\"37\":1}}],[\"he\",{\"1\":{\"60\":1}}],[\"henson\",{\"1\":{\"40\":2}}],[\"hour\",{\"1\":{\"94\":1}}],[\"hoping\",{\"1\":{\"94\":1}}],[\"home\",{\"1\":{\"35\":1,\"60\":1}}],[\"however\",{\"1\":{\"94\":1}}],[\"how\",{\"1\":{\"16\":1,\"87\":1}}],[\"hf\",{\"1\":{\"35\":1,\"60\":1}}],[\"him\",{\"1\":{\"60\":1}}],[\"his\",{\"1\":{\"60\":4}}],[\"highly\",{\"1\":{\"94\":1}}],[\"high\",{\"1\":{\"58\":1}}],[\"hidden\",{\"0\":{\"30\":1},\"1\":{\"29\":1,\"30\":2,\"31\":1,\"34\":4}}],[\"hierarchical\",{\"1\":{\"4\":1}}],[\"had\",{\"1\":{\"94\":2}}],[\"hanzhuo\",{\"1\":{\"86\":2}}],[\"have\",{\"1\":{\"50\":1,\"52\":1,\"53\":1,\"56\":1,\"94\":1}}],[\"hate\",{\"1\":{\"16\":1,\"28\":2}}],[\"has\",{\"1\":{\"4\":1,\"94\":3}}],[\"hurt\",{\"1\":{\"94\":1}}],[\"hub=true\",{\"1\":{\"64\":1,\"80\":2}}],[\"hub\",{\"0\":{\"60\":1,\"80\":1,\"81\":1,\"90\":1},\"1\":{\"15\":1,\"58\":1,\"59\":1,\"60\":1,\"64\":1,\"75\":1,\"76\":2,\"78\":2,\"80\":13,\"81\":3,\"83\":2,\"84\":2,\"85\":3,\"88\":1,\"89\":3}}],[\"huggingface\",{\"0\":{\"81\":1},\"1\":{\"16\":2,\"28\":2,\"35\":1,\"50\":3,\"52\":1,\"53\":1,\"55\":3,\"56\":1,\"60\":1,\"78\":1,\"80\":3,\"81\":5,\"84\":1,\"85\":1,\"86\":2}}],[\"hugging\",{\"0\":{\"90\":1},\"1\":{\"1\":1,\"76\":1,\"80\":1,\"83\":1,\"88\":1,\"89\":1}}],[\"误差项\",{\"1\":{\"4\":1}}],[\"概念\",{\"0\":{\"4\":1}}],[\"15\",{\"1\":{\"94\":1}}],[\"1445\",{\"1\":{\"95\":1}}],[\"1444\",{\"1\":{\"94\":1}}],[\"1468\",{\"1\":{\"94\":1}}],[\"1377\",{\"1\":{\"71\":1}}],[\"1`\",{\"1\":{\"64\":1}}],[\"1996\",{\"1\":{\"61\":2}}],[\"1110\",{\"1\":{\"43\":1,\"46\":1,\"47\":1}}],[\"11303\",{\"1\":{\"43\":1,\"46\":1,\"47\":1}}],[\"1725\",{\"1\":{\"60\":1,\"61\":1}}],[\"170\",{\"1\":{\"43\":1,\"46\":1,\"47\":1}}],[\"17662\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":1,\"55\":2}}],[\"125\",{\"1\":{\"94\":1}}],[\"125mcg\",{\"1\":{\"94\":1}}],[\"1200\",{\"1\":{\"43\":1,\"46\":1,\"47\":1}}],[\"12\",{\"1\":{\"34\":3}}],[\"12172\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":1,\"55\":2}}],[\"1e\",{\"1\":{\"34\":1}}],[\"1692\",{\"1\":{\"32\":1}}],[\"16\",{\"1\":{\"30\":1}}],[\"10\",{\"1\":{\"59\":1,\"65\":1,\"94\":1}}],[\"1000\",{\"1\":{\"94\":3,\"96\":1}}],[\"100\",{\"1\":{\"51\":1}}],[\"1005\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":2,\"55\":2}}],[\"1024\",{\"1\":{\"53\":1}}],[\"102\",{\"1\":{\"28\":2,\"37\":3,\"43\":1,\"50\":1,\"52\":8,\"53\":4,\"55\":1,\"61\":2}}],[\"1037\",{\"1\":{\"28\":1,\"50\":2,\"52\":4,\"53\":1,\"55\":2}}],[\"1045\",{\"1\":{\"28\":2,\"50\":2,\"52\":8,\"53\":4,\"55\":2}}],[\"1012\",{\"1\":{\"28\":1,\"37\":1,\"50\":2,\"52\":4,\"53\":1,\"55\":2,\"61\":2}}],[\"101\",{\"1\":{\"28\":2,\"37\":3,\"43\":1,\"50\":1,\"52\":8,\"53\":4,\"55\":1,\"61\":1}}],[\"1\",{\"0\":{\"4\":1,\"8\":1,\"12\":1,\"13\":1,\"14\":2,\"16\":1,\"19\":1,\"23\":1,\"27\":1,\"28\":2,\"29\":1,\"30\":2,\"31\":1,\"32\":1,\"34\":1,\"35\":2,\"36\":1,\"39\":1,\"40\":2,\"41\":1,\"42\":1,\"45\":1,\"50\":1,\"55\":1,\"59\":1,\"60\":2,\"61\":1,\"62\":1,\"64\":1,\"67\":1,\"68\":2,\"69\":1,\"70\":1,\"71\":1,\"77\":1,\"79\":1,\"80\":2,\"81\":1,\"82\":1,\"84\":1,\"90\":1,\"91\":2,\"92\":1,\"94\":1},\"1\":{\"16\":1,\"28\":24,\"32\":6,\"34\":3,\"37\":3,\"41\":1,\"43\":9,\"44\":1,\"50\":2,\"51\":6,\"52\":96,\"53\":36,\"60\":2,\"61\":30,\"65\":2,\"72\":1,\"73\":1,\"74\":2,\"94\":1}}],[\"nlisten\",{\"1\":{\"94\":1}}],[\"nlp\",{\"0\":{\"13\":1},\"1\":{\"1\":1,\"13\":2,\"25\":2,\"76\":1}}],[\"night\",{\"1\":{\"94\":1}}],[\"ni\",{\"1\":{\"94\":1}}],[\"nice\",{\"1\":{\"37\":1}}],[\"nthe\",{\"1\":{\"94\":1}}],[\"nasty\",{\"1\":{\"94\":1}}],[\"name=\",{\"1\":{\"94\":2}}],[\"name\",{\"1\":{\"80\":2,\"86\":1}}],[\"names\",{\"1\":{\"67\":1}}],[\"names=\",{\"1\":{\"60\":1}}],[\"np\",{\"1\":{\"50\":1,\"65\":3}}],[\"nn\",{\"1\":{\"32\":1}}],[\"null\",{\"1\":{\"34\":1}}],[\"nurse\",{\"1\":{\"17\":1}}],[\"numpy\",{\"1\":{\"50\":1,\"65\":1,\"95\":1}}],[\"num\",{\"1\":{\"16\":1,\"34\":2,\"60\":3,\"61\":4,\"64\":1,\"65\":1,\"69\":1,\"71\":6,\"72\":1,\"74\":16,\"91\":3,\"95\":1}}],[\"number\",{\"1\":{\"5\":2,\"72\":1}}],[\"numerical\",{\"1\":{\"4\":1}}],[\"n×p\",{\"1\":{\"5\":1}}],[\"n\",{\"1\":{\"5\":3,\"94\":4}}],[\"new\",{\"1\":{\"86\":6,\"94\":2}}],[\"negative\",{\"1\":{\"7\":2,\"11\":2,\"16\":1,\"31\":1,\"32\":3}}],[\"neighbor\",{\"1\":{\"4\":1}}],[\"nearest\",{\"1\":{\"4\":1}}],[\"network\",{\"1\":{\"4\":1,\"29\":2,\"43\":1,\"45\":2,\"47\":1}}],[\"neural\",{\"1\":{\"4\":1}}],[\"no\",{\"1\":{\"73\":1,\"94\":2}}],[\"notebook\",{\"1\":{\"74\":3}}],[\"not\",{\"1\":{\"60\":2,\"62\":1,\"86\":1,\"94\":3}}],[\"notation\",{\"0\":{\"5\":1},\"1\":{\"3\":1}}],[\"norm\",{\"1\":{\"34\":1}}],[\"nonetype\",{\"1\":{\"94\":1}}],[\"none\",{\"1\":{\"94\":3}}],[\"non\",{\"1\":{\"4\":4}}],[\"本章我们将介绍其他函数\",{\"1\":{\"94\":1}}],[\"本节内容介绍了\",{\"1\":{\"25\":1}}],[\"本篇文章记录了一些相关概念以及\",{\"1\":{\"3\":1}}],[\"本模块用于记录\",{\"1\":{\"1\":1}}],[\"简介\",{\"0\":{\"3\":1}}],[\"ai\",{\"0\":{\"97\":1}}],[\"at\",{\"1\":{\"94\":1}}],[\"attribute\",{\"1\":{\"94\":1}}],[\"attributeerror\",{\"1\":{\"94\":1}}],[\"attack\",{\"1\":{\"94\":1}}],[\"attention\",{\"0\":{\"23\":1,\"52\":1},\"1\":{\"24\":1,\"28\":1,\"34\":2,\"43\":2,\"51\":3,\"52\":10,\"53\":2,\"57\":1,\"61\":5,\"62\":1,\"67\":1,\"68\":1}}],[\"amount\",{\"1\":{\"94\":1}}],[\"am\",{\"1\":{\"94\":1}}],[\"amrozi\",{\"1\":{\"60\":2}}],[\"after\",{\"1\":{\"94\":1}}],[\"avoid\",{\"1\":{\"94\":1}}],[\"available\",{\"1\":{\"5\":1,\"72\":1,\"74\":1}}],[\"all\",{\"1\":{\"94\":1}}],[\"allergies\",{\"1\":{\"94\":1}}],[\"allergy\",{\"1\":{\"94\":1}}],[\"alot\",{\"1\":{\"94\":1}}],[\"also\",{\"1\":{\"94\":1}}],[\"albert\",{\"1\":{\"22\":1}}],[\"ago\",{\"1\":{\"94\":1}}],[\"again\",{\"1\":{\"94\":1}}],[\"about\",{\"1\":{\"81\":1,\"94\":1}}],[\"absolute\",{\"1\":{\"34\":1}}],[\"auth\",{\"1\":{\"80\":1}}],[\"automodelformaskedlm\",{\"1\":{\"80\":2,\"86\":2}}],[\"automodelforsequenceclassification\",{\"1\":{\"31\":3,\"50\":4,\"51\":1,\"56\":2,\"64\":2,\"65\":1,\"69\":2,\"74\":4}}],[\"automodel\",{\"1\":{\"29\":3,\"31\":1,\"33\":1,\"35\":3}}],[\"autotokenizer\",{\"1\":{\"28\":4,\"29\":1,\"43\":3,\"45\":2,\"50\":4,\"56\":2,\"61\":2,\"63\":2,\"66\":2,\"80\":2,\"86\":2}}],[\"auto\",{\"1\":{\"18\":2,\"22\":2,\"72\":1}}],[\"add\",{\"1\":{\"73\":2,\"85\":3,\"86\":1,\"95\":2}}],[\"additive\",{\"1\":{\"4\":2}}],[\"adam\",{\"1\":{\"70\":1}}],[\"adamw\",{\"1\":{\"70\":3,\"74\":4}}],[\"axis=\",{\"1\":{\"65\":2}}],[\"ached\",{\"1\":{\"94\":1}}],[\"acne\",{\"1\":{\"94\":1}}],[\"active\",{\"1\":{\"94\":1}}],[\"act\",{\"1\":{\"34\":1}}],[\"accelerator\",{\"1\":{\"74\":11}}],[\"accelerate\",{\"0\":{\"74\":1},\"1\":{\"1\":1,\"58\":1,\"64\":4,\"74\":8,\"75\":1}}],[\"accused\",{\"1\":{\"60\":2}}],[\"accuracy=tp+tn+fp+fntp+tn​\",{\"1\":{\"8\":1}}],[\"accuracy\",{\"0\":{\"8\":1},\"1\":{\"7\":1,\"8\":3,\"65\":2,\"73\":1}}],[\"apache\",{\"1\":{\"89\":1}}],[\"api\",{\"0\":{\"54\":1,\"63\":1,\"80\":1},\"1\":{\"25\":1,\"26\":2,\"58\":1,\"76\":1,\"78\":1,\"80\":1}}],[\"apply\",{\"1\":{\"94\":1}}],[\"apples\",{\"1\":{\"23\":2}}],[\"approaches\",{\"1\":{\"4\":1}}],[\"assertionerror\",{\"1\":{\"94\":1}}],[\"assert\",{\"1\":{\"94\":1}}],[\"as\",{\"1\":{\"17\":2,\"60\":1,\"65\":1}}],[\"arrow\",{\"1\":{\"89\":1}}],[\"arrays\",{\"1\":{\"50\":1}}],[\"argmax\",{\"1\":{\"65\":2,\"73\":1}}],[\"args\",{\"1\":{\"64\":3,\"65\":2,\"80\":1}}],[\"architecture\",{\"1\":{\"14\":4,\"43\":1}}],[\"architectures\",{\"0\":{\"14\":1},\"1\":{\"14\":1}}],[\"area\",{\"1\":{\"94\":2}}],[\"are\",{\"1\":{\"5\":1,\"53\":2}}],[\"a∈rk×s\",{\"1\":{\"5\":1}}],[\"a∈rk\",{\"1\":{\"5\":1}}],[\"a∈r\",{\"1\":{\"5\":1}}],[\"a\",{\"1\":{\"4\":1,\"5\":2,\"16\":2,\"17\":2,\"23\":1,\"28\":2,\"40\":2,\"43\":1,\"45\":2,\"47\":1,\"50\":3,\"52\":1,\"53\":1,\"55\":3,\"56\":1,\"80\":1,\"94\":3,\"95\":2,\"96\":1}}],[\"ankles\",{\"1\":{\"94\":1}}],[\"anxiety\",{\"1\":{\"94\":5}}],[\"annoy\",{\"1\":{\"42\":1}}],[\"annoyingly\",{\"1\":{\"42\":1}}],[\"and\",{\"0\":{\"94\":1},\"1\":{\"4\":3,\"29\":1,\"81\":2,\"85\":1,\"87\":1,\"94\":20}}],[\"analysis\",{\"1\":{\"4\":3,\"16\":2,\"28\":3}}],[\"an\",{\"1\":{\"2\":1,\"94\":2}}],[\"访问\",{\"1\":{\"2\":1}}],[\"书是开源的\",{\"1\":{\"2\":1}}],[\"统计学习导论\",{\"0\":{\"2\":1}}],[\"读书笔记\",{\"0\":{\"2\":1}}],[\"two\",{\"1\":{\"94\":1}}],[\"tired\",{\"1\":{\"94\":1}}],[\"tirosint\",{\"1\":{\"94\":6}}],[\"title\",{\"1\":{\"91\":3}}],[\"t\",{\"1\":{\"94\":1}}],[\"txt\",{\"1\":{\"90\":1}}],[\"tsv\",{\"1\":{\"90\":1,\"94\":4}}],[\"taking\",{\"1\":{\"94\":3}}],[\"take\",{\"1\":{\"94\":1}}],[\"tazorac\",{\"1\":{\"94\":1}}],[\"tasks\",{\"1\":{\"88\":1}}],[\"tag\",{\"1\":{\"85\":1}}],[\"tqdm\",{\"1\":{\"72\":4,\"74\":2}}],[\"tf\",{\"1\":{\"50\":1}}],[\"type\",{\"1\":{\"34\":3,\"43\":2,\"61\":8,\"62\":1,\"67\":1,\"68\":1,\"81\":1,\"84\":1}}],[\"tuning\",{\"1\":{\"21\":1,\"58\":1}}],[\"tune\",{\"1\":{\"19\":1,\"86\":1,\"89\":1}}],[\"t5\",{\"1\":{\"18\":1,\"19\":1,\"22\":1}}],[\"tn\",{\"1\":{\"7\":1}}],[\"tpu\",{\"1\":{\"65\":1,\"74\":2,\"75\":1}}],[\"tp\",{\"1\":{\"7\":1}}],[\"thinking\",{\"1\":{\"94\":1}}],[\"this\",{\"1\":{\"16\":2,\"17\":2,\"28\":2,\"50\":1,\"61\":6,\"94\":2}}],[\"three\",{\"1\":{\"94\":1}}],[\"thyroidectomy\",{\"1\":{\"94\":1}}],[\"thyroid\",{\"1\":{\"94\":1}}],[\"than\",{\"1\":{\"53\":2,\"94\":1}}],[\"that\",{\"1\":{\"4\":1,\"5\":1,\"53\":2,\"94\":5}}],[\"then\",{\"1\":{\"94\":5}}],[\"the\",{\"1\":{\"5\":7,\"22\":1,\"25\":1,\"29\":1,\"52\":6,\"53\":4,\"60\":2,\"61\":6,\"64\":1,\"81\":1,\"86\":1,\"94\":16}}],[\"tensorflow\",{\"1\":{\"50\":1}}],[\"tensors=\",{\"1\":{\"28\":1,\"50\":4,\"56\":1}}],[\"tensors\",{\"1\":{\"28\":1,\"50\":3,\"54\":1,\"62\":1}}],[\"tensor\",{\"1\":{\"28\":3,\"32\":2,\"37\":3,\"50\":6,\"51\":8,\"52\":5,\"57\":1,\"67\":1,\"69\":1}}],[\"teacher\",{\"1\":{\"17\":1}}],[\"teach\",{\"1\":{\"16\":1}}],[\"text\",{\"1\":{\"16\":1,\"40\":2,\"90\":3,\"95\":2}}],[\"test\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"11\":1,\"60\":2,\"61\":1,\"64\":1,\"65\":1,\"91\":7,\"92\":2,\"94\":1,\"95\":1}}],[\"testing\",{\"1\":{\"4\":1}}],[\"term\",{\"1\":{\"4\":1}}],[\"try\",{\"1\":{\"94\":1}}],[\"trainerarguments\",{\"1\":{\"80\":3}}],[\"trainer\",{\"0\":{\"63\":1},\"1\":{\"63\":2,\"64\":13,\"65\":10,\"66\":2,\"67\":1,\"70\":1,\"71\":1,\"80\":4}}],[\"trained\",{\"1\":{\"35\":2}}],[\"trainingarguments\",{\"1\":{\"64\":4,\"65\":1}}],[\"training\",{\"0\":{\"64\":1,\"72\":1},\"1\":{\"60\":1,\"64\":5,\"65\":2,\"67\":1,\"71\":7,\"72\":1,\"74\":10,\"75\":1,\"80\":1,\"87\":2}}],[\"train\",{\"1\":{\"60\":6,\"61\":5,\"62\":2,\"63\":1,\"64\":4,\"65\":3,\"67\":1,\"68\":3,\"71\":1,\"72\":2,\"74\":12,\"80\":1,\"86\":1,\"91\":11,\"92\":2,\"94\":2,\"95\":2}}],[\"transform\",{\"1\":{\"45\":1}}],[\"transformer\",{\"0\":{\"12\":1,\"18\":1,\"19\":1,\"20\":1,\"22\":1,\"34\":1,\"37\":1},\"1\":{\"18\":5,\"19\":1,\"22\":4,\"24\":2,\"25\":2,\"26\":2,\"28\":2,\"29\":4,\"30\":1,\"37\":1,\"43\":1,\"45\":1,\"47\":1,\"51\":1,\"53\":1,\"57\":2,\"95\":2}}],[\"transformers\",{\"0\":{\"15\":1,\"16\":1,\"26\":1},\"1\":{\"1\":1,\"15\":1,\"16\":3,\"17\":1,\"25\":1,\"26\":1,\"28\":2,\"29\":3,\"31\":1,\"32\":1,\"34\":2,\"35\":2,\"36\":1,\"43\":2,\"45\":1,\"50\":3,\"56\":1,\"61\":1,\"62\":2,\"63\":2,\"64\":6,\"66\":1,\"69\":1,\"70\":1,\"71\":1,\"74\":2,\"76\":1,\"80\":2,\"86\":1}}],[\"transfer\",{\"0\":{\"21\":1},\"1\":{\"19\":1}}],[\"truncate\",{\"1\":{\"53\":2,\"54\":1,\"56\":1}}],[\"truncation=true\",{\"1\":{\"28\":1,\"53\":2,\"56\":1,\"61\":2,\"63\":1,\"66\":1}}],[\"truncation\",{\"1\":{\"28\":1}}],[\"true\",{\"1\":{\"7\":2,\"34\":1,\"61\":1,\"96\":1}}],[\"tree\",{\"1\":{\"4\":1}}],[\"trees\",{\"1\":{\"4\":1}}],[\"toner\",{\"1\":{\"94\":1}}],[\"too\",{\"1\":{\"94\":1}}],[\"touch\",{\"1\":{\"94\":1}}],[\"total\",{\"1\":{\"94\":1}}],[\"torch\",{\"1\":{\"30\":1,\"31\":1,\"32\":2,\"37\":2,\"50\":4,\"51\":3,\"52\":1,\"56\":1,\"62\":4,\"64\":1,\"67\":1,\"68\":5,\"69\":1,\"72\":4,\"73\":2,\"74\":3}}],[\"token=\",{\"1\":{\"80\":1}}],[\"tokenize\",{\"1\":{\"45\":2,\"50\":2,\"55\":1,\"63\":2,\"66\":2}}],[\"tokenized\",{\"1\":{\"40\":2,\"50\":2,\"61\":7,\"62\":1,\"63\":1,\"65\":1,\"66\":1,\"67\":8,\"68\":2}}],[\"tokenizer=tokenizer\",{\"1\":{\"62\":1,\"63\":1,\"64\":1,\"65\":1,\"66\":1}}],[\"tokenizer\",{\"0\":{\"28\":1,\"54\":1},\"1\":{\"26\":3,\"28\":6,\"30\":1,\"32\":1,\"37\":2,\"38\":1,\"40\":6,\"41\":1,\"43\":10,\"44\":4,\"45\":3,\"46\":1,\"47\":1,\"48\":1,\"50\":12,\"51\":2,\"52\":5,\"53\":3,\"54\":1,\"55\":7,\"56\":3,\"57\":3,\"58\":1,\"61\":8,\"62\":1,\"63\":2,\"64\":2,\"66\":2,\"74\":1,\"75\":1,\"80\":8,\"85\":5,\"86\":9}}],[\"tokenizers\",{\"0\":{\"38\":1,\"76\":1},\"1\":{\"1\":1,\"61\":1,\"80\":1}}],[\"tokenization\",{\"0\":{\"39\":1,\"42\":1,\"45\":1},\"1\":{\"38\":1,\"41\":1,\"42\":3,\"44\":1,\"45\":1,\"48\":1,\"57\":1,\"61\":2}}],[\"tokens\",{\"0\":{\"46\":1},\"1\":{\"28\":1,\"41\":2,\"44\":2,\"45\":2,\"46\":3,\"47\":2,\"48\":1,\"50\":6,\"51\":2,\"52\":1,\"53\":1,\"55\":3,\"56\":2,\"61\":1,\"80\":1,\"86\":3}}],[\"token\",{\"0\":{\"55\":1},\"1\":{\"17\":2,\"28\":1,\"34\":1,\"40\":7,\"41\":2,\"43\":2,\"51\":5,\"52\":1,\"55\":1,\"61\":8,\"62\":2,\"67\":1,\"68\":1,\"80\":1,\"81\":3,\"84\":1}}],[\"to\",{\"0\":{\"80\":1},\"1\":{\"2\":1,\"16\":1,\"18\":1,\"22\":1,\"46\":2,\"50\":3,\"52\":3,\"55\":1,\"60\":1,\"61\":1,\"64\":1,\"72\":2,\"73\":1,\"74\":2,\"78\":1,\"80\":9,\"81\":1,\"84\":1,\"85\":5,\"86\":8,\"87\":1,\"94\":21}}],[\"🤗\",{\"0\":{\"16\":1,\"26\":1,\"74\":1,\"89\":1},\"1\":{\"1\":4,\"16\":1,\"25\":1,\"26\":1,\"29\":3,\"32\":1,\"36\":1,\"50\":1,\"58\":1,\"60\":1,\"61\":2,\"62\":1,\"63\":1,\"64\":2,\"65\":1,\"73\":1,\"74\":3,\"75\":1,\"76\":1,\"84\":1,\"89\":2,\"90\":1,\"91\":1,\"94\":2}}],[\"介绍\",{\"0\":{\"1\":1,\"13\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(st(t,v[s],n)):e==="search"?self.postMessage(et(t,v[s],n)):self.postMessage({suggestions:st(t,v[s],n),results:et(t,v[s],n)})};
//# sourceMappingURL=index.js.map
